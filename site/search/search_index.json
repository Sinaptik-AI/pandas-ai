{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83d\udc3c PandasAI PandasAI is a Python library that adds Generative AI capabilities to pandas, the popular data analysis and manipulation tool. It is designed to be used in conjunction with pandas, and is not a replacement for it. PandasAI makes pandas (and all the most used data analyst libraries) conversational, allowing you to ask questions to your data in natural language. For example, you can ask PandasAI to find all the rows in a DataFrame where the value of a column is greater than 5, and it will return a DataFrame containing only those rows. You can also ask PandasAI to draw graphs, clean data, impute missing values, and generate features. What are the value props of PandasAI? PandasAI provides two main value props: Ease of use: PandasAI is designed to be easy to use, even if you are not familiar with generative AI or with pandas . You can simply ask questions to your data in natural language, and PandasAI will generate the code to answer your question. Power: PandasAI can be used to perform a wide variety of tasks, including data exploration, analysis, visualization, cleaning, imputation, and feature engineering. How does PandasAI work? PandasAI works by using a generative AI model to generate Python code. When you ask PandasAI a question, the model will first try to understand the question. Then, it will generate the Python code that would answer the question. Finally, the code will be executed, and the results will be returned to you. Who should use PandasAI? PandasAI is a good choice for anyone who wants to make their data analysis and manipulation workflow more efficient. It is especially useful for people who are not familiar with pandas , but also for people who are familiar with it and want to make their workflow more efficient. How to get started with PandasAI? To get started with PandasAI, you first need to install it. You can do this by running the following command: # Using poetry (recommended) poetry add pandasai # Using pip pip install pandasai Once you have installed PandasAI, you can start using it by importing it into your Python code. Now you can start asking questions to your data in natural language. For example, the following code will ask PandasAI to find all the rows in a DataFrame where the value of the gdp column is greater than 5: from pandasai import SmartDataframe df = pd.DataFrame({ \"country\": [ \"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [ 19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064 ], }) df = SmartDataframe(df) df.chat('Which are the countries with GDP greater than 3000000000000?') # Output: # 0 United States # 3 Germany # 8 Japan # 9 China # Name: country, dtype: object This will return a DataFrame containing only the rows where the value of the gdp column is greater than 5. Demo Try out PandasAI in your browser: Support If you have any questions or need help, please join our discord server . License PandasAI is licensed under the MIT License. See the LICENSE file for more details.","title":"Introduction"},{"location":"#pandasai","text":"PandasAI is a Python library that adds Generative AI capabilities to pandas, the popular data analysis and manipulation tool. It is designed to be used in conjunction with pandas, and is not a replacement for it. PandasAI makes pandas (and all the most used data analyst libraries) conversational, allowing you to ask questions to your data in natural language. For example, you can ask PandasAI to find all the rows in a DataFrame where the value of a column is greater than 5, and it will return a DataFrame containing only those rows. You can also ask PandasAI to draw graphs, clean data, impute missing values, and generate features.","title":"\ud83d\udc3c PandasAI"},{"location":"#what-are-the-value-props-of-pandasai","text":"PandasAI provides two main value props: Ease of use: PandasAI is designed to be easy to use, even if you are not familiar with generative AI or with pandas . You can simply ask questions to your data in natural language, and PandasAI will generate the code to answer your question. Power: PandasAI can be used to perform a wide variety of tasks, including data exploration, analysis, visualization, cleaning, imputation, and feature engineering.","title":"What are the value props of PandasAI?"},{"location":"#how-does-pandasai-work","text":"PandasAI works by using a generative AI model to generate Python code. When you ask PandasAI a question, the model will first try to understand the question. Then, it will generate the Python code that would answer the question. Finally, the code will be executed, and the results will be returned to you.","title":"How does PandasAI work?"},{"location":"#who-should-use-pandasai","text":"PandasAI is a good choice for anyone who wants to make their data analysis and manipulation workflow more efficient. It is especially useful for people who are not familiar with pandas , but also for people who are familiar with it and want to make their workflow more efficient.","title":"Who should use PandasAI?"},{"location":"#how-to-get-started-with-pandasai","text":"To get started with PandasAI, you first need to install it. You can do this by running the following command: # Using poetry (recommended) poetry add pandasai # Using pip pip install pandasai Once you have installed PandasAI, you can start using it by importing it into your Python code. Now you can start asking questions to your data in natural language. For example, the following code will ask PandasAI to find all the rows in a DataFrame where the value of the gdp column is greater than 5: from pandasai import SmartDataframe df = pd.DataFrame({ \"country\": [ \"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [ 19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064 ], }) df = SmartDataframe(df) df.chat('Which are the countries with GDP greater than 3000000000000?') # Output: # 0 United States # 3 Germany # 8 Japan # 9 China # Name: country, dtype: object This will return a DataFrame containing only the rows where the value of the gdp column is greater than 5.","title":"How to get started with PandasAI?"},{"location":"#demo","text":"Try out PandasAI in your browser:","title":"Demo"},{"location":"#support","text":"If you have any questions or need help, please join our discord server .","title":"Support"},{"location":"#license","text":"PandasAI is licensed under the MIT License. See the LICENSE file for more details.","title":"License"},{"location":"CONTRIBUTING/","text":"\ud83d\udc3c Contributing to PandasAI Hi there! We're thrilled that you'd like to contribute to this project. Your help is essential for keeping it great. \ud83e\udd1d How to submit a contribution To make a contribution, follow the following steps: Fork and clone this repository Do the changes on your fork If you modified the code (new feature or bug-fix), please add tests for it Check the linting see below Ensure that all tests pass see below Submit a pull request For more details about pull requests, please read GitHub's guides . \ud83d\udce6 Package manager We use poetry as our package manager. You can install poetry by following the instructions here . Please DO NOT use pip or conda to install the dependencies. Instead, use poetry: poetry install --all-extras \ud83d\udccc Pre-commit To ensure our standards, make sure to install pre-commit before starting to contribute. pre-commit install \ud83e\uddf9 Linting We use ruff to lint our code. You can run the linter by running the following command: ruff pandasai examples Make sure that the linter does not report any errors or warnings before submitting a pull request. Code Format with black We use black to reformat the code by running the following command: black pandasai \ud83e\uddea Testing We use pytest to test our code. You can run the tests by running the following command: poetry run pytest Make sure that all tests pass before submitting a pull request. \ud83d\ude80 Release Process At the moment, the release process is manual. We try to make frequent releases. Usually, we release a new version when we have a new feature or bugfix. A developer with admin rights to the repository will create a new release on GitHub, and then publish the new version to PyPI.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-to-pandasai","text":"Hi there! We're thrilled that you'd like to contribute to this project. Your help is essential for keeping it great.","title":"\ud83d\udc3c Contributing to PandasAI"},{"location":"CONTRIBUTING/#how-to-submit-a-contribution","text":"To make a contribution, follow the following steps: Fork and clone this repository Do the changes on your fork If you modified the code (new feature or bug-fix), please add tests for it Check the linting see below Ensure that all tests pass see below Submit a pull request For more details about pull requests, please read GitHub's guides .","title":"\ud83e\udd1d How to submit a contribution"},{"location":"CONTRIBUTING/#package-manager","text":"We use poetry as our package manager. You can install poetry by following the instructions here . Please DO NOT use pip or conda to install the dependencies. Instead, use poetry: poetry install --all-extras","title":"\ud83d\udce6 Package manager"},{"location":"CONTRIBUTING/#pre-commit","text":"To ensure our standards, make sure to install pre-commit before starting to contribute. pre-commit install","title":"\ud83d\udccc Pre-commit"},{"location":"CONTRIBUTING/#linting","text":"We use ruff to lint our code. You can run the linter by running the following command: ruff pandasai examples Make sure that the linter does not report any errors or warnings before submitting a pull request.","title":"\ud83e\uddf9 Linting"},{"location":"CONTRIBUTING/#code-format-with-black","text":"We use black to reformat the code by running the following command: black pandasai","title":"Code Format with black"},{"location":"CONTRIBUTING/#testing","text":"We use pytest to test our code. You can run the tests by running the following command: poetry run pytest Make sure that all tests pass before submitting a pull request.","title":"\ud83e\uddea Testing"},{"location":"CONTRIBUTING/#release-process","text":"At the moment, the release process is manual. We try to make frequent releases. Usually, we release a new version when we have a new feature or bugfix. A developer with admin rights to the repository will create a new release on GitHub, and then publish the new version to PyPI.","title":"\ud83d\ude80 Release Process"},{"location":"building_docs/","text":"=================================== This example shows a basic MkDocs project with Read the Docs. This project is using mkdocs with readthedocs project template. Some useful links are given below to lear and contribute in the project. \ud83d\udcda docs/ A basic MkDocs project lives in docs/ , it was generated using MkDocs defaults. All the *.md make up sections in the documentation. \u2699\ufe0f .readthedocs.yaml Read the Docs Build configuration is stored in .readthedocs.yaml . \u2699\ufe0f mkdocs.yml A basic MkDocs configuration is stored here, including a few extensions for MkDocs and Markdown. Add your own configurations here, such as extensions and themes. Remember that many extensions and themes require additional Python packages to be installed. \ud83d\udccd docs/requirements.txt and docs/requirements.in Python dependencies are pinned (uses pip-tools ) here. Make sure to add your Python dependencies to requirements.txt or if you choose pip-tools , edit docs/requirements.in and remember to run to run pip-compile docs/requirements.in . Example Project usage Poetry is the package manager for pandasai . In order to build documentation, we have to add requirements in development environment. This project has a standard MkDocs layout which is built by Read the Docs almost the same way that you would build it locally (on your own laptop!). You can build and view this documentation project locally - we recommend that you activate a Poetry environment and dependency management tool. # Install required Python dependencies (MkDocs etc.) poetry install --with docs # Run the mkdocs development server mkdocs serve Project Docs Structure If you are new to Read the Docs, you may want to refer to the Read the Docs User documentation . Below is the rundown of documentation structure for pandasai , you need to know: place your docs/ folder alongside your Python project. copy mkdocs.yml , .readthedocs.yaml and the docs/ folder into your project root. docs/API contains the API documentation created using docstring . For any new module, add the links here Project is using standard Google Docstring Style. Rebuild the documenation locally to see that it works. Documentation are hosted on Read the Docs tutorial Define the release version in mkdocs.yml file. Read the Docs tutorial To get started with Read the Docs, you may also refer to the Read the Docs tutorial . I With every release, build the documentation manually.","title":"Documents Building"},{"location":"building_docs/#example-project-usage","text":"Poetry is the package manager for pandasai . In order to build documentation, we have to add requirements in development environment. This project has a standard MkDocs layout which is built by Read the Docs almost the same way that you would build it locally (on your own laptop!). You can build and view this documentation project locally - we recommend that you activate a Poetry environment and dependency management tool. # Install required Python dependencies (MkDocs etc.) poetry install --with docs # Run the mkdocs development server mkdocs serve","title":"Example Project usage"},{"location":"building_docs/#project-docs-structure","text":"If you are new to Read the Docs, you may want to refer to the Read the Docs User documentation . Below is the rundown of documentation structure for pandasai , you need to know: place your docs/ folder alongside your Python project. copy mkdocs.yml , .readthedocs.yaml and the docs/ folder into your project root. docs/API contains the API documentation created using docstring . For any new module, add the links here Project is using standard Google Docstring Style. Rebuild the documenation locally to see that it works. Documentation are hosted on Read the Docs tutorial Define the release version in mkdocs.yml file.","title":"Project Docs Structure"},{"location":"building_docs/#read-the-docs-tutorial","text":"To get started with Read the Docs, you may also refer to the Read the Docs tutorial . I With every release, build the documentation manually.","title":"Read the Docs tutorial"},{"location":"cache/","text":"Cache PandasAI uses a cache to store the results of previous queries. This is useful for two reasons: It allows the user to quickly retrieve the results of a query without having to wait for the model to generate a response. It cuts down on the number of API calls made to the model, reducing the cost of using the model. The cache is stored in a file called cache.db in the /cache directory of the project. The cache is a SQLite database, and can be viewed using any SQLite client. The file will be created automatically when the first query is made. Disabling the cache The cache can be disabled by setting the enable_cache parameter to False when creating the PandasAI object: df = SmartDataframe('data.csv', {\"enable_cache\": False}) By default, the cache is enabled. Clearing the cache The cache can be cleared by deleting the cache.db file. The file will be recreated automatically when the next query is made. Alternatively, the cache can be cleared by calling the clear_cache() method on the PandasAI object: import pandas_ai as pai pai.clear_cache()","title":"Cache"},{"location":"cache/#cache","text":"PandasAI uses a cache to store the results of previous queries. This is useful for two reasons: It allows the user to quickly retrieve the results of a query without having to wait for the model to generate a response. It cuts down on the number of API calls made to the model, reducing the cost of using the model. The cache is stored in a file called cache.db in the /cache directory of the project. The cache is a SQLite database, and can be viewed using any SQLite client. The file will be created automatically when the first query is made.","title":"Cache"},{"location":"cache/#disabling-the-cache","text":"The cache can be disabled by setting the enable_cache parameter to False when creating the PandasAI object: df = SmartDataframe('data.csv', {\"enable_cache\": False}) By default, the cache is enabled.","title":"Disabling the cache"},{"location":"cache/#clearing-the-cache","text":"The cache can be cleared by deleting the cache.db file. The file will be recreated automatically when the next query is made. Alternatively, the cache can be cleared by calling the clear_cache() method on the PandasAI object: import pandas_ai as pai pai.clear_cache()","title":"Clearing the cache"},{"location":"callbacks/","text":"Callbacks Callbacks are functions that are called at specific points during the execution of the PandasAI class. They can be used, for example, to get the code as soon as it is generated. from pandasai import SmartDataframe from pandasai.callbacks import StdoutCallback # The callback will print the generated code to the console as soon as it is generated df = SmartDataframe(\"data.csv\", {\"callback\": StdoutCallback()}) Creating a custom callback To create a custom callback, you must inherit from the BaseCallback class and implement the methods you want to use. from pandasai import SmartDataframe from pandasai.callbacks import BaseCallback # MyCallback class MyCustomCallback(BaseCallback): def on_code(self, response: str): # Do something with the generated code ... df = SmartDataframe(\"data.csv\", {\"callback\": MyCustomCallback()}) Built-in callbacks PandasAI comes with a few built-in callbacks that can be used to modify the generated code. StdoutCallback The StdoutCallback callback prints the generated code to the console as soon as it is generated. from pandasai import SmartDataframe from pandasai.callbacks import BaseCallback, StdoutCallback # The callback will print the generated code to the console as soon as it is generated df = SmartDataframe(\"data.csv\", {\"callback\": StdoutCallback()}) FileCallback The FileCallback callback writes the generated code to a file as soon as it is generated. from pandasai import SmartDataframe from pandasai.callbacks import FileCallback # The callback will write the generated code to a file as soon as it is generated df = SmartDataframe(\"data.csv\", {\"callback\": FileCallback(\"output.py\")})","title":"Callbacks"},{"location":"callbacks/#callbacks","text":"Callbacks are functions that are called at specific points during the execution of the PandasAI class. They can be used, for example, to get the code as soon as it is generated. from pandasai import SmartDataframe from pandasai.callbacks import StdoutCallback # The callback will print the generated code to the console as soon as it is generated df = SmartDataframe(\"data.csv\", {\"callback\": StdoutCallback()})","title":"Callbacks"},{"location":"callbacks/#creating-a-custom-callback","text":"To create a custom callback, you must inherit from the BaseCallback class and implement the methods you want to use. from pandasai import SmartDataframe from pandasai.callbacks import BaseCallback # MyCallback class MyCustomCallback(BaseCallback): def on_code(self, response: str): # Do something with the generated code ... df = SmartDataframe(\"data.csv\", {\"callback\": MyCustomCallback()})","title":"Creating a custom callback"},{"location":"callbacks/#built-in-callbacks","text":"PandasAI comes with a few built-in callbacks that can be used to modify the generated code.","title":"Built-in callbacks"},{"location":"callbacks/#stdoutcallback","text":"The StdoutCallback callback prints the generated code to the console as soon as it is generated. from pandasai import SmartDataframe from pandasai.callbacks import BaseCallback, StdoutCallback # The callback will print the generated code to the console as soon as it is generated df = SmartDataframe(\"data.csv\", {\"callback\": StdoutCallback()})","title":"StdoutCallback"},{"location":"callbacks/#filecallback","text":"The FileCallback callback writes the generated code to a file as soon as it is generated. from pandasai import SmartDataframe from pandasai.callbacks import FileCallback # The callback will write the generated code to a file as soon as it is generated df = SmartDataframe(\"data.csv\", {\"callback\": FileCallback(\"output.py\")})","title":"FileCallback"},{"location":"connectors/","text":"Connectors Overview PandasAI mission is to make data analysis and manipulation more efficient and accessible to everyone. This includes making it easier to connect to data sources and to use them in your data analysis and manipulation workflow. PandasAI provides a number of connectors that allow you to connect to different data sources. These connectors are designed to be easy to use, even if you are not familiar with the data source or with PandasAI. To use a connector, you first need to install the required dependencies. You can do this by running the following command: # Using poetry (recommended) poetry add pandasai[connectors] # Using pip pip install pandasai[connectors] Have a look at the video of how to use the connectors: SQL connectors PandasAI provides connectors for the following SQL databases: PostgreSQL MySQL Generic SQL Snowflake DataBricks Yahoo Finance Additionally, PandasAI provides a generic SQL connector that can be used to connect to any SQL database. PostgreSQL connector The PostgreSQL connector allows you to connect to a PostgreSQL database. It is designed to be easy to use, even if you are not familiar with PostgreSQL or with PandasAI. To use the PostgreSQL connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors import PostgreSQLConnector postgres_connector = PostgreSQLConnector( config={ \"host\": \"localhost\", \"port\": 5432, \"database\": \"mydb\", \"username\": \"root\", \"password\": \"root\", \"table\": \"payments\", \"where\": [ # this is optional and filters the data to # reduce the size of the dataframe [\"payment_status\", \"=\", \"PAIDOFF\"], ], } ) df = SmartDataframe(postgres_connector) df.chat('What is the total amount of payments in the last year?') MySQL connector Similarly to the PostgreSQL connector, the MySQL connector allows you to connect to a MySQL database. It is designed to be easy to use, even if you are not familiar with MySQL or with PandasAI. To use the MySQL connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors import MySQLConnector mysql_connector = MySQLConnector( config={ \"host\": \"localhost\", \"port\": 3306, \"database\": \"mydb\", \"username\": \"root\", \"password\": \"root\", \"table\": \"loans\", \"where\": [ # this is optional and filters the data to # reduce the size of the dataframe [\"loan_status\", \"=\", \"PAIDOFF\"], ], } ) df = SmartDataframe(mysql_connector) df.chat('What is the total amount of loans in the last year?') Generic SQL connector The generic SQL connector allows you to connect to any SQL database that is supported by SQLAlchemy. To use the generic SQL connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors import SQLConnector sql_connector = SQLConnector( config={ \"dialect\": \"sqlite\", \"driver\": \"pysqlite\", \"host\": \"localhost\", \"port\": 3306, \"database\": \"mydb\", \"username\": \"root\", \"password\": \"root\", \"table\": \"loans\", \"where\": [ # this is optional and filters the data to # reduce the size of the dataframe [\"loan_status\", \"=\", \"PAIDOFF\"], ], } ) Snowflake connector The Snowflake connector allows you to connect to Snowflake. It is very similar to the SQL connectors, but it has some differences. To use the Snowflake connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors import SnowFlakeConnector snowflake_connector = SnowFlakeConnector( config={ \"account\": \"ehxzojy-ue47135\", \"database\": \"SNOWFLAKE_SAMPLE_DATA\", \"username\": \"test\", \"password\": \"*****\", \"table\": \"lineitem\", \"warehouse\": \"COMPUTE_WH\", \"dbSchema\": \"tpch_sf1\", \"where\": [ # this is optional and filters the data to # reduce the size of the dataframe [\"l_quantity\", \">\", \"49\"] ], } ) df = SmartDataframe(snowflake_connector) df.chat(\"How many records has status 'F'?\") DataBricks connector The DataBricks connector allows you to connect to DataBricks. It is very similar to the SQL connectors, but it has some differences. To use the DataBricks connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors import DataBricksConnector databricks_connector = DatabricksConnector( config={ \"host\": \"adb-*****.azuredatabricks.net\", \"database\": \"default\", \"token\": \"dapidfd412321\", \"port\": 443, \"table\": \"loan_payments_data\", \"httpPath\": \"/sql/1.0/warehouses/213421312\", \"where\": [ # this is optional and filters the data to # reduce the size of the dataframe [\"loan_status\", \"=\", \"PAIDOFF\"], ], } ) Yahoo Finance connector The Yahoo Finance connector allows you to connect to Yahoo Finance, by simply passing the ticker symbol of the stock you want to analyze. To use the Yahoo Finance connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors.yahoo_finance import YahooFinanceConnector yahoo_connector = YahooFinanceConnector(\"MSFT\") df = SmartDataframe(yahoo_connector) df.chat(\"What is the closing price for yesterday?\")","title":"Connectors"},{"location":"connectors/#connectors","text":"","title":"Connectors"},{"location":"connectors/#overview","text":"PandasAI mission is to make data analysis and manipulation more efficient and accessible to everyone. This includes making it easier to connect to data sources and to use them in your data analysis and manipulation workflow. PandasAI provides a number of connectors that allow you to connect to different data sources. These connectors are designed to be easy to use, even if you are not familiar with the data source or with PandasAI. To use a connector, you first need to install the required dependencies. You can do this by running the following command: # Using poetry (recommended) poetry add pandasai[connectors] # Using pip pip install pandasai[connectors] Have a look at the video of how to use the connectors:","title":"Overview"},{"location":"connectors/#sql-connectors","text":"PandasAI provides connectors for the following SQL databases: PostgreSQL MySQL Generic SQL Snowflake DataBricks Yahoo Finance Additionally, PandasAI provides a generic SQL connector that can be used to connect to any SQL database.","title":"SQL connectors"},{"location":"connectors/#postgresql-connector","text":"The PostgreSQL connector allows you to connect to a PostgreSQL database. It is designed to be easy to use, even if you are not familiar with PostgreSQL or with PandasAI. To use the PostgreSQL connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors import PostgreSQLConnector postgres_connector = PostgreSQLConnector( config={ \"host\": \"localhost\", \"port\": 5432, \"database\": \"mydb\", \"username\": \"root\", \"password\": \"root\", \"table\": \"payments\", \"where\": [ # this is optional and filters the data to # reduce the size of the dataframe [\"payment_status\", \"=\", \"PAIDOFF\"], ], } ) df = SmartDataframe(postgres_connector) df.chat('What is the total amount of payments in the last year?')","title":"PostgreSQL connector"},{"location":"connectors/#mysql-connector","text":"Similarly to the PostgreSQL connector, the MySQL connector allows you to connect to a MySQL database. It is designed to be easy to use, even if you are not familiar with MySQL or with PandasAI. To use the MySQL connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors import MySQLConnector mysql_connector = MySQLConnector( config={ \"host\": \"localhost\", \"port\": 3306, \"database\": \"mydb\", \"username\": \"root\", \"password\": \"root\", \"table\": \"loans\", \"where\": [ # this is optional and filters the data to # reduce the size of the dataframe [\"loan_status\", \"=\", \"PAIDOFF\"], ], } ) df = SmartDataframe(mysql_connector) df.chat('What is the total amount of loans in the last year?')","title":"MySQL connector"},{"location":"connectors/#generic-sql-connector","text":"The generic SQL connector allows you to connect to any SQL database that is supported by SQLAlchemy. To use the generic SQL connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors import SQLConnector sql_connector = SQLConnector( config={ \"dialect\": \"sqlite\", \"driver\": \"pysqlite\", \"host\": \"localhost\", \"port\": 3306, \"database\": \"mydb\", \"username\": \"root\", \"password\": \"root\", \"table\": \"loans\", \"where\": [ # this is optional and filters the data to # reduce the size of the dataframe [\"loan_status\", \"=\", \"PAIDOFF\"], ], } )","title":"Generic SQL connector"},{"location":"connectors/#snowflake-connector","text":"The Snowflake connector allows you to connect to Snowflake. It is very similar to the SQL connectors, but it has some differences. To use the Snowflake connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors import SnowFlakeConnector snowflake_connector = SnowFlakeConnector( config={ \"account\": \"ehxzojy-ue47135\", \"database\": \"SNOWFLAKE_SAMPLE_DATA\", \"username\": \"test\", \"password\": \"*****\", \"table\": \"lineitem\", \"warehouse\": \"COMPUTE_WH\", \"dbSchema\": \"tpch_sf1\", \"where\": [ # this is optional and filters the data to # reduce the size of the dataframe [\"l_quantity\", \">\", \"49\"] ], } ) df = SmartDataframe(snowflake_connector) df.chat(\"How many records has status 'F'?\")","title":"Snowflake connector"},{"location":"connectors/#databricks-connector","text":"The DataBricks connector allows you to connect to DataBricks. It is very similar to the SQL connectors, but it has some differences. To use the DataBricks connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors import DataBricksConnector databricks_connector = DatabricksConnector( config={ \"host\": \"adb-*****.azuredatabricks.net\", \"database\": \"default\", \"token\": \"dapidfd412321\", \"port\": 443, \"table\": \"loan_payments_data\", \"httpPath\": \"/sql/1.0/warehouses/213421312\", \"where\": [ # this is optional and filters the data to # reduce the size of the dataframe [\"loan_status\", \"=\", \"PAIDOFF\"], ], } )","title":"DataBricks connector"},{"location":"connectors/#yahoo-finance-connector","text":"The Yahoo Finance connector allows you to connect to Yahoo Finance, by simply passing the ticker symbol of the stock you want to analyze. To use the Yahoo Finance connector, you only need to import it into your Python code and pass it to a SmartDataframe or SmartDatalake object: from pandasai.connectors.yahoo_finance import YahooFinanceConnector yahoo_connector = YahooFinanceConnector(\"MSFT\") df = SmartDataframe(yahoo_connector) df.chat(\"What is the closing price for yesterday?\")","title":"Yahoo Finance connector"},{"location":"custom-head/","text":"Use custom head In some cases, you might want to share a custom sample head to the LLM. For example, you might not be willing to share potential sensitive information with the LLM. Or you might just want to provide better examples to the LLM to improve the quality of the answers. You can do so by passing a custom head to the LLM as follows: from pandasai import SmartDataframe import pandas as pd # head df head_df = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) df = SmartDataframe(\"data/country_gdp.csv\", { \"sample_head\": head_df }) Doing so will make the LLM use the head_df as the sample head instead of the first 5 rows of the dataframe.","title":"Use custom head"},{"location":"custom-head/#use-custom-head","text":"In some cases, you might want to share a custom sample head to the LLM. For example, you might not be willing to share potential sensitive information with the LLM. Or you might just want to provide better examples to the LLM to improve the quality of the answers. You can do so by passing a custom head to the LLM as follows: from pandasai import SmartDataframe import pandas as pd # head df head_df = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) df = SmartDataframe(\"data/country_gdp.csv\", { \"sample_head\": head_df }) Doing so will make the LLM use the head_df as the sample head instead of the first 5 rows of the dataframe.","title":"Use custom head"},{"location":"custom-instructions/","text":"Custom instructions In some cases, you may want to customize the instructions that are used by PandasAI. For example, you may want to use a different instruction for a specific use case to improve the results for certain types of queries. With PandasAI, you can easily customize the instructions that are used by the library. You can do this by passing a custom_instructions string in the config dictionary to the SmartDataframe constructor. Example from pandasai import SmartDataframe df = SmartDataframe(\"data.csv\", { \"custom_instructions\": \"Custom instructions for the generation of Python code\" })","title":"Custom instructions"},{"location":"custom-instructions/#custom-instructions","text":"In some cases, you may want to customize the instructions that are used by PandasAI. For example, you may want to use a different instruction for a specific use case to improve the results for certain types of queries. With PandasAI, you can easily customize the instructions that are used by the library. You can do this by passing a custom_instructions string in the config dictionary to the SmartDataframe constructor.","title":"Custom instructions"},{"location":"custom-instructions/#example","text":"from pandasai import SmartDataframe df = SmartDataframe(\"data.csv\", { \"custom_instructions\": \"Custom instructions for the generation of Python code\" })","title":"Example"},{"location":"custom-prompts/","text":"Custom prompts In some cases, you may want to customize the prompts that are used by PandasAI. For example, you may want to use a different prompt for a specific use case to improve the results for certain types of queries. With PandasAI, you can easily customize the prompts that are used by the library. You can do this by passing a prompts dictionary to the PandasAI constructor. The keys of the dictionary are the names of the prompts, and the values are the prompts themselves. There are 5 types of prompts that you can override at the moment: generate_python_code : this is the prompt that is used to generate Python code from a natural language query. PandasAI uses this prompt as the standard prompt for the first query. correct_error : this is the prompt that is used to correct the generated Python code. Whenever the code generated by PandasAI is not correct, an exception is raised and a new call to the LLM is made with this prompt to correct the error. How to create custom prompts To create your custom prompt create a new CustomPromptClass inherited from base Prompt class. from pandasai import SmartDataframe from pandasai.prompts import AbstractPrompt class MyCustomPrompt(AbstractPrompt): @property def template(self): return \"\"\"This is your custom text for your prompt with custom {my_custom_value}\"\"\" def setup(self, kwargs): # This method is called before the prompt is intialized # You can use it to setup your prompt and pass any additional # variables to the template self.set_var(\"my_custom_value\", kwargs[\"my_custom_value\"]) df = SmartDataframe(\"data.csv\", { \"custom_prompts\": { \"generate_python_code\": MyCustomPrompt( my_custom_value=\"my custom value\") } }) You can also use FileBasedPrompt in case you prefer to store prompt template in a file: my_prompt_template.tmpl: This is your custom text for your prompt with custom {my_custom_value} python code: from pandasai import SmartDataframe from pandasai.prompts import FileBasedPrompt class MyCustomFileBasedPrompt(FileBasedPrompt): _path_to_template = \"path/to/my_prompt_template.tmpl\" df = SmartDataframe(\"data.csv\", { \"custom_prompts\": { \"generate_python_code\": MyCustomFileBasedPrompt( my_custom_value=\"my custom value\") } }) Using dynamic prompt values Variable interpolation You can directly access the default prompt variables (for example dfs, conversation, etc) and call their methods from prompt text itself. from pandasai import SmartDataframe from pandasai.prompts import AbstractPrompt class MyCustomPrompt(AbstractPrompt): template = \"\"\"You are given a dataframe with number if rows equal to {dfs[0].shape[0]} and number of columns equal to {dfs[0].shape[1]} Here's the conversation: {conversation} \"\"\" df = SmartDataframe(\"data.csv\", { \"custom_prompts\": { \"generate_python_code\": MyCustomPrompt() } })","title":"Custom prompts"},{"location":"custom-prompts/#custom-prompts","text":"In some cases, you may want to customize the prompts that are used by PandasAI. For example, you may want to use a different prompt for a specific use case to improve the results for certain types of queries. With PandasAI, you can easily customize the prompts that are used by the library. You can do this by passing a prompts dictionary to the PandasAI constructor. The keys of the dictionary are the names of the prompts, and the values are the prompts themselves. There are 5 types of prompts that you can override at the moment: generate_python_code : this is the prompt that is used to generate Python code from a natural language query. PandasAI uses this prompt as the standard prompt for the first query. correct_error : this is the prompt that is used to correct the generated Python code. Whenever the code generated by PandasAI is not correct, an exception is raised and a new call to the LLM is made with this prompt to correct the error.","title":"Custom prompts"},{"location":"custom-prompts/#how-to-create-custom-prompts","text":"To create your custom prompt create a new CustomPromptClass inherited from base Prompt class. from pandasai import SmartDataframe from pandasai.prompts import AbstractPrompt class MyCustomPrompt(AbstractPrompt): @property def template(self): return \"\"\"This is your custom text for your prompt with custom {my_custom_value}\"\"\" def setup(self, kwargs): # This method is called before the prompt is intialized # You can use it to setup your prompt and pass any additional # variables to the template self.set_var(\"my_custom_value\", kwargs[\"my_custom_value\"]) df = SmartDataframe(\"data.csv\", { \"custom_prompts\": { \"generate_python_code\": MyCustomPrompt( my_custom_value=\"my custom value\") } }) You can also use FileBasedPrompt in case you prefer to store prompt template in a file: my_prompt_template.tmpl: This is your custom text for your prompt with custom {my_custom_value} python code: from pandasai import SmartDataframe from pandasai.prompts import FileBasedPrompt class MyCustomFileBasedPrompt(FileBasedPrompt): _path_to_template = \"path/to/my_prompt_template.tmpl\" df = SmartDataframe(\"data.csv\", { \"custom_prompts\": { \"generate_python_code\": MyCustomFileBasedPrompt( my_custom_value=\"my custom value\") } })","title":"How to create custom prompts"},{"location":"custom-prompts/#using-dynamic-prompt-values","text":"","title":"Using dynamic prompt values"},{"location":"custom-prompts/#variable-interpolation","text":"You can directly access the default prompt variables (for example dfs, conversation, etc) and call their methods from prompt text itself. from pandasai import SmartDataframe from pandasai.prompts import AbstractPrompt class MyCustomPrompt(AbstractPrompt): template = \"\"\"You are given a dataframe with number if rows equal to {dfs[0].shape[0]} and number of columns equal to {dfs[0].shape[1]} Here's the conversation: {conversation} \"\"\" df = SmartDataframe(\"data.csv\", { \"custom_prompts\": { \"generate_python_code\": MyCustomPrompt() } })","title":"Variable interpolation"},{"location":"custom-response/","text":"Custom Response PandasAI offers the flexibility to handle chat responses in a customized manner. By default, PandasAI includes a ResponseParser class that can be extended to modify the response output according to your needs. You have the option to provide a custom parser, such as StreamlitResponse , to the configuration object like this: Example Usage import pandas as pd from pandasai import SmartDatalake from pandasai.llm import OpenAI from pandasai.responses.response_parser import ResponseParser # This class overrides default behaviour how dataframe is returned # By Default PandasAI returns the SmartDataFrame class PandasDataFrame(ResponseParser): def __init__(self, context) -> None: super().__init__(context) def format_dataframe(self, result): # Returns Pandas Dataframe instead of SmartDataFrame return result[\"value\"] employees_df = pd.DataFrame( { \"EmployeeID\": [1, 2, 3, 4, 5], \"Name\": [\"John\", \"Emma\", \"Liam\", \"Olivia\", \"William\"], \"Department\": [\"HR\", \"Sales\", \"IT\", \"Marketing\", \"Finance\"], } ) salaries_df = pd.DataFrame( { \"EmployeeID\": [1, 2, 3, 4, 5], \"Salary\": [5000, 6000, 4500, 7000, 5500], } ) llm = OpenAI(\"OPENAI-KEY\") dl = SmartDatalake( [employees_df, salaries_df], config={\"llm\": llm, \"verbose\": True, \"response_parser\": PandasDataFrame}, ) response = dl.chat(\"Return a dataframe of name against salaries\") # Returns the response as Pandas DataFrame Streamlit Example import pandas as pd from pandasai import SmartDatalake from pandasai.llm import OpenAI from pandasai.responses.streamlit_response import StreamlitResponse employees_df = pd.DataFrame( { \"EmployeeID\": [1, 2, 3, 4, 5], \"Name\": [\"John\", \"Emma\", \"Liam\", \"Olivia\", \"William\"], \"Department\": [\"HR\", \"Sales\", \"IT\", \"Marketing\", \"Finance\"], } ) salaries_df = pd.DataFrame( { \"EmployeeID\": [1, 2, 3, 4, 5], \"Salary\": [5000, 6000, 4500, 7000, 5500], } ) llm = OpenAI() dl = SmartDatalake( [employees_df, salaries_df], config={\"llm\": llm, \"verbose\": True, \"response_parser\": StreamlitResponse}, ) dl.chat(\"Plot salaries against name\")","title":"Custom Response"},{"location":"custom-response/#custom-response","text":"PandasAI offers the flexibility to handle chat responses in a customized manner. By default, PandasAI includes a ResponseParser class that can be extended to modify the response output according to your needs. You have the option to provide a custom parser, such as StreamlitResponse , to the configuration object like this:","title":"Custom Response"},{"location":"custom-response/#example-usage","text":"import pandas as pd from pandasai import SmartDatalake from pandasai.llm import OpenAI from pandasai.responses.response_parser import ResponseParser # This class overrides default behaviour how dataframe is returned # By Default PandasAI returns the SmartDataFrame class PandasDataFrame(ResponseParser): def __init__(self, context) -> None: super().__init__(context) def format_dataframe(self, result): # Returns Pandas Dataframe instead of SmartDataFrame return result[\"value\"] employees_df = pd.DataFrame( { \"EmployeeID\": [1, 2, 3, 4, 5], \"Name\": [\"John\", \"Emma\", \"Liam\", \"Olivia\", \"William\"], \"Department\": [\"HR\", \"Sales\", \"IT\", \"Marketing\", \"Finance\"], } ) salaries_df = pd.DataFrame( { \"EmployeeID\": [1, 2, 3, 4, 5], \"Salary\": [5000, 6000, 4500, 7000, 5500], } ) llm = OpenAI(\"OPENAI-KEY\") dl = SmartDatalake( [employees_df, salaries_df], config={\"llm\": llm, \"verbose\": True, \"response_parser\": PandasDataFrame}, ) response = dl.chat(\"Return a dataframe of name against salaries\") # Returns the response as Pandas DataFrame","title":"Example Usage"},{"location":"custom-response/#streamlit-example","text":"import pandas as pd from pandasai import SmartDatalake from pandasai.llm import OpenAI from pandasai.responses.streamlit_response import StreamlitResponse employees_df = pd.DataFrame( { \"EmployeeID\": [1, 2, 3, 4, 5], \"Name\": [\"John\", \"Emma\", \"Liam\", \"Olivia\", \"William\"], \"Department\": [\"HR\", \"Sales\", \"IT\", \"Marketing\", \"Finance\"], } ) salaries_df = pd.DataFrame( { \"EmployeeID\": [1, 2, 3, 4, 5], \"Salary\": [5000, 6000, 4500, 7000, 5500], } ) llm = OpenAI() dl = SmartDatalake( [employees_df, salaries_df], config={\"llm\": llm, \"verbose\": True, \"response_parser\": StreamlitResponse}, ) dl.chat(\"Plot salaries against name\")","title":"Streamlit Example"},{"location":"custom-whitelisted-dependencies/","text":"Custom whitelisted dependencies By default, PandasAI only allows to run code that uses some whitelisted modules. This is to prevent malicious code from being executed on the server or locally. However, it is possible to add custom modules to the whitelist. This can be done by passing a list of modules to the custom_whitelisted_dependencies parameter when instantiating the SmartDataframe or SmartDatalake class. from pandasai import SmartDataframe df = SmartDataframe(\"data.csv\", { \"custom_whitelisted_dependencies\": [\"any_module\"] }) The custom_whitelisted_dependencies parameter accepts a list of strings, where each string is the name of a module. The module must be installed in the environment where PandasAI is running. Please, make sure you have installed the module in the environment where PandasAI is running. Otherwise, you will get an error when trying to run the code.","title":"Custom whitelisted dependencies"},{"location":"custom-whitelisted-dependencies/#custom-whitelisted-dependencies","text":"By default, PandasAI only allows to run code that uses some whitelisted modules. This is to prevent malicious code from being executed on the server or locally. However, it is possible to add custom modules to the whitelist. This can be done by passing a list of modules to the custom_whitelisted_dependencies parameter when instantiating the SmartDataframe or SmartDatalake class. from pandasai import SmartDataframe df = SmartDataframe(\"data.csv\", { \"custom_whitelisted_dependencies\": [\"any_module\"] }) The custom_whitelisted_dependencies parameter accepts a list of strings, where each string is the name of a module. The module must be installed in the environment where PandasAI is running. Please, make sure you have installed the module in the environment where PandasAI is running. Otherwise, you will get an error when trying to run the code.","title":"Custom whitelisted dependencies"},{"location":"examples/","text":"Examples Some examples of using PandasAI with different data sources. Other examples are included in the repository along with samples of data. Working with pandas dataframes Example of using PandasAI with a Pandas DataFrame from pandasai import SmartDataframe import pandas as pd # pandas dataframe df = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) # convert to SmartDataframe df = SmartDataframe(df) df.chat('Calculate the sum of the gdp of north american countries') print(response) # Output: 20901884461056 Working with CSVs Example of using PandasAI with a CSV file from pandasai import SmartDataframe # You can instantiate a SmartDataframe with a path to a CSV file df = Smartdataframe(\"data/Loan payments data.csv\") df.chat(\"How many loans are from men and have been paid off?\") print(response) # Output: 247 loans have been paid off by men. Working with Excel files Example of using PandasAI with an Excel file. In order to use Excel files as a data source, you need to install the pandasai[excel] extra dependency. pip install pandasai[excel] Then, you can use PandasAI with an Excel file as follows: from pandasai import SmartDataframe # You can instantiate a SmartDataframe with a path to an Excel file df = Smartdataframe(\"data/Loan payments data.xlsx\") df.chat(\"How many loans are from men and have been paid off?\") print(response) # Output: 247 loans have been paid off by men. Working with Google Sheets Example of using PandasAI with a Google Sheet. In order to use Google Sheets as a data source, you need to install the pandasai[google-sheet] extra dependency. pip install pandasai[google-sheet] Then, you can use PandasAI with a Google Sheet as follows: from pandasai import SmartDataframe # You can instantiate a SmartDataframe with a path to a Google Sheet df = Smartdataframe(\"https://docs.google.com/spreadsheets/d/fake/edit#gid=0\") df.chat(\"How many loans are from men and have been paid off?\") print(response) # Output: 247 loans have been paid off by men. Remember that at the moment, you need to make sure that the Google Sheet is public. Working with Polars dataframes Example of using PandasAI with a Polars DataFrame (still in beta). In order to use Polars dataframes as a data source, you need to install the pandasai[polars] extra dependency. pip install pandasai[polars] Then, you can use PandasAI with a Polars DataFrame as follows: from pandasai import SmartDataframe import polars as pl # You can instantiate a SmartDataframe with a Polars DataFrame df = pd.DataFrame([ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] ]) df.chat(\"How many loans are from men and have been paid off?\") print(response) # Output: 247 loans have been paid off by men. Plotting Example of using PandasAI to generate a chart from a Pandas DataFrame from pandasai import SmartDataframe df = SmartDataframe(\"data/Countries.csv\") response = df.chat( \"Plot the histogram of countries showing for each the gpd, using different colors for each bar\" ) print(response) # Output: check out images/histogram-chart.png Saving Plots with User Defined Path You can pass a custom path to save the charts. The path must be a valid global path. Below is the example to Save Charts with user defined location. from pandasai import SmartDataframe user_defined_path = os.getcwd() df = SmartDataframe(\"data/Countries.csv\", { \"save_charts\": True, \"save_charts_path\": user_defined_path, }) response = df.chat( \"Plot the histogram of countries showing for each the gpd,\" \" using different colors for each bar\", ) print(response) # Output: check out $pwd/exports/charts/{hashid}/chart.png Working with multiple dataframes (with SmartDatalake) Example of using PandasAI with multiple dataframes. In order to use multiple dataframes as a data source, you need to use a SmartDatalake instead of a SmartDataframe . You can instantiate a SmartDatalake as follows: from pandasai import SmartDatalake import pandas as pd employees_data = { 'EmployeeID': [1, 2, 3, 4, 5], 'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'], 'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance'] } salaries_data = { 'EmployeeID': [1, 2, 3, 4, 5], 'Salary': [5000, 6000, 4500, 7000, 5500] } employees_df = pd.DataFrame(employees_data) salaries_df = pd.DataFrame(salaries_data) df = SmartDatalake([employees_df, salaries_df]) response = df.chat(\"Who gets paid the most?\") print(response) # Output: Olivia gets paid the most. Chain of commands You can chain commands by passing the output of one command to the next one. In the example, we first filter the original dataframe by gender and then by loans that have been paid off. from pandasai import SmartDataframe df = SmartDataframe(\"data/Loan payments data.csv\") # We filter by males only from_males_df = df.chat(\"Filter the dataframe by women\") # We filter by loans that have been paid off paid_from_males_df = from_males_df.chat(\"Filter the dataframe by loans that have been paid off\") print(paid_from_males_df) # Output: # [247 rows x 11 columns] # Loan_ID loan_status Principal terms effective_date due_date paid_off_time past_due_days age education Gender # 0 xqd20166231 PAIDOFF 1000 30 9/8/2016 10/7/2016 9/14/2016 19:31 NaN 45 High School or Below male # 3 xqd20160004 PAIDOFF 1000 15 9/8/2016 9/22/2016 9/22/2016 20:00 NaN 27 college male # 5 xqd20160706 PAIDOFF 300 7 9/9/2016 9/15/2016 9/9/2016 13:45 NaN 35 Master or Above male # 6 xqd20160007 PAIDOFF 1000 30 9/9/2016 10/8/2016 10/7/2016 23:07 NaN 29 college male # 7 xqd20160008 PAIDOFF 1000 30 9/9/2016 10/8/2016 10/5/2016 20:33 NaN 36 college male # .. ... ... ... ... ... ... ... ... ... ... ... # 294 xqd20160295 PAIDOFF 1000 30 9/14/2016 10/13/2016 10/13/2016 13:00 NaN 36 Bechalor male # 296 xqd20160297 PAIDOFF 800 15 9/14/2016 9/28/2016 9/21/2016 4:42 NaN 27 college male # 297 xqd20160298 PAIDOFF 1000 30 9/14/2016 10/13/2016 10/13/2016 9:00 NaN 29 High School or Below male # 298 xqd20160299 PAIDOFF 1000 30 9/14/2016 10/13/2016 10/13/2016 9:00 NaN 40 High School or Below male # 299 xqd20160300 PAIDOFF 1000 30 9/14/2016 10/13/2016 10/13/2016 11:00 NaN 28 college male # [247 rows x 11 columns] Working with Agent With the chat agent, you can engage in dynamic conversations where the agent retains context throughout the discussion. This enables you to have more interactive and meaningful exchanges. Key Features Context Retention: The agent remembers the conversation history, allowing for seamless, context-aware interactions. Clarification Questions: You can use the clarification_questions method to request clarification on any aspect of the conversation. This helps ensure you fully understand the information provided. Explanation: The explain method is available to obtain detailed explanations of how the agent arrived at a particular solution or response. It offers transparency and insights into the agent's decision-making process. Feel free to initiate conversations, seek clarifications, and explore explanations to enhance your interactions with the chat agent! import pandas as pd from pandasai import Agent from pandasai.llm.openai import OpenAI employees_data = { \"EmployeeID\": [1, 2, 3, 4, 5], \"Name\": [\"John\", \"Emma\", \"Liam\", \"Olivia\", \"William\"], \"Department\": [\"HR\", \"Sales\", \"IT\", \"Marketing\", \"Finance\"], } salaries_data = { \"EmployeeID\": [1, 2, 3, 4, 5], \"Salary\": [5000, 6000, 4500, 7000, 5500], } employees_df = pd.DataFrame(employees_data) salaries_df = pd.DataFrame(salaries_data) llm = OpenAI(\"OpenAI_API_KEY\") agent = Agent([employees_df, salaries_df], config={\"llm\": llm}, memory_size=10) # Chat with the agent response = agent.chat(\"Who gets paid the most?\") print(response) # Get Clarification Questions questions = agent.clarification_questions() for question in questions: print(question) # Explain how the chat response is generated response = agent.explain() print(response)","title":"Examples"},{"location":"examples/#examples","text":"Some examples of using PandasAI with different data sources. Other examples are included in the repository along with samples of data.","title":"Examples"},{"location":"examples/#working-with-pandas-dataframes","text":"Example of using PandasAI with a Pandas DataFrame from pandasai import SmartDataframe import pandas as pd # pandas dataframe df = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) # convert to SmartDataframe df = SmartDataframe(df) df.chat('Calculate the sum of the gdp of north american countries') print(response) # Output: 20901884461056","title":"Working with pandas dataframes"},{"location":"examples/#working-with-csvs","text":"Example of using PandasAI with a CSV file from pandasai import SmartDataframe # You can instantiate a SmartDataframe with a path to a CSV file df = Smartdataframe(\"data/Loan payments data.csv\") df.chat(\"How many loans are from men and have been paid off?\") print(response) # Output: 247 loans have been paid off by men.","title":"Working with CSVs"},{"location":"examples/#working-with-excel-files","text":"Example of using PandasAI with an Excel file. In order to use Excel files as a data source, you need to install the pandasai[excel] extra dependency. pip install pandasai[excel] Then, you can use PandasAI with an Excel file as follows: from pandasai import SmartDataframe # You can instantiate a SmartDataframe with a path to an Excel file df = Smartdataframe(\"data/Loan payments data.xlsx\") df.chat(\"How many loans are from men and have been paid off?\") print(response) # Output: 247 loans have been paid off by men.","title":"Working with Excel files"},{"location":"examples/#working-with-google-sheets","text":"Example of using PandasAI with a Google Sheet. In order to use Google Sheets as a data source, you need to install the pandasai[google-sheet] extra dependency. pip install pandasai[google-sheet] Then, you can use PandasAI with a Google Sheet as follows: from pandasai import SmartDataframe # You can instantiate a SmartDataframe with a path to a Google Sheet df = Smartdataframe(\"https://docs.google.com/spreadsheets/d/fake/edit#gid=0\") df.chat(\"How many loans are from men and have been paid off?\") print(response) # Output: 247 loans have been paid off by men. Remember that at the moment, you need to make sure that the Google Sheet is public.","title":"Working with Google Sheets"},{"location":"examples/#working-with-polars-dataframes","text":"Example of using PandasAI with a Polars DataFrame (still in beta). In order to use Polars dataframes as a data source, you need to install the pandasai[polars] extra dependency. pip install pandasai[polars] Then, you can use PandasAI with a Polars DataFrame as follows: from pandasai import SmartDataframe import polars as pl # You can instantiate a SmartDataframe with a Polars DataFrame df = pd.DataFrame([ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] ]) df.chat(\"How many loans are from men and have been paid off?\") print(response) # Output: 247 loans have been paid off by men.","title":"Working with Polars dataframes"},{"location":"examples/#plotting","text":"Example of using PandasAI to generate a chart from a Pandas DataFrame from pandasai import SmartDataframe df = SmartDataframe(\"data/Countries.csv\") response = df.chat( \"Plot the histogram of countries showing for each the gpd, using different colors for each bar\" ) print(response) # Output: check out images/histogram-chart.png","title":"Plotting"},{"location":"examples/#saving-plots-with-user-defined-path","text":"You can pass a custom path to save the charts. The path must be a valid global path. Below is the example to Save Charts with user defined location. from pandasai import SmartDataframe user_defined_path = os.getcwd() df = SmartDataframe(\"data/Countries.csv\", { \"save_charts\": True, \"save_charts_path\": user_defined_path, }) response = df.chat( \"Plot the histogram of countries showing for each the gpd,\" \" using different colors for each bar\", ) print(response) # Output: check out $pwd/exports/charts/{hashid}/chart.png","title":"Saving Plots with User Defined Path"},{"location":"examples/#working-with-multiple-dataframes-with-smartdatalake","text":"Example of using PandasAI with multiple dataframes. In order to use multiple dataframes as a data source, you need to use a SmartDatalake instead of a SmartDataframe . You can instantiate a SmartDatalake as follows: from pandasai import SmartDatalake import pandas as pd employees_data = { 'EmployeeID': [1, 2, 3, 4, 5], 'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'], 'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance'] } salaries_data = { 'EmployeeID': [1, 2, 3, 4, 5], 'Salary': [5000, 6000, 4500, 7000, 5500] } employees_df = pd.DataFrame(employees_data) salaries_df = pd.DataFrame(salaries_data) df = SmartDatalake([employees_df, salaries_df]) response = df.chat(\"Who gets paid the most?\") print(response) # Output: Olivia gets paid the most.","title":"Working with multiple dataframes (with SmartDatalake)"},{"location":"examples/#chain-of-commands","text":"You can chain commands by passing the output of one command to the next one. In the example, we first filter the original dataframe by gender and then by loans that have been paid off. from pandasai import SmartDataframe df = SmartDataframe(\"data/Loan payments data.csv\") # We filter by males only from_males_df = df.chat(\"Filter the dataframe by women\") # We filter by loans that have been paid off paid_from_males_df = from_males_df.chat(\"Filter the dataframe by loans that have been paid off\") print(paid_from_males_df) # Output: # [247 rows x 11 columns] # Loan_ID loan_status Principal terms effective_date due_date paid_off_time past_due_days age education Gender # 0 xqd20166231 PAIDOFF 1000 30 9/8/2016 10/7/2016 9/14/2016 19:31 NaN 45 High School or Below male # 3 xqd20160004 PAIDOFF 1000 15 9/8/2016 9/22/2016 9/22/2016 20:00 NaN 27 college male # 5 xqd20160706 PAIDOFF 300 7 9/9/2016 9/15/2016 9/9/2016 13:45 NaN 35 Master or Above male # 6 xqd20160007 PAIDOFF 1000 30 9/9/2016 10/8/2016 10/7/2016 23:07 NaN 29 college male # 7 xqd20160008 PAIDOFF 1000 30 9/9/2016 10/8/2016 10/5/2016 20:33 NaN 36 college male # .. ... ... ... ... ... ... ... ... ... ... ... # 294 xqd20160295 PAIDOFF 1000 30 9/14/2016 10/13/2016 10/13/2016 13:00 NaN 36 Bechalor male # 296 xqd20160297 PAIDOFF 800 15 9/14/2016 9/28/2016 9/21/2016 4:42 NaN 27 college male # 297 xqd20160298 PAIDOFF 1000 30 9/14/2016 10/13/2016 10/13/2016 9:00 NaN 29 High School or Below male # 298 xqd20160299 PAIDOFF 1000 30 9/14/2016 10/13/2016 10/13/2016 9:00 NaN 40 High School or Below male # 299 xqd20160300 PAIDOFF 1000 30 9/14/2016 10/13/2016 10/13/2016 11:00 NaN 28 college male # [247 rows x 11 columns]","title":"Chain of commands"},{"location":"examples/#working-with-agent","text":"With the chat agent, you can engage in dynamic conversations where the agent retains context throughout the discussion. This enables you to have more interactive and meaningful exchanges. Key Features Context Retention: The agent remembers the conversation history, allowing for seamless, context-aware interactions. Clarification Questions: You can use the clarification_questions method to request clarification on any aspect of the conversation. This helps ensure you fully understand the information provided. Explanation: The explain method is available to obtain detailed explanations of how the agent arrived at a particular solution or response. It offers transparency and insights into the agent's decision-making process. Feel free to initiate conversations, seek clarifications, and explore explanations to enhance your interactions with the chat agent! import pandas as pd from pandasai import Agent from pandasai.llm.openai import OpenAI employees_data = { \"EmployeeID\": [1, 2, 3, 4, 5], \"Name\": [\"John\", \"Emma\", \"Liam\", \"Olivia\", \"William\"], \"Department\": [\"HR\", \"Sales\", \"IT\", \"Marketing\", \"Finance\"], } salaries_data = { \"EmployeeID\": [1, 2, 3, 4, 5], \"Salary\": [5000, 6000, 4500, 7000, 5500], } employees_df = pd.DataFrame(employees_data) salaries_df = pd.DataFrame(salaries_data) llm = OpenAI(\"OpenAI_API_KEY\") agent = Agent([employees_df, salaries_df], config={\"llm\": llm}, memory_size=10) # Chat with the agent response = agent.chat(\"Who gets paid the most?\") print(response) # Get Clarification Questions questions = agent.clarification_questions() for question in questions: print(question) # Explain how the chat response is generated response = agent.explain() print(response)","title":"Working with Agent"},{"location":"getting-started/","text":"Usage Installation To use pandasai , first install it # Using poetry (recommended) poetry add pandasai # Using pip pip install pandasai Before you install it, we recommended to create a Virtual environment using your preffred choice of Environment Managers e.g Poetry , Pipenv , Conda , Virtualenv , Venv etc. Optional Installs To keep the package size small, we have decided to make some dependencies that are not required by default. These dependencies are required for some features of pandasai . To install pandasai with these extra dependencies, run pip install pandasai[extra-dependency-name] You can replace extra-dependency-name with any of the following: google-aip : this extra dependency is required if you want to use Google PaLM as a language model. google-sheet : this extra dependency is required if you want to use Google Sheets as a data source. excel : this extra dependency is required if you want to use Excel files as a data source. polars : this extra dependency is required if you want to use Polars dataframes as a data source. langchain : this extra dependency is required if you want to support the LangChain LLMs. numpy : this extra dependency is required if you want to support numpy. ggplot : this extra dependency is required if you want to support ggplot for plotting. seaborn : this extra dependency is required if you want to support seaborn for plotting. plotly : this extra dependency is required if you want to support plotly for plotting. statsmodels : this extra dependency is required if you want to support statsmodels. scikit-learn : this extra dependency is required if you want to support scikit-learn. streamlit : this extra dependency is required if you want to support the streamlit. SmartDataframe Below is simple example to get started with pandasai . import pandas as pd from pandasai import SmartDataframe # Sample DataFrame df = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) # Instantiate a LLM from pandasai.llm import OpenAI llm = OpenAI(api_token=\"YOUR_API_TOKEN\") df = SmartDataframe(df, config={\"llm\": llm}) df.chat('Which are the 5 happiest countries?') # Output: United Kingdom, Canada, Australia, United States, Germany If you want to get to know more about the SmartDataframe class, check out this video: How to generate OpenAI API Token Users are required to generate YOUR_API_TOKEN . Follow below simple steps to generate your API_TOKEN with openai . Go to https://openai.com/api/ and signup with your email address or connect your Google Account. Go to View API Keys on left side of your Personal Account Settings Select Create new Secret key The API access to openai is a paid service. You have to set up billing. Read the Pricing information before experimenting. SmartDatalake PandasAI also supports queries with multiple dataframes. To perform such queries, you can use a SmartDatalake instead of a SmartDataframe . A SmartDatalake is a collection of SmartDataframe s. You can instantiate a SmartDatalake as follows: from pandasai import SmartDatalake import pandas as pd # Sample DataFrames df1 = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) df2 = \"data/Loan payments data.csv\" df3 = \"data/Loan payments data.xlsx\" dl = SmartDatalake([df1, df2, df3]) Then, you can use the SmartDatalake as follows, similar to how you would use a SmartDataframe : dl.chat('Which are the 5 happiest countries?') # Output: United Kingdom, Canada, Australia, United States, Germany PandasAI will automatically figure out which dataframe or dataframes are relevant to the query and will use only those dataframes to answer the query. Agent PandasAI also supports agents. While a SmartDataframe or a SmartDatalake can be used to answer a single query and are meant to be used in a single session and for exploratory data analysis, an agent can be used for multi-turn conversations and for production use cases. You can instantiate an agent as follows: from pandasai import Agent import pandas as pd # Sample DataFrames df1 = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) agent = Agent([df1]) Then, you can use the agent as follows: agent.chat('Which are the 5 happiest countries?') # Output: United Kingdom, Canada, Australia, United States, Germany Contrary to a SmartDataframe or a SmartDatalake , an agent will keep track of the state of the conversation and will be able to answer multi-turn conversations. For example: agent.chat('And what is the GDP of these countries?') # Output: 2891615567872, 1607402389504, 1490967855104, 19294482071552, 3435817336832 Clarification questions An agent will also be able to ask clarification questions if it does not have enough information to answer the query. For example: agent.clarification_question('What is the GDP of the United States?') this will return up to 3 clarification questions that the agent can ask to the user to get more information to answer the query. Explanation An agent will also be able to explain the answer to the user. For example: response = agent.chat('What is the GDP of the United States?') explanation = agent.explain() print(\"The answer is\", response) print(\"The explanation is\", explanation) Rephrase Question Rephrase question to get accurate and comprehensive response from the model. For example: rephrased_query = agent.rephrase_query('What is the GDP of the United States?') print(\"The answer is\", rephrased_query) Config When you instantiate a SmartDataframe , you can pass a config object as the second argument. This object can contain custom settings that will be used by pandasai when generating code. As an alternative, you can simply edit the pandasai.json file in the root of your project. This file will be automatically loaded by pandasai and these will be the default settings. You will still be able to override these settings by passing the settings that you want to override when instantiating a SmartDataframe . Settings: llm : the LLM to use. You can pass an instance of an LLM or the name of an LLM. You can use one of the LLMs supported. You can find more information about LLMs here . llm_options : the options to use for the LLM (for example the api token, etc). You can find more information about the settings here . save_logs : whether to save the logs of the LLM. Defaults to True . You will find the logs in the pandasai.log file in the root of your project. verbose : whether to print the logs in the console as PandasAI is executed. Defaults to False . enforce_privacy : whether to enforce privacy. Defaults to False . If set to True , PandasAI will not send any data to the LLM, but only the metadata. By default, PandasAI will send 5 samples that are anonymized to improve the accuracy of the results. save_charts : whether to save the charts generated by PandasAI. Defaults to False . You will find the charts in the root of your project or in the path specified by save_charts_path . save_charts_path : the path where to save the charts. Defaults to exports/charts/ . You can use this setting to override the default path. enable_cache : whether to enable caching. Defaults to True . If set to True , PandasAI will cache the results of the LLM to improve the response time. If set to False , PandasAI will always call the LLM. use_error_correction_framework : whether to use the error correction framework. Defaults to True . If set to True , PandasAI will try to correct the errors in the code generated by the LLM with further calls to the LLM. If set to False , PandasAI will not try to correct the errors in the code generated by the LLM. max_retries : the maximum number of retries to use when using the error correction framework. Defaults to 3 . You can use this setting to override the default number of retries. custom_prompts : the custom prompts to use. Defaults to {} . You can use this setting to override the default custom prompts. You can find more information about custom prompts here . custom_whitelisted_dependencies : the custom whitelisted dependencies to use. Defaults to {} . You can use this setting to override the default custom whitelisted dependencies. You can find more information about custom whitelisted dependencies here . middlewares : the middlewares to use. Defaults to [] . You can use this setting to override the default middlewares. You can find more information about middlewares here . callback : the callback to use. Defaults to None . You can use this setting to override the default callback. You can find more information about callbacks here . Demo in Google Colab Try out PandasAI in your browser: Examples You can find some examples here .","title":"Getting Started"},{"location":"getting-started/#usage","text":"","title":"Usage"},{"location":"getting-started/#installation","text":"To use pandasai , first install it # Using poetry (recommended) poetry add pandasai # Using pip pip install pandasai Before you install it, we recommended to create a Virtual environment using your preffred choice of Environment Managers e.g Poetry , Pipenv , Conda , Virtualenv , Venv etc.","title":"Installation"},{"location":"getting-started/#optional-installs","text":"To keep the package size small, we have decided to make some dependencies that are not required by default. These dependencies are required for some features of pandasai . To install pandasai with these extra dependencies, run pip install pandasai[extra-dependency-name] You can replace extra-dependency-name with any of the following: google-aip : this extra dependency is required if you want to use Google PaLM as a language model. google-sheet : this extra dependency is required if you want to use Google Sheets as a data source. excel : this extra dependency is required if you want to use Excel files as a data source. polars : this extra dependency is required if you want to use Polars dataframes as a data source. langchain : this extra dependency is required if you want to support the LangChain LLMs. numpy : this extra dependency is required if you want to support numpy. ggplot : this extra dependency is required if you want to support ggplot for plotting. seaborn : this extra dependency is required if you want to support seaborn for plotting. plotly : this extra dependency is required if you want to support plotly for plotting. statsmodels : this extra dependency is required if you want to support statsmodels. scikit-learn : this extra dependency is required if you want to support scikit-learn. streamlit : this extra dependency is required if you want to support the streamlit.","title":"Optional Installs"},{"location":"getting-started/#smartdataframe","text":"Below is simple example to get started with pandasai . import pandas as pd from pandasai import SmartDataframe # Sample DataFrame df = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) # Instantiate a LLM from pandasai.llm import OpenAI llm = OpenAI(api_token=\"YOUR_API_TOKEN\") df = SmartDataframe(df, config={\"llm\": llm}) df.chat('Which are the 5 happiest countries?') # Output: United Kingdom, Canada, Australia, United States, Germany If you want to get to know more about the SmartDataframe class, check out this video:","title":"SmartDataframe"},{"location":"getting-started/#how-to-generate-openai-api-token","text":"Users are required to generate YOUR_API_TOKEN . Follow below simple steps to generate your API_TOKEN with openai . Go to https://openai.com/api/ and signup with your email address or connect your Google Account. Go to View API Keys on left side of your Personal Account Settings Select Create new Secret key The API access to openai is a paid service. You have to set up billing. Read the Pricing information before experimenting.","title":"How to generate OpenAI API Token"},{"location":"getting-started/#smartdatalake","text":"PandasAI also supports queries with multiple dataframes. To perform such queries, you can use a SmartDatalake instead of a SmartDataframe . A SmartDatalake is a collection of SmartDataframe s. You can instantiate a SmartDatalake as follows: from pandasai import SmartDatalake import pandas as pd # Sample DataFrames df1 = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) df2 = \"data/Loan payments data.csv\" df3 = \"data/Loan payments data.xlsx\" dl = SmartDatalake([df1, df2, df3]) Then, you can use the SmartDatalake as follows, similar to how you would use a SmartDataframe : dl.chat('Which are the 5 happiest countries?') # Output: United Kingdom, Canada, Australia, United States, Germany PandasAI will automatically figure out which dataframe or dataframes are relevant to the query and will use only those dataframes to answer the query.","title":"SmartDatalake"},{"location":"getting-started/#agent","text":"PandasAI also supports agents. While a SmartDataframe or a SmartDatalake can be used to answer a single query and are meant to be used in a single session and for exploratory data analysis, an agent can be used for multi-turn conversations and for production use cases. You can instantiate an agent as follows: from pandasai import Agent import pandas as pd # Sample DataFrames df1 = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) agent = Agent([df1]) Then, you can use the agent as follows: agent.chat('Which are the 5 happiest countries?') # Output: United Kingdom, Canada, Australia, United States, Germany Contrary to a SmartDataframe or a SmartDatalake , an agent will keep track of the state of the conversation and will be able to answer multi-turn conversations. For example: agent.chat('And what is the GDP of these countries?') # Output: 2891615567872, 1607402389504, 1490967855104, 19294482071552, 3435817336832","title":"Agent"},{"location":"getting-started/#clarification-questions","text":"An agent will also be able to ask clarification questions if it does not have enough information to answer the query. For example: agent.clarification_question('What is the GDP of the United States?') this will return up to 3 clarification questions that the agent can ask to the user to get more information to answer the query.","title":"Clarification questions"},{"location":"getting-started/#explanation","text":"An agent will also be able to explain the answer to the user. For example: response = agent.chat('What is the GDP of the United States?') explanation = agent.explain() print(\"The answer is\", response) print(\"The explanation is\", explanation)","title":"Explanation"},{"location":"getting-started/#rephrase-question","text":"Rephrase question to get accurate and comprehensive response from the model. For example: rephrased_query = agent.rephrase_query('What is the GDP of the United States?') print(\"The answer is\", rephrased_query)","title":"Rephrase Question"},{"location":"getting-started/#config","text":"When you instantiate a SmartDataframe , you can pass a config object as the second argument. This object can contain custom settings that will be used by pandasai when generating code. As an alternative, you can simply edit the pandasai.json file in the root of your project. This file will be automatically loaded by pandasai and these will be the default settings. You will still be able to override these settings by passing the settings that you want to override when instantiating a SmartDataframe . Settings: llm : the LLM to use. You can pass an instance of an LLM or the name of an LLM. You can use one of the LLMs supported. You can find more information about LLMs here . llm_options : the options to use for the LLM (for example the api token, etc). You can find more information about the settings here . save_logs : whether to save the logs of the LLM. Defaults to True . You will find the logs in the pandasai.log file in the root of your project. verbose : whether to print the logs in the console as PandasAI is executed. Defaults to False . enforce_privacy : whether to enforce privacy. Defaults to False . If set to True , PandasAI will not send any data to the LLM, but only the metadata. By default, PandasAI will send 5 samples that are anonymized to improve the accuracy of the results. save_charts : whether to save the charts generated by PandasAI. Defaults to False . You will find the charts in the root of your project or in the path specified by save_charts_path . save_charts_path : the path where to save the charts. Defaults to exports/charts/ . You can use this setting to override the default path. enable_cache : whether to enable caching. Defaults to True . If set to True , PandasAI will cache the results of the LLM to improve the response time. If set to False , PandasAI will always call the LLM. use_error_correction_framework : whether to use the error correction framework. Defaults to True . If set to True , PandasAI will try to correct the errors in the code generated by the LLM with further calls to the LLM. If set to False , PandasAI will not try to correct the errors in the code generated by the LLM. max_retries : the maximum number of retries to use when using the error correction framework. Defaults to 3 . You can use this setting to override the default number of retries. custom_prompts : the custom prompts to use. Defaults to {} . You can use this setting to override the default custom prompts. You can find more information about custom prompts here . custom_whitelisted_dependencies : the custom whitelisted dependencies to use. Defaults to {} . You can use this setting to override the default custom whitelisted dependencies. You can find more information about custom whitelisted dependencies here . middlewares : the middlewares to use. Defaults to [] . You can use this setting to override the default middlewares. You can find more information about middlewares here . callback : the callback to use. Defaults to None . You can use this setting to override the default callback. You can find more information about callbacks here .","title":"Config"},{"location":"getting-started/#demo-in-google-colab","text":"Try out PandasAI in your browser:","title":"Demo in Google Colab"},{"location":"getting-started/#examples","text":"You can find some examples here .","title":"Examples"},{"location":"license/","text":"Copyright (c) Gabriele Venturi Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"middlewares/","text":"Middlewares Middlewares are used to modify the code generated by PandasAI before it is executed. This allows the user to add custom functionality to the generated code or tailor the generated code to their specific use case. You can pass a list of middlewares to the SmartDataFrame or SmartDatalake constructor: # Dataframe df = SmartDataFrame(\"data.csv\", {\"middlewares\": [middleware1, middleware2]}) # Datalake dl = SmartDatalake([\"data.csv\", \"data2.csv\"], {\"middlewares\": [middleware1, middleware2]}) or you can add middlewares to the SmartDataFrame or SmartDatalake instance after it has been created: # Dataframe df = SmartDataFrame(\"data.csv\") df.add_middlewares(middleware1) df.add_middlewares(middleware2) # Datalake dl = SmartDatalake(\"data.csv\") dl.add_middlewares(middleware1) dl.add_middlewares(middleware2) Creating a custom middleware A middleware is a class that implements the Middleware interface. The interface has one method, run , which takes a code parameter and returns a modified version of the code. class MyCustomMiddleware: def run(self, code: str) -> str: # do some changes to the code return code Example The following example shows how to create a middleware that adds a print statement to the generated code: from pandasai.middlewares import Middleware class PrintMiddleware(Middleware): def run(self, code: str) -> str: return f\"print('Hello, world!')\\n{code}\" The middleware can then be used as follows: df = SmartDataframe(\"data.csv\", {\"middlewares\": [PrintMiddleware()]}) Built-in middlewares pandasai comes with a few built-in middlewares that can be used to modify the generated code. StreamlitMiddleware The StreamlitMiddleware middleware adds the compatibility required to run the generated code in a Streamlit app. from pandasai import SmartDataframe from pandasai.middlewares import StreamlitMiddleware df = SmartDataframe(\"data.csv\", {\"middlewares\": [StreamlitMiddleware()]})","title":"Middlewares"},{"location":"middlewares/#middlewares","text":"Middlewares are used to modify the code generated by PandasAI before it is executed. This allows the user to add custom functionality to the generated code or tailor the generated code to their specific use case. You can pass a list of middlewares to the SmartDataFrame or SmartDatalake constructor: # Dataframe df = SmartDataFrame(\"data.csv\", {\"middlewares\": [middleware1, middleware2]}) # Datalake dl = SmartDatalake([\"data.csv\", \"data2.csv\"], {\"middlewares\": [middleware1, middleware2]}) or you can add middlewares to the SmartDataFrame or SmartDatalake instance after it has been created: # Dataframe df = SmartDataFrame(\"data.csv\") df.add_middlewares(middleware1) df.add_middlewares(middleware2) # Datalake dl = SmartDatalake(\"data.csv\") dl.add_middlewares(middleware1) dl.add_middlewares(middleware2)","title":"Middlewares"},{"location":"middlewares/#creating-a-custom-middleware","text":"A middleware is a class that implements the Middleware interface. The interface has one method, run , which takes a code parameter and returns a modified version of the code. class MyCustomMiddleware: def run(self, code: str) -> str: # do some changes to the code return code","title":"Creating a custom middleware"},{"location":"middlewares/#example","text":"The following example shows how to create a middleware that adds a print statement to the generated code: from pandasai.middlewares import Middleware class PrintMiddleware(Middleware): def run(self, code: str) -> str: return f\"print('Hello, world!')\\n{code}\" The middleware can then be used as follows: df = SmartDataframe(\"data.csv\", {\"middlewares\": [PrintMiddleware()]})","title":"Example"},{"location":"middlewares/#built-in-middlewares","text":"pandasai comes with a few built-in middlewares that can be used to modify the generated code.","title":"Built-in middlewares"},{"location":"middlewares/#streamlitmiddleware","text":"The StreamlitMiddleware middleware adds the compatibility required to run the generated code in a Streamlit app. from pandasai import SmartDataframe from pandasai.middlewares import StreamlitMiddleware df = SmartDataframe(\"data.csv\", {\"middlewares\": [StreamlitMiddleware()]})","title":"StreamlitMiddleware"},{"location":"release-notes/","text":"Release Process At the moment, the release process is manual. We try to make frequent releases. Usually, we release a new version when we have a new feature or bugfix. A developer with admin rights to the repository will create a new release on GitHub, and then publish the new version to PyPI. Project Documentation The release of project documentation is also a manual process and hosted on readthedocs server. NOTE: This project is under active development!","title":"Release Notes"},{"location":"release-notes/#release-process","text":"At the moment, the release process is manual. We try to make frequent releases. Usually, we release a new version when we have a new feature or bugfix. A developer with admin rights to the repository will create a new release on GitHub, and then publish the new version to PyPI.","title":"Release Process"},{"location":"release-notes/#project-documentation","text":"The release of project documentation is also a manual process and hosted on readthedocs server. NOTE: This project is under active development!","title":"Project Documentation"},{"location":"save-dataframes/","text":"Save and load dataframes In some cases, you might want to save the configuration of a SmartDataframe (including the name, the description, the file path and the sample head, if any). You can do so by calling the save method of the SmartDataframe as follows: from pandasai import SmartDataframe import pandas as pd # head df head_df = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) df = SmartDataframe( \"data/country_gdp.csv\", name=\"Country GDP\", description=\"A dataset containing the GDP of countries\", sample_head=head_df ) df.save(\"country\") From now on, you will be able to instantiate your smart dataframe without having to pass the configuration again, like this: from pandasai import SmartDataframe df = SmartDataframe(\"country\") If you don't pass any argument to the save method, the name will be equals to the name param of the dataframe. The configurations that you save are stored in the pandasai.json file, which is located in the root of your project.","title":"Save and load dataframes"},{"location":"save-dataframes/#save-and-load-dataframes","text":"In some cases, you might want to save the configuration of a SmartDataframe (including the name, the description, the file path and the sample head, if any). You can do so by calling the save method of the SmartDataframe as follows: from pandasai import SmartDataframe import pandas as pd # head df head_df = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) df = SmartDataframe( \"data/country_gdp.csv\", name=\"Country GDP\", description=\"A dataset containing the GDP of countries\", sample_head=head_df ) df.save(\"country\") From now on, you will be able to instantiate your smart dataframe without having to pass the configuration again, like this: from pandasai import SmartDataframe df = SmartDataframe(\"country\") If you don't pass any argument to the save method, the name will be equals to the name param of the dataframe. The configurations that you save are stored in the pandasai.json file, which is located in the root of your project.","title":"Save and load dataframes"},{"location":"shortcuts/","text":"Shortcuts Shortcuts are a way to quickly access the most common queries. At the moment, shortcuts are in beta, and only a few are available. More will be added in the future. Available shortcuts clean_data df = SmartDataframe('data.csv') df.clean_data() This shortcut will do data cleaning on the data frame. impute_missing_values df = SmartDataframe('data.csv') df.impute_missing_values() This shortcut will impute missing values in the data frame. generate_features df = SmartDataframe('data.csv') df.generate_features() This shortcut will generate features in the data frame. plot_pie_chart df = SmartDataframe('data.csv') df.plot_pie_chart(labels = ['a', 'b', 'c'], values = [1, 2, 3]) This shortcut will plot a pie chart of the data frame. plot_bar_chart df = SmartDataframe('data.csv') df.plot_bar_chart(x = ['a', 'b', 'c'], y = [1, 2, 3]) This shortcut will plot a bar chart of the data frame. plot_histogram df = SmartDataframe('data.csv') df.plot_histogram(column = 'a') This shortcut will plot a histogram of the data frame. plot_line_chart df = SmartDataframe('data.csv') df.plot_line_chart(x = ['a', 'b', 'c'], y = [1, 2, 3]) This shortcut will plot a line chart of the data frame. plot_scatter_chart df = SmartDataframe('data.csv') df.plot_scatter_chart(x = ['a', 'b', 'c'], y = [1, 2, 3]) This shortcut will plot a scatter chart of the data frame. plot_correlation_heatmap df = SmartDataframe('data.csv') df.plot_correlation_heatmap(df) This shortcut will plot a correlation heatmap of the data frame. plot_confusion_matrix df = SmartDataframe('data.csv') df.plot_confusion_matrix(y_true = [1, 2, 3], y_pred = [1, 2, 3]) This shortcut will plot a confusion matrix of the data frame. plot_roc_curve df = SmartDataframe('data.csv') df.plot_roc_curve(y_true = [1, 2, 3], y_pred = [1, 2, 3]) This shortcut will plot a ROC curve of the data frame. boxplot df = SmartDataframe('data.csv') df.boxplot(col='A', by='B', style='Highlight outliers with a x') This shortcut plots a box-and-whisker plot using the DataFrame df , focusing on the 'A' column and grouping the data by the 'B' column. The style parameter allows users to communicate their desired plot customizations to the Language Model, providing flexibility for further refinement and adaptability to specific visual requirements. rolling_mean df = SmartDataframe('data.csv') df.rolling_mean(column = 'a', window = 5) This shortcut will calculate the rolling mean of the data frame. rolling_median df = SmartDataframe('data.csv') df.rolling_median(column = 'a', window = 5) This shortcut will calculate the rolling median of the data frame. rolling_std df = SmartDataframe('data.csv') df.rolling_std(column = 'a', window = 5) This shortcut will calculate the rolling standard deviation of the data frame. segment_customers df = SmartDataframe('data.csv') df.segment_customers(features = ['a', 'b', 'c'], n_clusters = 5) This shortcut will segment customers in the data frame.","title":"Shortcuts"},{"location":"shortcuts/#shortcuts","text":"Shortcuts are a way to quickly access the most common queries. At the moment, shortcuts are in beta, and only a few are available. More will be added in the future.","title":"Shortcuts"},{"location":"shortcuts/#available-shortcuts","text":"","title":"Available shortcuts"},{"location":"shortcuts/#clean_data","text":"df = SmartDataframe('data.csv') df.clean_data() This shortcut will do data cleaning on the data frame.","title":"clean_data"},{"location":"shortcuts/#impute_missing_values","text":"df = SmartDataframe('data.csv') df.impute_missing_values() This shortcut will impute missing values in the data frame.","title":"impute_missing_values"},{"location":"shortcuts/#generate_features","text":"df = SmartDataframe('data.csv') df.generate_features() This shortcut will generate features in the data frame.","title":"generate_features"},{"location":"shortcuts/#plot_pie_chart","text":"df = SmartDataframe('data.csv') df.plot_pie_chart(labels = ['a', 'b', 'c'], values = [1, 2, 3]) This shortcut will plot a pie chart of the data frame.","title":"plot_pie_chart"},{"location":"shortcuts/#plot_bar_chart","text":"df = SmartDataframe('data.csv') df.plot_bar_chart(x = ['a', 'b', 'c'], y = [1, 2, 3]) This shortcut will plot a bar chart of the data frame.","title":"plot_bar_chart"},{"location":"shortcuts/#plot_histogram","text":"df = SmartDataframe('data.csv') df.plot_histogram(column = 'a') This shortcut will plot a histogram of the data frame.","title":"plot_histogram"},{"location":"shortcuts/#plot_line_chart","text":"df = SmartDataframe('data.csv') df.plot_line_chart(x = ['a', 'b', 'c'], y = [1, 2, 3]) This shortcut will plot a line chart of the data frame.","title":"plot_line_chart"},{"location":"shortcuts/#plot_scatter_chart","text":"df = SmartDataframe('data.csv') df.plot_scatter_chart(x = ['a', 'b', 'c'], y = [1, 2, 3]) This shortcut will plot a scatter chart of the data frame.","title":"plot_scatter_chart"},{"location":"shortcuts/#plot_correlation_heatmap","text":"df = SmartDataframe('data.csv') df.plot_correlation_heatmap(df) This shortcut will plot a correlation heatmap of the data frame.","title":"plot_correlation_heatmap"},{"location":"shortcuts/#plot_confusion_matrix","text":"df = SmartDataframe('data.csv') df.plot_confusion_matrix(y_true = [1, 2, 3], y_pred = [1, 2, 3]) This shortcut will plot a confusion matrix of the data frame.","title":"plot_confusion_matrix"},{"location":"shortcuts/#plot_roc_curve","text":"df = SmartDataframe('data.csv') df.plot_roc_curve(y_true = [1, 2, 3], y_pred = [1, 2, 3]) This shortcut will plot a ROC curve of the data frame.","title":"plot_roc_curve"},{"location":"shortcuts/#boxplot","text":"df = SmartDataframe('data.csv') df.boxplot(col='A', by='B', style='Highlight outliers with a x') This shortcut plots a box-and-whisker plot using the DataFrame df , focusing on the 'A' column and grouping the data by the 'B' column. The style parameter allows users to communicate their desired plot customizations to the Language Model, providing flexibility for further refinement and adaptability to specific visual requirements.","title":"boxplot"},{"location":"shortcuts/#rolling_mean","text":"df = SmartDataframe('data.csv') df.rolling_mean(column = 'a', window = 5) This shortcut will calculate the rolling mean of the data frame.","title":"rolling_mean"},{"location":"shortcuts/#rolling_median","text":"df = SmartDataframe('data.csv') df.rolling_median(column = 'a', window = 5) This shortcut will calculate the rolling median of the data frame.","title":"rolling_median"},{"location":"shortcuts/#rolling_std","text":"df = SmartDataframe('data.csv') df.rolling_std(column = 'a', window = 5) This shortcut will calculate the rolling standard deviation of the data frame.","title":"rolling_std"},{"location":"shortcuts/#segment_customers","text":"df = SmartDataframe('data.csv') df.segment_customers(features = ['a', 'b', 'c'], n_clusters = 5) This shortcut will segment customers in the data frame.","title":"segment_customers"},{"location":"API/helpers/","text":"Helpers This module includes methods classified as helpers. Anonymizer A collection of methods to help handle sensitive information pandasai.helpers.anonymizer Helper class to anonymize a dataframe head by replacing the values of the columns that contain personal or sensitive information with random values. Anonymizer Source code in pandasai\\helpers\\anonymizer.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class Anonymizer : def _is_valid_email ( email : str ) -> bool : \"\"\"Check if the given email is valid based on regex pattern. Args: email (str): email address to be checked. Returns (bool): True if the email is valid, otherwise False. \"\"\" email_regex = r \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\" return re . match ( email_regex , email ) is not None def _is_valid_phone_number ( phone_number : str ) -> bool : \"\"\"Check if the given phone number is valid based on regex pattern. Args: phone_number (str): phone number to be checked. Returns (bool): True if the phone number is valid, otherwise False. \"\"\" pattern = r \"\\b(?:\\+?\\d{1,3}[- ]?)?\\(?\\d {3} \\)?[- ]?\\d {3} [- ]?\\d {4} \\b\" return re . search ( pattern , phone_number ) is not None def _is_valid_credit_card ( credit_card_number : str ) -> bool : \"\"\"Check if the given credit card number is valid based on regex pattern. Args: credit_card_number (str): credit card number to be checked. Returns (str): True if the credit card number is valid, otherwise False. \"\"\" pattern = r \"^\\d {4} [- ]?\\d {4} [- ]?\\d {4} [- ]?\\d {4} $\" return re . search ( pattern , credit_card_number ) is not None def _generate_random_email () -> str : \"\"\"Generates a random email address using predefined domains. Returns (str): generated random email address. \"\"\" domains = [ \"gmail.com\" , \"yahoo.com\" , \"hotmail.com\" , \"outlook.com\" , \"icloud.com\" , \"aol.com\" , \"protonmail.com\" , \"zoho.com\" , ] name_length = random . randint ( 6 , 12 ) domain = random . choice ( domains ) letters = string . ascii_lowercase + string . digits + \"-_\" username = \"\" . join ( random . choice ( letters ) for i in range ( name_length )) email = username + \"@\" + domain return email def _generate_random_phone_number ( original_field : str ) -> str : \"\"\"Generate a random phone number with country code if originally present. Args: original_field (str): original phone number field. Returns (str): generated random phone number. \"\"\" if original_field . startswith ( \"+\" ): # Extract country code if present country_code = original_field . split ()[ 0 ] else : country_code = \"\" number = \"\" . join ( random . choices ( \"0123456789\" , k = 10 )) if country_code : phone_number = f \" { country_code } { number } \" else : phone_number = number return phone_number def _generate_random_credit_card () -> str : \"\"\"Generate a random credit card number. Returns (str): generated random credit card number. \"\"\" groups = [] for _i in range ( 4 ): group = \"\" . join ( random . choices ( \"0123456789\" , k = 4 )) groups . append ( group ) separator = random . choice ([ \"-\" , \" \" ]) return separator . join ( groups ) # static method to anonymize a dataframe head def anonymize_dataframe_head ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Anonymize a dataframe head by replacing the values of the columns that contain personal or sensitive information with random values. Args: df (pd.DataFrame): Dataframe to anonymize. Returns: pd.DataFrame: Anonymized dataframe. \"\"\" if len ( df ) == 0 : return df # create a copy of the dataframe head df_head = df . head () . copy () # for each column, check if it contains personal or sensitive information # and if so, replace the values with random values for col in df_head . columns : if Anonymizer . _is_valid_email ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_email () ) elif Anonymizer . _is_valid_phone_number ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_phone_number ( str ( x )) ) elif Anonymizer . _is_valid_credit_card ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_credit_card () ) return df_head anonymize_dataframe_head ( df ) Anonymize a dataframe head by replacing the values of the columns that contain personal or sensitive information with random values. Parameters: df ( DataFrame ) \u2013 Dataframe to anonymize. Returns: DataFrame \u2013 pd.DataFrame: Anonymized dataframe. pandasai\\helpers\\anonymizer.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def anonymize_dataframe_head ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Anonymize a dataframe head by replacing the values of the columns that contain personal or sensitive information with random values. Args: df (pd.DataFrame): Dataframe to anonymize. Returns: pd.DataFrame: Anonymized dataframe. \"\"\" if len ( df ) == 0 : return df # create a copy of the dataframe head df_head = df . head () . copy () # for each column, check if it contains personal or sensitive information # and if so, replace the values with random values for col in df_head . columns : if Anonymizer . _is_valid_email ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_email () ) elif Anonymizer . _is_valid_phone_number ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_phone_number ( str ( x )) ) elif Anonymizer . _is_valid_credit_card ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_credit_card () ) return df_head","title":"Helpers"},{"location":"API/helpers/#helpers","text":"This module includes methods classified as helpers.","title":"Helpers"},{"location":"API/helpers/#anonymizer","text":"A collection of methods to help handle sensitive information","title":"Anonymizer"},{"location":"API/helpers/#pandasai.helpers.anonymizer","text":"Helper class to anonymize a dataframe head by replacing the values of the columns that contain personal or sensitive information with random values.","title":"anonymizer"},{"location":"API/helpers/#pandasai.helpers.anonymizer.Anonymizer","text":"Source code in pandasai\\helpers\\anonymizer.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class Anonymizer : def _is_valid_email ( email : str ) -> bool : \"\"\"Check if the given email is valid based on regex pattern. Args: email (str): email address to be checked. Returns (bool): True if the email is valid, otherwise False. \"\"\" email_regex = r \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\" return re . match ( email_regex , email ) is not None def _is_valid_phone_number ( phone_number : str ) -> bool : \"\"\"Check if the given phone number is valid based on regex pattern. Args: phone_number (str): phone number to be checked. Returns (bool): True if the phone number is valid, otherwise False. \"\"\" pattern = r \"\\b(?:\\+?\\d{1,3}[- ]?)?\\(?\\d {3} \\)?[- ]?\\d {3} [- ]?\\d {4} \\b\" return re . search ( pattern , phone_number ) is not None def _is_valid_credit_card ( credit_card_number : str ) -> bool : \"\"\"Check if the given credit card number is valid based on regex pattern. Args: credit_card_number (str): credit card number to be checked. Returns (str): True if the credit card number is valid, otherwise False. \"\"\" pattern = r \"^\\d {4} [- ]?\\d {4} [- ]?\\d {4} [- ]?\\d {4} $\" return re . search ( pattern , credit_card_number ) is not None def _generate_random_email () -> str : \"\"\"Generates a random email address using predefined domains. Returns (str): generated random email address. \"\"\" domains = [ \"gmail.com\" , \"yahoo.com\" , \"hotmail.com\" , \"outlook.com\" , \"icloud.com\" , \"aol.com\" , \"protonmail.com\" , \"zoho.com\" , ] name_length = random . randint ( 6 , 12 ) domain = random . choice ( domains ) letters = string . ascii_lowercase + string . digits + \"-_\" username = \"\" . join ( random . choice ( letters ) for i in range ( name_length )) email = username + \"@\" + domain return email def _generate_random_phone_number ( original_field : str ) -> str : \"\"\"Generate a random phone number with country code if originally present. Args: original_field (str): original phone number field. Returns (str): generated random phone number. \"\"\" if original_field . startswith ( \"+\" ): # Extract country code if present country_code = original_field . split ()[ 0 ] else : country_code = \"\" number = \"\" . join ( random . choices ( \"0123456789\" , k = 10 )) if country_code : phone_number = f \" { country_code } { number } \" else : phone_number = number return phone_number def _generate_random_credit_card () -> str : \"\"\"Generate a random credit card number. Returns (str): generated random credit card number. \"\"\" groups = [] for _i in range ( 4 ): group = \"\" . join ( random . choices ( \"0123456789\" , k = 4 )) groups . append ( group ) separator = random . choice ([ \"-\" , \" \" ]) return separator . join ( groups ) # static method to anonymize a dataframe head def anonymize_dataframe_head ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Anonymize a dataframe head by replacing the values of the columns that contain personal or sensitive information with random values. Args: df (pd.DataFrame): Dataframe to anonymize. Returns: pd.DataFrame: Anonymized dataframe. \"\"\" if len ( df ) == 0 : return df # create a copy of the dataframe head df_head = df . head () . copy () # for each column, check if it contains personal or sensitive information # and if so, replace the values with random values for col in df_head . columns : if Anonymizer . _is_valid_email ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_email () ) elif Anonymizer . _is_valid_phone_number ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_phone_number ( str ( x )) ) elif Anonymizer . _is_valid_credit_card ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_credit_card () ) return df_head","title":"Anonymizer"},{"location":"API/helpers/#pandasai.helpers.anonymizer.Anonymizer.anonymize_dataframe_head","text":"Anonymize a dataframe head by replacing the values of the columns that contain personal or sensitive information with random values. Parameters: df ( DataFrame ) \u2013 Dataframe to anonymize. Returns: DataFrame \u2013 pd.DataFrame: Anonymized dataframe. pandasai\\helpers\\anonymizer.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def anonymize_dataframe_head ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Anonymize a dataframe head by replacing the values of the columns that contain personal or sensitive information with random values. Args: df (pd.DataFrame): Dataframe to anonymize. Returns: pd.DataFrame: Anonymized dataframe. \"\"\" if len ( df ) == 0 : return df # create a copy of the dataframe head df_head = df . head () . copy () # for each column, check if it contains personal or sensitive information # and if so, replace the values with random values for col in df_head . columns : if Anonymizer . _is_valid_email ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_email () ) elif Anonymizer . _is_valid_phone_number ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_phone_number ( str ( x )) ) elif Anonymizer . _is_valid_credit_card ( str ( df_head [ col ] . iloc [ 0 ])): df_head [ col ] = df_head [ col ] . apply ( lambda x : Anonymizer . _generate_random_credit_card () ) return df_head","title":"anonymize_dataframe_head()"},{"location":"API/llms/","text":"LLMs This document outlines the LLMs API wrappers included in the pandasai . Base This is a base class to implement any LLM to be used with pandasai framework. Base class to implement a new LLM This module is the base class to integrate the various LLMs API. This module also includes the Base LLM classes for OpenAI, HuggingFace and Google PaLM. Example: ``` from .base import BaseOpenAI class CustomLLM(BaseOpenAI): Custom Class Starts here!! ``` BaseGoogle Bases: LLM Base class to implement a new Google LLM LLM base class is extended to be used with Source code in pandasai\\llm\\base.py 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 class BaseGoogle ( LLM ): \"\"\"Base class to implement a new Google LLM LLM base class is extended to be used with \"\"\" temperature : Optional [ float ] = 0 top_p : Optional [ float ] = 0.8 top_k : Optional [ int ] = 40 max_output_tokens : Optional [ int ] = 1000 def _valid_params ( self ): return [ \"temperature\" , \"top_p\" , \"top_k\" , \"max_output_tokens\" ] def _set_params ( self , ** kwargs ): \"\"\" Dynamically set Parameters for the object. Args: **kwargs: Possible keyword arguments: \"temperature\", \"top_p\", \"top_k\", \"max_output_tokens\". Returns: None. \"\"\" valid_params = self . _valid_params () for key , value in kwargs . items (): if key in valid_params : setattr ( self , key , value ) def _validate ( self ): \"\"\"Validates the parameters for Google\"\"\" if self . temperature is not None and not 0 <= self . temperature <= 1 : raise ValueError ( \"temperature must be in the range [0.0, 1.0]\" ) if self . top_p is not None and not 0 <= self . top_p <= 1 : raise ValueError ( \"top_p must be in the range [0.0, 1.0]\" ) if self . top_k is not None and not 0 <= self . top_k <= 100 : raise ValueError ( \"top_k must be in the range [0.0, 100.0]\" ) if self . max_output_tokens is not None and self . max_output_tokens <= 0 : raise ValueError ( \"max_output_tokens must be greater than zero\" ) @abstractmethod def _generate_text ( self , prompt : str ) -> str : \"\"\" Generates text for prompt, specific to implementation. Args: prompt (str): A string representation of the prompt. Returns: str: LLM response. \"\"\" raise MethodNotImplementedError ( \"method has not been implemented\" ) def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the Google LLM. Args: instruction (AbstractPrompt): Instruction to pass. suffix (str): Suffix to pass. Defaults to an empty string (\"\"). Returns: str: LLM response. \"\"\" self . last_prompt = instruction . to_string () + suffix return self . _generate_text ( self . last_prompt ) call ( instruction , suffix = '' ) Call the Google LLM. Parameters: instruction ( AbstractPrompt ) \u2013 Instruction to pass. suffix ( str , default: '' ) \u2013 Suffix to pass. Defaults to an empty string (\"\"). Returns: str ( str ) \u2013 LLM response. pandasai\\llm\\base.py 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the Google LLM. Args: instruction (AbstractPrompt): Instruction to pass. suffix (str): Suffix to pass. Defaults to an empty string (\"\"). Returns: str: LLM response. \"\"\" self . last_prompt = instruction . to_string () + suffix return self . _generate_text ( self . last_prompt ) BaseOpenAI Bases: LLM , ABC Base class to implement a new OpenAI LLM. LLM base class, this class is extended to be used with OpenAI API. Source code in pandasai\\llm\\base.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 class BaseOpenAI ( LLM , ABC ): \"\"\"Base class to implement a new OpenAI LLM. LLM base class, this class is extended to be used with OpenAI API. \"\"\" api_token : str temperature : float = 0 max_tokens : int = 1000 top_p : float = 1 frequency_penalty : float = 0 presence_penalty : float = 0.6 stop : Optional [ str ] = None # support explicit proxy for OpenAI openai_proxy : Optional [ str ] = None def _set_params ( self , ** kwargs ): \"\"\" Set Parameters Args: **kwargs: [\"model\", \"engine\", \"deployment_id\", \"temperature\",\"max_tokens\", \"top_p\", \"frequency_penalty\", \"presence_penalty\", \"stop\", ] Returns: None. \"\"\" valid_params = [ \"model\" , \"engine\" , \"deployment_id\" , \"temperature\" , \"max_tokens\" , \"top_p\" , \"frequency_penalty\" , \"presence_penalty\" , \"stop\" , ] for key , value in kwargs . items (): if key in valid_params : setattr ( self , key , value ) @property def _default_params ( self ) -> Dict [ str , Any ]: \"\"\" Get the default parameters for calling OpenAI API Returns Dict: A dict of OpenAi API parameters. \"\"\" return { \"temperature\" : self . temperature , \"max_tokens\" : self . max_tokens , \"top_p\" : self . top_p , \"frequency_penalty\" : self . frequency_penalty , \"presence_penalty\" : self . presence_penalty , } def completion ( self , prompt : str ) -> str : \"\"\" Query the completion API Args: prompt (str): A string representation of the prompt. Returns: str: LLM response. \"\"\" params = { ** self . _default_params , \"prompt\" : prompt } if self . stop is not None : params [ \"stop\" ] = [ self . stop ] response = openai . Completion . create ( ** params ) openai_handler = openai_callback_var . get () if openai_handler : openai_handler ( response ) return response [ \"choices\" ][ 0 ][ \"text\" ] def chat_completion ( self , value : str ) -> str : \"\"\" Query the chat completion API Args: value (str): Prompt Returns: str: LLM response. \"\"\" params = { ** self . _default_params , \"messages\" : [ { \"role\" : \"system\" , \"content\" : value , } ], } if self . stop is not None : params [ \"stop\" ] = [ self . stop ] response = openai . ChatCompletion . create ( ** params ) openai_handler = openai_callback_var . get () if openai_handler : openai_handler ( response ) return response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ] chat_completion ( value ) Query the chat completion API Parameters: value ( str ) \u2013 Prompt Returns: str ( str ) \u2013 LLM response. pandasai\\llm\\base.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def chat_completion ( self , value : str ) -> str : \"\"\" Query the chat completion API Args: value (str): Prompt Returns: str: LLM response. \"\"\" params = { ** self . _default_params , \"messages\" : [ { \"role\" : \"system\" , \"content\" : value , } ], } if self . stop is not None : params [ \"stop\" ] = [ self . stop ] response = openai . ChatCompletion . create ( ** params ) openai_handler = openai_callback_var . get () if openai_handler : openai_handler ( response ) return response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ] completion ( prompt ) Query the completion API Parameters: prompt ( str ) \u2013 A string representation of the prompt. Returns: str ( str ) \u2013 LLM response. pandasai\\llm\\base.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def completion ( self , prompt : str ) -> str : \"\"\" Query the completion API Args: prompt (str): A string representation of the prompt. Returns: str: LLM response. \"\"\" params = { ** self . _default_params , \"prompt\" : prompt } if self . stop is not None : params [ \"stop\" ] = [ self . stop ] response = openai . Completion . create ( ** params ) openai_handler = openai_callback_var . get () if openai_handler : openai_handler ( response ) return response [ \"choices\" ][ 0 ][ \"text\" ] HuggingFaceLLM Bases: LLM Base class to implement a new Hugging Face LLM. LLM base class is extended to be used with HuggingFace LLM Modes APIs. Source code in pandasai\\llm\\base.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 class HuggingFaceLLM ( LLM ): \"\"\"Base class to implement a new Hugging Face LLM. LLM base class is extended to be used with HuggingFace LLM Modes APIs. \"\"\" last_prompt : Optional [ str ] = None api_token : str _api_url : str = \"https://api-inference.huggingface.co/models/\" _max_retries : int = 3 @property def type ( self ) -> str : return \"huggingface-llm\" def _setup ( self , ** kwargs ): \"\"\" Setup the HuggingFace LLM Args: **kwargs: [\"api_token\", \"max_retries\"] \"\"\" self . api_token = ( kwargs . get ( \"api_token\" ) or os . getenv ( \"HUGGINGFACE_API_KEY\" ) or None ) if self . api_token is None : raise APIKeyNotFoundError ( \"HuggingFace Hub API key is required\" ) # Since the huggingface API only returns few tokens at a time, we need to # call the API multiple times to get all the tokens. This is the maximum # number of retries we will do. if kwargs . get ( \"max_retries\" ): self . _max_retries = kwargs . get ( \"max_retries\" ) def __init__ ( self , ** kwargs ): \"\"\" __init__ method of HuggingFaceLLM Class Args: **kwargs: [\"api_token\", \"max_retries\"] \"\"\" self . _setup ( ** kwargs ) def query ( self , payload ) -> str : \"\"\" Query the HF API Args: payload: A JSON form payload Returns: str: Value of the field \"generated_text\" in response JSON given by the remote server. \"\"\" headers = { \"Authorization\" : f \"Bearer { self . api_token } \" } response = requests . post ( self . _api_url , headers = headers , json = payload , timeout = 60 ) return response . json ()[ 0 ][ \"generated_text\" ] def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" A call method of HuggingFaceLLM class. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): A string representing the suffix to be truncated from the generated response. Returns str: LLM response. \"\"\" prompt = instruction . to_string () payload = prompt + suffix # sometimes the API doesn't return a valid response, so we retry passing the # output generated from the previous call as the input for _i in range ( self . _max_retries ): response = self . query ({ \"inputs\" : payload }) payload = response match = re . search ( \"(```python)(.*)(```)\" , response . replace ( prompt + suffix , \"\" ), re . DOTALL | re . MULTILINE , ) if match : break return response . replace ( prompt + suffix , \"\" ) __init__ ( ** kwargs ) init method of HuggingFaceLLM Class Parameters: **kwargs \u2013 [\"api_token\", \"max_retries\"] pandasai\\llm\\base.py 307 308 309 310 311 312 313 314 315 def __init__ ( self , ** kwargs ): \"\"\" __init__ method of HuggingFaceLLM Class Args: **kwargs: [\"api_token\", \"max_retries\"] \"\"\" self . _setup ( ** kwargs ) call ( instruction , suffix = '' ) A call method of HuggingFaceLLM class. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): A string representing the suffix to be truncated from the generated response. Returns str: LLM response. pandasai\\llm\\base.py 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" A call method of HuggingFaceLLM class. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): A string representing the suffix to be truncated from the generated response. Returns str: LLM response. \"\"\" prompt = instruction . to_string () payload = prompt + suffix # sometimes the API doesn't return a valid response, so we retry passing the # output generated from the previous call as the input for _i in range ( self . _max_retries ): response = self . query ({ \"inputs\" : payload }) payload = response match = re . search ( \"(```python)(.*)(```)\" , response . replace ( prompt + suffix , \"\" ), re . DOTALL | re . MULTILINE , ) if match : break return response . replace ( prompt + suffix , \"\" ) query ( payload ) Query the HF API Args: payload: A JSON form payload Returns: str ( str ) \u2013 Value of the field \"generated_text\" in response JSON given by the remote server. pandasai\\llm\\base.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 def query ( self , payload ) -> str : \"\"\" Query the HF API Args: payload: A JSON form payload Returns: str: Value of the field \"generated_text\" in response JSON given by the remote server. \"\"\" headers = { \"Authorization\" : f \"Bearer { self . api_token } \" } response = requests . post ( self . _api_url , headers = headers , json = payload , timeout = 60 ) return response . json ()[ 0 ][ \"generated_text\" ] LLM Base class to implement a new LLM. Source code in pandasai\\llm\\base.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class LLM : \"\"\"Base class to implement a new LLM.\"\"\" last_prompt : Optional [ str ] = None def is_pandasai_llm ( self ) -> bool : \"\"\" Return True if the LLM is from pandasAI. Returns: bool: True if the LLM is from pandasAI \"\"\" return True @property def type ( self ) -> str : \"\"\" Return type of LLM. Raises: APIKeyNotFoundError: Type has not been implemented Returns: str: Type of LLM a string \"\"\" raise APIKeyNotFoundError ( \"Type has not been implemented\" ) def _polish_code ( self , code : str ) -> str : \"\"\" Polish the code by removing the leading \"python\" or \"py\", \\ removing the imports and removing trailing spaces and new lines. Args: code (str): A sting of Python code. Returns: str: Polished code. \"\"\" if re . match ( r \"^(python|py)\" , code ): code = re . sub ( r \"^(python|py)\" , \"\" , code ) if re . match ( r \"^`.*`$\" , code ): code = re . sub ( r \"^`(.*)`$\" , r \"\\1\" , code ) code = code . strip () return code def _is_python_code ( self , string ): \"\"\" Return True if it is valid python code. Args: string (str): Returns (bool): True if Python Code otherwise False \"\"\" try : ast . parse ( string ) return True except SyntaxError : return False def _extract_code ( self , response : str , separator : str = \"```\" ) -> str : \"\"\" Extract the code from the response. Args: response (str): Response separator (str, optional): Separator. Defaults to \"```\". Raises: NoCodeFoundError: No code found in the response Returns: str: Extracted code from the response \"\"\" code = response if len ( code . split ( separator )) > 1 : code = code . split ( separator )[ 1 ] code = self . _polish_code ( code ) if not self . _is_python_code ( code ): raise NoCodeFoundError ( \"No code found in the response\" ) return code @abstractmethod def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Execute the LLM with given prompt. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str, optional): Suffix. Defaults to \"\". Raises: MethodNotImplementedError: Call method has not been implemented \"\"\" raise MethodNotImplementedError ( \"Call method has not been implemented\" ) def generate_code ( self , instruction : AbstractPrompt ) -> str : \"\"\" Generate the code based on the instruction and the given prompt. Args: instruction (AbstractPrompt): Prompt with instruction for LLM. Returns: str: A string of Python code. \"\"\" code = self . call ( instruction , suffix = \"\" ) return self . _extract_code ( code ) type : str property Return type of LLM. Raises: APIKeyNotFoundError \u2013 Type has not been implemented Returns: str ( str ) \u2013 Type of LLM a string call ( instruction , suffix = '' ) abstractmethod Execute the LLM with given prompt. Parameters: instruction ( AbstractPrompt ) \u2013 A prompt object with instruction for LLM. suffix ( str , default: '' ) \u2013 Suffix. Defaults to \"\". Raises: MethodNotImplementedError \u2013 Call method has not been implemented pandasai\\llm\\base.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 @abstractmethod def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Execute the LLM with given prompt. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str, optional): Suffix. Defaults to \"\". Raises: MethodNotImplementedError: Call method has not been implemented \"\"\" raise MethodNotImplementedError ( \"Call method has not been implemented\" ) generate_code ( instruction ) Generate the code based on the instruction and the given prompt. Parameters: instruction ( AbstractPrompt ) \u2013 Prompt with instruction for LLM. Returns: str ( str ) \u2013 A string of Python code. pandasai\\llm\\base.py 137 138 139 140 141 142 143 144 145 146 147 148 149 def generate_code ( self , instruction : AbstractPrompt ) -> str : \"\"\" Generate the code based on the instruction and the given prompt. Args: instruction (AbstractPrompt): Prompt with instruction for LLM. Returns: str: A string of Python code. \"\"\" code = self . call ( instruction , suffix = \"\" ) return self . _extract_code ( code ) is_pandasai_llm () Return True if the LLM is from pandasAI. Returns: bool ( bool ) \u2013 True if the LLM is from pandasAI pandasai\\llm\\base.py 40 41 42 43 44 45 46 47 48 def is_pandasai_llm ( self ) -> bool : \"\"\" Return True if the LLM is from pandasAI. Returns: bool: True if the LLM is from pandasAI \"\"\" return True options: show_root_heading: true OpenAI OpenAI API wrapper extended through BaseOpenAI class. OpenAI LLM API This module is to run the OpenAI API using OpenAI API. Example Use below example to call OpenAI Model from pandasai.llm.openai import OpenAI OpenAI Bases: BaseOpenAI OpenAI LLM using BaseOpenAI Class. An API call to OpenAI API is sent and response is recorded and returned. The default chat model is gpt-3.5-turbo . The list of supported Chat models includes [\"gpt-4\", \"gpt-4-0613\", \"gpt-4-32k\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-instruct\"]. The list of supported Completion models includes \"gpt-3.5-turbo-instruct\" and \"text-davinci-003\" (soon to be deprecated). Source code in pandasai\\llm\\openai.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class OpenAI ( BaseOpenAI ): \"\"\"OpenAI LLM using BaseOpenAI Class. An API call to OpenAI API is sent and response is recorded and returned. The default chat model is **gpt-3.5-turbo**. The list of supported Chat models includes [\"gpt-4\", \"gpt-4-0613\", \"gpt-4-32k\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-instruct\"]. The list of supported Completion models includes \"gpt-3.5-turbo-instruct\" and \"text-davinci-003\" (soon to be deprecated). \"\"\" _supported_chat_models = [ \"gpt-4\" , \"gpt-4-0613\" , \"gpt-4-32k\" , \"gpt-4-32k-0613\" , \"gpt-3.5-turbo\" , \"gpt-3.5-turbo-16k\" , \"gpt-3.5-turbo-0613\" , \"gpt-3.5-turbo-16k-0613\" , ] _supported_completion_models = [ \"text-davinci-003\" , \"gpt-3.5-turbo-instruct\" ] model : str = \"gpt-3.5-turbo\" def __init__ ( self , api_token : Optional [ str ] = None , api_key_path : Optional [ str ] = None , ** kwargs , ): \"\"\" __init__ method of OpenAI Class Args: api_token (str): API Token for OpenAI platform. **kwargs: Extended Parameters inferred from BaseOpenAI class \"\"\" self . api_token = api_token or os . getenv ( \"OPENAI_API_KEY\" ) or None self . api_key_path = api_key_path if ( not self . api_token ) and ( not self . api_key_path ): raise APIKeyNotFoundError ( \"Either OpenAI API key or key path is required\" ) if self . api_token : openai . api_key = self . api_token else : openai . api_key_path = self . api_key_path self . openai_proxy = kwargs . get ( \"openai_proxy\" ) or os . getenv ( \"OPENAI_PROXY\" ) if self . openai_proxy : openai . proxy = { \"http\" : self . openai_proxy , \"https\" : self . openai_proxy } self . _set_params ( ** kwargs ) @property def _default_params ( self ) -> Dict [ str , Any ]: \"\"\"Get the default parameters for calling OpenAI API\"\"\" return { ** super () . _default_params , \"model\" : self . model , } def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the OpenAI LLM. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): Suffix to pass. Raises: UnsupportedOpenAIModelError: Unsupported model Returns: str: Response \"\"\" self . last_prompt = instruction . to_string () + suffix if self . model in self . _supported_chat_models : response = self . chat_completion ( self . last_prompt ) elif self . model in self . _supported_completion_models : response = self . completion ( self . last_prompt ) else : raise UnsupportedOpenAIModelError ( \"Unsupported model\" ) return response @property def type ( self ) -> str : return \"openai\" __init__ ( api_token = None , api_key_path = None , ** kwargs ) init method of OpenAI Class Parameters: api_token ( str , default: None ) \u2013 API Token for OpenAI platform. **kwargs \u2013 Extended Parameters inferred from BaseOpenAI class pandasai\\llm\\openai.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , api_token : Optional [ str ] = None , api_key_path : Optional [ str ] = None , ** kwargs , ): \"\"\" __init__ method of OpenAI Class Args: api_token (str): API Token for OpenAI platform. **kwargs: Extended Parameters inferred from BaseOpenAI class \"\"\" self . api_token = api_token or os . getenv ( \"OPENAI_API_KEY\" ) or None self . api_key_path = api_key_path if ( not self . api_token ) and ( not self . api_key_path ): raise APIKeyNotFoundError ( \"Either OpenAI API key or key path is required\" ) if self . api_token : openai . api_key = self . api_token else : openai . api_key_path = self . api_key_path self . openai_proxy = kwargs . get ( \"openai_proxy\" ) or os . getenv ( \"OPENAI_PROXY\" ) if self . openai_proxy : openai . proxy = { \"http\" : self . openai_proxy , \"https\" : self . openai_proxy } self . _set_params ( ** kwargs ) call ( instruction , suffix = '' ) Call the OpenAI LLM. Parameters: instruction ( AbstractPrompt ) \u2013 A prompt object with instruction for LLM. suffix ( str , default: '' ) \u2013 Suffix to pass. Raises: UnsupportedOpenAIModelError \u2013 Unsupported model Returns: str ( str ) \u2013 Response pandasai\\llm\\openai.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the OpenAI LLM. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): Suffix to pass. Raises: UnsupportedOpenAIModelError: Unsupported model Returns: str: Response \"\"\" self . last_prompt = instruction . to_string () + suffix if self . model in self . _supported_chat_models : response = self . chat_completion ( self . last_prompt ) elif self . model in self . _supported_completion_models : response = self . completion ( self . last_prompt ) else : raise UnsupportedOpenAIModelError ( \"Unsupported model\" ) return response options: show_root_heading: true Starcoder (deprecated) Starcoder wrapper extended through Base HuggingFace Class Note: Starcoder is deprecated and will be removed in future versions. Please use another LLM. Starcoder LLM This module is to run the StartCoder API hosted and maintained by HuggingFace.co. To generate HF_TOKEN go to https://huggingface.co/settings/tokens after creating Account on the platform. Example Use below example to call Starcoder Model from pandasai.llm.starcoder import Starcoder Starcoder Bases: HuggingFaceLLM Starcoder LLM API (Deprecated: Kept for backwards compatibility) Source code in pandasai\\llm\\starcoder.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class Starcoder ( HuggingFaceLLM ): \"\"\"Starcoder LLM API (Deprecated: Kept for backwards compatibility)\"\"\" api_token : str _api_url : str = \"https://api-inference.huggingface.co/models/bigcode/starcoder\" _max_retries : int = 30 def __init__ ( self , ** kwargs ): warnings . warn ( \"\"\"Starcoder is deprecated and will be removed in a future release. Please use langchain.llms.HuggingFaceHub instead, although please be aware that it may perform poorly. \"\"\" ) super () . __init__ ( ** kwargs ) @property def type ( self ) -> str : return \"starcoder\" options: show_root_heading: true Falcon (deprecated) Falcon wrapper extended through Base HuggingFace Class Note: Falcon is deprecated and will be removed in future versions. Please use another LLM. Falcon LLM This module is to run the Falcon API hosted and maintained by HuggingFace.co. To generate HF_TOKEN go to https://huggingface.co/settings/tokens after creating Account on the platform. Example Use below example to call Falcon Model from pandasai.llm.falcon import Falcon Falcon Bases: HuggingFaceLLM Falcon LLM API (Deprecated: Kept for backwards compatibility) Source code in pandasai\\llm\\falcon.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Falcon ( HuggingFaceLLM ): \"\"\"Falcon LLM API (Deprecated: Kept for backwards compatibility)\"\"\" api_token : str _api_url : str = ( \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\" ) _max_retries : int = 30 def __init__ ( self , ** kwargs ): warnings . warn ( \"\"\"Falcon is deprecated and will be removed in a future release. Please use langchain.llms.HuggingFaceHub instead, although please be aware that it may perform poorly. \"\"\" ) super () . __init__ ( ** kwargs ) @property def type ( self ) -> str : return \"falcon\" options: show_root_heading: true Azure OpenAI OpenAI API through Azure Platform wrapper OpenAI LLM via Microsoft Azure Cloud This module is to run the OpenAI API when using Microsoft Cloud infrastructure. Azure has implemented the openai API access to its platform. For details https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference. Example Use below example to call AzureOpenAI class from pandasai.llm.azure_openai import AzureOpenAI AzureOpenAI Bases: BaseOpenAI OpenAI LLM via Microsoft Azure This class uses BaseOpenAI class to support Azure OpenAI features. Source code in pandasai\\llm\\azure_openai.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class AzureOpenAI ( BaseOpenAI ): \"\"\"OpenAI LLM via Microsoft Azure This class uses `BaseOpenAI` class to support Azure OpenAI features. \"\"\" api_base : str api_type : str = \"azure\" api_version : str engine : str def __init__ ( self , api_token : Optional [ str ] = None , api_base : Optional [ str ] = None , api_version : Optional [ str ] = None , deployment_name : str = None , is_chat_model : bool = True , ** kwargs , ): \"\"\" __init__ method of AzureOpenAI Class. Args: api_token (str): Azure OpenAI API token. api_base (str): Base url of the Azure endpoint. It should look like the following: <https://YOUR_RESOURCE_NAME.openai.azure.com/> api_version (str): Version of the Azure OpenAI API. Be aware the API version may change. deployment_name (str): Custom name of the deployed model is_chat_model (bool): Whether ``deployment_name`` corresponds to a Chat or a Completion model. **kwargs: Inference Parameters. \"\"\" self . api_token = api_token or os . getenv ( \"OPENAI_API_KEY\" ) or None self . api_base = api_base or os . getenv ( \"OPENAI_API_BASE\" ) or None self . api_version = api_version or os . getenv ( \"OPENAI_API_VERSION\" ) if self . api_token is None : raise APIKeyNotFoundError ( \"Azure OpenAI key is required. Please add an environment variable \" \"`OPENAI_API_KEY` or pass `api_token` as a named parameter\" ) if self . api_base is None : raise APIKeyNotFoundError ( \"Azure OpenAI base is required. Please add an environment variable \" \"`OPENAI_API_BASE` or pass `api_base` as a named parameter\" ) if self . api_version is None : raise APIKeyNotFoundError ( \"Azure OpenAI version is required. Please add an environment variable \" \"`OPENAI_API_VERSION` or pass `api_version` as a named parameter\" ) openai . api_key = self . api_token openai . api_base = self . api_base openai . api_version = self . api_version openai . api_type = self . api_type if deployment_name is None : raise UnsupportedOpenAIModelError ( \"Model deployment name is required.\" ) self . is_chat_model = is_chat_model self . engine = deployment_name self . openai_proxy = kwargs . get ( \"openai_proxy\" ) or os . getenv ( \"OPENAI_PROXY\" ) if self . openai_proxy : openai . proxy = { \"http\" : self . openai_proxy , \"https\" : self . openai_proxy } self . _set_params ( ** kwargs ) @property def _default_params ( self ) -> Dict [ str , Any ]: \"\"\" Get the default parameters for calling OpenAI API. Returns: dict: A dictionary containing Default Params. \"\"\" return { ** super () . _default_params , \"engine\" : self . engine } def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the Azure OpenAI LLM. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): Suffix to pass. Returns: str: LLM response. \"\"\" self . last_prompt = instruction . to_string () + suffix if self . is_chat_model : response = self . chat_completion ( self . last_prompt ) else : response = self . completion ( self . last_prompt ) return response @property def type ( self ) -> str : return \"azure-openai\" __init__ ( api_token = None , api_base = None , api_version = None , deployment_name = None , is_chat_model = True , ** kwargs ) init method of AzureOpenAI Class. Parameters: api_token ( str , default: None ) \u2013 Azure OpenAI API token. api_base ( str , default: None ) \u2013 Base url of the Azure endpoint. It should look like the following: https://YOUR_RESOURCE_NAME.openai.azure.com/ api_version ( str , default: None ) \u2013 Version of the Azure OpenAI API. Be aware the API version may change. deployment_name ( str , default: None ) \u2013 Custom name of the deployed model is_chat_model ( bool , default: True ) \u2013 Whether deployment_name corresponds to a Chat or a Completion model. **kwargs \u2013 Inference Parameters. pandasai\\llm\\azure_openai.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , api_token : Optional [ str ] = None , api_base : Optional [ str ] = None , api_version : Optional [ str ] = None , deployment_name : str = None , is_chat_model : bool = True , ** kwargs , ): \"\"\" __init__ method of AzureOpenAI Class. Args: api_token (str): Azure OpenAI API token. api_base (str): Base url of the Azure endpoint. It should look like the following: <https://YOUR_RESOURCE_NAME.openai.azure.com/> api_version (str): Version of the Azure OpenAI API. Be aware the API version may change. deployment_name (str): Custom name of the deployed model is_chat_model (bool): Whether ``deployment_name`` corresponds to a Chat or a Completion model. **kwargs: Inference Parameters. \"\"\" self . api_token = api_token or os . getenv ( \"OPENAI_API_KEY\" ) or None self . api_base = api_base or os . getenv ( \"OPENAI_API_BASE\" ) or None self . api_version = api_version or os . getenv ( \"OPENAI_API_VERSION\" ) if self . api_token is None : raise APIKeyNotFoundError ( \"Azure OpenAI key is required. Please add an environment variable \" \"`OPENAI_API_KEY` or pass `api_token` as a named parameter\" ) if self . api_base is None : raise APIKeyNotFoundError ( \"Azure OpenAI base is required. Please add an environment variable \" \"`OPENAI_API_BASE` or pass `api_base` as a named parameter\" ) if self . api_version is None : raise APIKeyNotFoundError ( \"Azure OpenAI version is required. Please add an environment variable \" \"`OPENAI_API_VERSION` or pass `api_version` as a named parameter\" ) openai . api_key = self . api_token openai . api_base = self . api_base openai . api_version = self . api_version openai . api_type = self . api_type if deployment_name is None : raise UnsupportedOpenAIModelError ( \"Model deployment name is required.\" ) self . is_chat_model = is_chat_model self . engine = deployment_name self . openai_proxy = kwargs . get ( \"openai_proxy\" ) or os . getenv ( \"OPENAI_PROXY\" ) if self . openai_proxy : openai . proxy = { \"http\" : self . openai_proxy , \"https\" : self . openai_proxy } self . _set_params ( ** kwargs ) call ( instruction , suffix = '' ) Call the Azure OpenAI LLM. Parameters: instruction ( AbstractPrompt ) \u2013 A prompt object with instruction for LLM. suffix ( str , default: '' ) \u2013 Suffix to pass. Returns: str ( str ) \u2013 LLM response. pandasai\\llm\\azure_openai.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the Azure OpenAI LLM. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): Suffix to pass. Returns: str: LLM response. \"\"\" self . last_prompt = instruction . to_string () + suffix if self . is_chat_model : response = self . chat_completion ( self . last_prompt ) else : response = self . completion ( self . last_prompt ) return response options: show_root_heading: true GooglePalm GooglePalm class extended through BaseGoogle Class Google Palm LLM This module is to run the Google PaLM API hosted and maintained by Google. To read more on Google PaLM follow https://developers.generativeai.google/products/palm. Example Use below example to call GooglePalm Model from pandasai.llm.google_palm import GooglePalm GooglePalm Bases: BaseGoogle Google Palm LLM BaseGoogle class is extended for Google Palm model. The default and only model support at the moment is models/text-bison-001. Source code in pandasai\\llm\\google_palm.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class GooglePalm ( BaseGoogle ): \"\"\"Google Palm LLM BaseGoogle class is extended for Google Palm model. The default and only model support at the moment is models/text-bison-001. \"\"\" model : str = \"models/text-bison-001\" google_palm : Any def __init__ ( self , api_key : str , ** kwargs ): \"\"\" __init__ method of GooglePalm Class Args: api_key (str): API Key **kwargs: Extended Parameters inferred from BaseGoogle Class \"\"\" self . _configure ( api_key = api_key ) self . _set_params ( ** kwargs ) def _configure ( self , api_key : str ): \"\"\" Configure Google Palm API Key Args: api_key (str): A string of API keys generated from Google Cloud. Returns: None. \"\"\" if not api_key : raise APIKeyNotFoundError ( \"Google Palm API key is required\" ) err_msg = \"Install google-generativeai >= 0.1 for Google Palm API\" google_palm = import_dependency ( \"google.generativeai\" , extra = err_msg ) google_palm . configure ( api_key = api_key ) self . google_palm = google_palm def _valid_params ( self ): \"\"\"Returns if the Parameters are valid or Not\"\"\" return super () . _valid_params () + [ \"model\" ] def _validate ( self ): \"\"\" A method to Validate the Model \"\"\" super () . _validate () if not self . model : raise ValueError ( \"model is required.\" ) def _generate_text ( self , prompt : str ) -> str : \"\"\" Generates text for prompt. Args: prompt (str): A string representation of the prompt. Returns: str: LLM response. \"\"\" self . _validate () completion = self . google_palm . generate_text ( model = self . model , prompt = prompt , temperature = self . temperature , top_p = self . top_p , top_k = self . top_k , max_output_tokens = self . max_output_tokens , ) return completion . result @property def type ( self ) -> str : return \"google-palm\" __init__ ( api_key , ** kwargs ) init method of GooglePalm Class Args: api_key (str): API Key **kwargs: Extended Parameters inferred from BaseGoogle Class pandasai\\llm\\google_palm.py 28 29 30 31 32 33 34 35 36 def __init__ ( self , api_key : str , ** kwargs ): \"\"\" __init__ method of GooglePalm Class Args: api_key (str): API Key **kwargs: Extended Parameters inferred from BaseGoogle Class \"\"\" self . _configure ( api_key = api_key ) self . _set_params ( ** kwargs ) options: show_root_heading: true Fake A test fake class Fake LLM FakeLLM Bases: LLM Fake LLM Source code in pandasai\\llm\\fake.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class FakeLLM ( LLM ): \"\"\"Fake LLM\"\"\" _output : str = \"\"\"def analyze_data(dfs): return { 'type': 'text', 'value': \"Hello World\" }\"\"\" def __init__ ( self , output : Optional [ str ] = None ): if output is not None : self . _output = output def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : self . last_prompt = instruction . to_string () + suffix return self . _output @property def type ( self ) -> str : return \"fake\" options: show_root_heading: true","title":"Llms"},{"location":"API/llms/#llms","text":"This document outlines the LLMs API wrappers included in the pandasai .","title":"LLMs"},{"location":"API/llms/#base","text":"This is a base class to implement any LLM to be used with pandasai framework. Base class to implement a new LLM This module is the base class to integrate the various LLMs API. This module also includes the Base LLM classes for OpenAI, HuggingFace and Google PaLM. Example: ``` from .base import BaseOpenAI class CustomLLM(BaseOpenAI): Custom Class Starts here!! ```","title":"Base"},{"location":"API/llms/#pandasai.llm.base.BaseGoogle","text":"Bases: LLM Base class to implement a new Google LLM LLM base class is extended to be used with Source code in pandasai\\llm\\base.py 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 class BaseGoogle ( LLM ): \"\"\"Base class to implement a new Google LLM LLM base class is extended to be used with \"\"\" temperature : Optional [ float ] = 0 top_p : Optional [ float ] = 0.8 top_k : Optional [ int ] = 40 max_output_tokens : Optional [ int ] = 1000 def _valid_params ( self ): return [ \"temperature\" , \"top_p\" , \"top_k\" , \"max_output_tokens\" ] def _set_params ( self , ** kwargs ): \"\"\" Dynamically set Parameters for the object. Args: **kwargs: Possible keyword arguments: \"temperature\", \"top_p\", \"top_k\", \"max_output_tokens\". Returns: None. \"\"\" valid_params = self . _valid_params () for key , value in kwargs . items (): if key in valid_params : setattr ( self , key , value ) def _validate ( self ): \"\"\"Validates the parameters for Google\"\"\" if self . temperature is not None and not 0 <= self . temperature <= 1 : raise ValueError ( \"temperature must be in the range [0.0, 1.0]\" ) if self . top_p is not None and not 0 <= self . top_p <= 1 : raise ValueError ( \"top_p must be in the range [0.0, 1.0]\" ) if self . top_k is not None and not 0 <= self . top_k <= 100 : raise ValueError ( \"top_k must be in the range [0.0, 100.0]\" ) if self . max_output_tokens is not None and self . max_output_tokens <= 0 : raise ValueError ( \"max_output_tokens must be greater than zero\" ) @abstractmethod def _generate_text ( self , prompt : str ) -> str : \"\"\" Generates text for prompt, specific to implementation. Args: prompt (str): A string representation of the prompt. Returns: str: LLM response. \"\"\" raise MethodNotImplementedError ( \"method has not been implemented\" ) def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the Google LLM. Args: instruction (AbstractPrompt): Instruction to pass. suffix (str): Suffix to pass. Defaults to an empty string (\"\"). Returns: str: LLM response. \"\"\" self . last_prompt = instruction . to_string () + suffix return self . _generate_text ( self . last_prompt )","title":"BaseGoogle"},{"location":"API/llms/#pandasai.llm.base.BaseGoogle.call","text":"Call the Google LLM. Parameters: instruction ( AbstractPrompt ) \u2013 Instruction to pass. suffix ( str , default: '' ) \u2013 Suffix to pass. Defaults to an empty string (\"\"). Returns: str ( str ) \u2013 LLM response. pandasai\\llm\\base.py 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the Google LLM. Args: instruction (AbstractPrompt): Instruction to pass. suffix (str): Suffix to pass. Defaults to an empty string (\"\"). Returns: str: LLM response. \"\"\" self . last_prompt = instruction . to_string () + suffix return self . _generate_text ( self . last_prompt )","title":"call()"},{"location":"API/llms/#pandasai.llm.base.BaseOpenAI","text":"Bases: LLM , ABC Base class to implement a new OpenAI LLM. LLM base class, this class is extended to be used with OpenAI API. Source code in pandasai\\llm\\base.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 class BaseOpenAI ( LLM , ABC ): \"\"\"Base class to implement a new OpenAI LLM. LLM base class, this class is extended to be used with OpenAI API. \"\"\" api_token : str temperature : float = 0 max_tokens : int = 1000 top_p : float = 1 frequency_penalty : float = 0 presence_penalty : float = 0.6 stop : Optional [ str ] = None # support explicit proxy for OpenAI openai_proxy : Optional [ str ] = None def _set_params ( self , ** kwargs ): \"\"\" Set Parameters Args: **kwargs: [\"model\", \"engine\", \"deployment_id\", \"temperature\",\"max_tokens\", \"top_p\", \"frequency_penalty\", \"presence_penalty\", \"stop\", ] Returns: None. \"\"\" valid_params = [ \"model\" , \"engine\" , \"deployment_id\" , \"temperature\" , \"max_tokens\" , \"top_p\" , \"frequency_penalty\" , \"presence_penalty\" , \"stop\" , ] for key , value in kwargs . items (): if key in valid_params : setattr ( self , key , value ) @property def _default_params ( self ) -> Dict [ str , Any ]: \"\"\" Get the default parameters for calling OpenAI API Returns Dict: A dict of OpenAi API parameters. \"\"\" return { \"temperature\" : self . temperature , \"max_tokens\" : self . max_tokens , \"top_p\" : self . top_p , \"frequency_penalty\" : self . frequency_penalty , \"presence_penalty\" : self . presence_penalty , } def completion ( self , prompt : str ) -> str : \"\"\" Query the completion API Args: prompt (str): A string representation of the prompt. Returns: str: LLM response. \"\"\" params = { ** self . _default_params , \"prompt\" : prompt } if self . stop is not None : params [ \"stop\" ] = [ self . stop ] response = openai . Completion . create ( ** params ) openai_handler = openai_callback_var . get () if openai_handler : openai_handler ( response ) return response [ \"choices\" ][ 0 ][ \"text\" ] def chat_completion ( self , value : str ) -> str : \"\"\" Query the chat completion API Args: value (str): Prompt Returns: str: LLM response. \"\"\" params = { ** self . _default_params , \"messages\" : [ { \"role\" : \"system\" , \"content\" : value , } ], } if self . stop is not None : params [ \"stop\" ] = [ self . stop ] response = openai . ChatCompletion . create ( ** params ) openai_handler = openai_callback_var . get () if openai_handler : openai_handler ( response ) return response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ]","title":"BaseOpenAI"},{"location":"API/llms/#pandasai.llm.base.BaseOpenAI.chat_completion","text":"Query the chat completion API Parameters: value ( str ) \u2013 Prompt Returns: str ( str ) \u2013 LLM response. pandasai\\llm\\base.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def chat_completion ( self , value : str ) -> str : \"\"\" Query the chat completion API Args: value (str): Prompt Returns: str: LLM response. \"\"\" params = { ** self . _default_params , \"messages\" : [ { \"role\" : \"system\" , \"content\" : value , } ], } if self . stop is not None : params [ \"stop\" ] = [ self . stop ] response = openai . ChatCompletion . create ( ** params ) openai_handler = openai_callback_var . get () if openai_handler : openai_handler ( response ) return response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ]","title":"chat_completion()"},{"location":"API/llms/#pandasai.llm.base.BaseOpenAI.completion","text":"Query the completion API Parameters: prompt ( str ) \u2013 A string representation of the prompt. Returns: str ( str ) \u2013 LLM response. pandasai\\llm\\base.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def completion ( self , prompt : str ) -> str : \"\"\" Query the completion API Args: prompt (str): A string representation of the prompt. Returns: str: LLM response. \"\"\" params = { ** self . _default_params , \"prompt\" : prompt } if self . stop is not None : params [ \"stop\" ] = [ self . stop ] response = openai . Completion . create ( ** params ) openai_handler = openai_callback_var . get () if openai_handler : openai_handler ( response ) return response [ \"choices\" ][ 0 ][ \"text\" ]","title":"completion()"},{"location":"API/llms/#pandasai.llm.base.HuggingFaceLLM","text":"Bases: LLM Base class to implement a new Hugging Face LLM. LLM base class is extended to be used with HuggingFace LLM Modes APIs. Source code in pandasai\\llm\\base.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 class HuggingFaceLLM ( LLM ): \"\"\"Base class to implement a new Hugging Face LLM. LLM base class is extended to be used with HuggingFace LLM Modes APIs. \"\"\" last_prompt : Optional [ str ] = None api_token : str _api_url : str = \"https://api-inference.huggingface.co/models/\" _max_retries : int = 3 @property def type ( self ) -> str : return \"huggingface-llm\" def _setup ( self , ** kwargs ): \"\"\" Setup the HuggingFace LLM Args: **kwargs: [\"api_token\", \"max_retries\"] \"\"\" self . api_token = ( kwargs . get ( \"api_token\" ) or os . getenv ( \"HUGGINGFACE_API_KEY\" ) or None ) if self . api_token is None : raise APIKeyNotFoundError ( \"HuggingFace Hub API key is required\" ) # Since the huggingface API only returns few tokens at a time, we need to # call the API multiple times to get all the tokens. This is the maximum # number of retries we will do. if kwargs . get ( \"max_retries\" ): self . _max_retries = kwargs . get ( \"max_retries\" ) def __init__ ( self , ** kwargs ): \"\"\" __init__ method of HuggingFaceLLM Class Args: **kwargs: [\"api_token\", \"max_retries\"] \"\"\" self . _setup ( ** kwargs ) def query ( self , payload ) -> str : \"\"\" Query the HF API Args: payload: A JSON form payload Returns: str: Value of the field \"generated_text\" in response JSON given by the remote server. \"\"\" headers = { \"Authorization\" : f \"Bearer { self . api_token } \" } response = requests . post ( self . _api_url , headers = headers , json = payload , timeout = 60 ) return response . json ()[ 0 ][ \"generated_text\" ] def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" A call method of HuggingFaceLLM class. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): A string representing the suffix to be truncated from the generated response. Returns str: LLM response. \"\"\" prompt = instruction . to_string () payload = prompt + suffix # sometimes the API doesn't return a valid response, so we retry passing the # output generated from the previous call as the input for _i in range ( self . _max_retries ): response = self . query ({ \"inputs\" : payload }) payload = response match = re . search ( \"(```python)(.*)(```)\" , response . replace ( prompt + suffix , \"\" ), re . DOTALL | re . MULTILINE , ) if match : break return response . replace ( prompt + suffix , \"\" )","title":"HuggingFaceLLM"},{"location":"API/llms/#pandasai.llm.base.HuggingFaceLLM.__init__","text":"init method of HuggingFaceLLM Class Parameters: **kwargs \u2013 [\"api_token\", \"max_retries\"] pandasai\\llm\\base.py 307 308 309 310 311 312 313 314 315 def __init__ ( self , ** kwargs ): \"\"\" __init__ method of HuggingFaceLLM Class Args: **kwargs: [\"api_token\", \"max_retries\"] \"\"\" self . _setup ( ** kwargs )","title":"__init__()"},{"location":"API/llms/#pandasai.llm.base.HuggingFaceLLM.call","text":"A call method of HuggingFaceLLM class. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): A string representing the suffix to be truncated from the generated response. Returns str: LLM response. pandasai\\llm\\base.py 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" A call method of HuggingFaceLLM class. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): A string representing the suffix to be truncated from the generated response. Returns str: LLM response. \"\"\" prompt = instruction . to_string () payload = prompt + suffix # sometimes the API doesn't return a valid response, so we retry passing the # output generated from the previous call as the input for _i in range ( self . _max_retries ): response = self . query ({ \"inputs\" : payload }) payload = response match = re . search ( \"(```python)(.*)(```)\" , response . replace ( prompt + suffix , \"\" ), re . DOTALL | re . MULTILINE , ) if match : break return response . replace ( prompt + suffix , \"\" )","title":"call()"},{"location":"API/llms/#pandasai.llm.base.HuggingFaceLLM.query","text":"Query the HF API Args: payload: A JSON form payload Returns: str ( str ) \u2013 Value of the field \"generated_text\" in response JSON given by the remote server. pandasai\\llm\\base.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 def query ( self , payload ) -> str : \"\"\" Query the HF API Args: payload: A JSON form payload Returns: str: Value of the field \"generated_text\" in response JSON given by the remote server. \"\"\" headers = { \"Authorization\" : f \"Bearer { self . api_token } \" } response = requests . post ( self . _api_url , headers = headers , json = payload , timeout = 60 ) return response . json ()[ 0 ][ \"generated_text\" ]","title":"query()"},{"location":"API/llms/#pandasai.llm.base.LLM","text":"Base class to implement a new LLM. Source code in pandasai\\llm\\base.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class LLM : \"\"\"Base class to implement a new LLM.\"\"\" last_prompt : Optional [ str ] = None def is_pandasai_llm ( self ) -> bool : \"\"\" Return True if the LLM is from pandasAI. Returns: bool: True if the LLM is from pandasAI \"\"\" return True @property def type ( self ) -> str : \"\"\" Return type of LLM. Raises: APIKeyNotFoundError: Type has not been implemented Returns: str: Type of LLM a string \"\"\" raise APIKeyNotFoundError ( \"Type has not been implemented\" ) def _polish_code ( self , code : str ) -> str : \"\"\" Polish the code by removing the leading \"python\" or \"py\", \\ removing the imports and removing trailing spaces and new lines. Args: code (str): A sting of Python code. Returns: str: Polished code. \"\"\" if re . match ( r \"^(python|py)\" , code ): code = re . sub ( r \"^(python|py)\" , \"\" , code ) if re . match ( r \"^`.*`$\" , code ): code = re . sub ( r \"^`(.*)`$\" , r \"\\1\" , code ) code = code . strip () return code def _is_python_code ( self , string ): \"\"\" Return True if it is valid python code. Args: string (str): Returns (bool): True if Python Code otherwise False \"\"\" try : ast . parse ( string ) return True except SyntaxError : return False def _extract_code ( self , response : str , separator : str = \"```\" ) -> str : \"\"\" Extract the code from the response. Args: response (str): Response separator (str, optional): Separator. Defaults to \"```\". Raises: NoCodeFoundError: No code found in the response Returns: str: Extracted code from the response \"\"\" code = response if len ( code . split ( separator )) > 1 : code = code . split ( separator )[ 1 ] code = self . _polish_code ( code ) if not self . _is_python_code ( code ): raise NoCodeFoundError ( \"No code found in the response\" ) return code @abstractmethod def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Execute the LLM with given prompt. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str, optional): Suffix. Defaults to \"\". Raises: MethodNotImplementedError: Call method has not been implemented \"\"\" raise MethodNotImplementedError ( \"Call method has not been implemented\" ) def generate_code ( self , instruction : AbstractPrompt ) -> str : \"\"\" Generate the code based on the instruction and the given prompt. Args: instruction (AbstractPrompt): Prompt with instruction for LLM. Returns: str: A string of Python code. \"\"\" code = self . call ( instruction , suffix = \"\" ) return self . _extract_code ( code )","title":"LLM"},{"location":"API/llms/#pandasai.llm.base.LLM.type","text":"Return type of LLM. Raises: APIKeyNotFoundError \u2013 Type has not been implemented Returns: str ( str ) \u2013 Type of LLM a string","title":"type"},{"location":"API/llms/#pandasai.llm.base.LLM.call","text":"Execute the LLM with given prompt. Parameters: instruction ( AbstractPrompt ) \u2013 A prompt object with instruction for LLM. suffix ( str , default: '' ) \u2013 Suffix. Defaults to \"\". Raises: MethodNotImplementedError \u2013 Call method has not been implemented pandasai\\llm\\base.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 @abstractmethod def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Execute the LLM with given prompt. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str, optional): Suffix. Defaults to \"\". Raises: MethodNotImplementedError: Call method has not been implemented \"\"\" raise MethodNotImplementedError ( \"Call method has not been implemented\" )","title":"call()"},{"location":"API/llms/#pandasai.llm.base.LLM.generate_code","text":"Generate the code based on the instruction and the given prompt. Parameters: instruction ( AbstractPrompt ) \u2013 Prompt with instruction for LLM. Returns: str ( str ) \u2013 A string of Python code. pandasai\\llm\\base.py 137 138 139 140 141 142 143 144 145 146 147 148 149 def generate_code ( self , instruction : AbstractPrompt ) -> str : \"\"\" Generate the code based on the instruction and the given prompt. Args: instruction (AbstractPrompt): Prompt with instruction for LLM. Returns: str: A string of Python code. \"\"\" code = self . call ( instruction , suffix = \"\" ) return self . _extract_code ( code )","title":"generate_code()"},{"location":"API/llms/#pandasai.llm.base.LLM.is_pandasai_llm","text":"Return True if the LLM is from pandasAI. Returns: bool ( bool ) \u2013 True if the LLM is from pandasAI pandasai\\llm\\base.py 40 41 42 43 44 45 46 47 48 def is_pandasai_llm ( self ) -> bool : \"\"\" Return True if the LLM is from pandasAI. Returns: bool: True if the LLM is from pandasAI \"\"\" return True options: show_root_heading: true","title":"is_pandasai_llm()"},{"location":"API/llms/#openai","text":"OpenAI API wrapper extended through BaseOpenAI class. OpenAI LLM API This module is to run the OpenAI API using OpenAI API. Example Use below example to call OpenAI Model from pandasai.llm.openai import OpenAI","title":"OpenAI"},{"location":"API/llms/#pandasai.llm.openai.OpenAI","text":"Bases: BaseOpenAI OpenAI LLM using BaseOpenAI Class. An API call to OpenAI API is sent and response is recorded and returned. The default chat model is gpt-3.5-turbo . The list of supported Chat models includes [\"gpt-4\", \"gpt-4-0613\", \"gpt-4-32k\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-instruct\"]. The list of supported Completion models includes \"gpt-3.5-turbo-instruct\" and \"text-davinci-003\" (soon to be deprecated). Source code in pandasai\\llm\\openai.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class OpenAI ( BaseOpenAI ): \"\"\"OpenAI LLM using BaseOpenAI Class. An API call to OpenAI API is sent and response is recorded and returned. The default chat model is **gpt-3.5-turbo**. The list of supported Chat models includes [\"gpt-4\", \"gpt-4-0613\", \"gpt-4-32k\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-instruct\"]. The list of supported Completion models includes \"gpt-3.5-turbo-instruct\" and \"text-davinci-003\" (soon to be deprecated). \"\"\" _supported_chat_models = [ \"gpt-4\" , \"gpt-4-0613\" , \"gpt-4-32k\" , \"gpt-4-32k-0613\" , \"gpt-3.5-turbo\" , \"gpt-3.5-turbo-16k\" , \"gpt-3.5-turbo-0613\" , \"gpt-3.5-turbo-16k-0613\" , ] _supported_completion_models = [ \"text-davinci-003\" , \"gpt-3.5-turbo-instruct\" ] model : str = \"gpt-3.5-turbo\" def __init__ ( self , api_token : Optional [ str ] = None , api_key_path : Optional [ str ] = None , ** kwargs , ): \"\"\" __init__ method of OpenAI Class Args: api_token (str): API Token for OpenAI platform. **kwargs: Extended Parameters inferred from BaseOpenAI class \"\"\" self . api_token = api_token or os . getenv ( \"OPENAI_API_KEY\" ) or None self . api_key_path = api_key_path if ( not self . api_token ) and ( not self . api_key_path ): raise APIKeyNotFoundError ( \"Either OpenAI API key or key path is required\" ) if self . api_token : openai . api_key = self . api_token else : openai . api_key_path = self . api_key_path self . openai_proxy = kwargs . get ( \"openai_proxy\" ) or os . getenv ( \"OPENAI_PROXY\" ) if self . openai_proxy : openai . proxy = { \"http\" : self . openai_proxy , \"https\" : self . openai_proxy } self . _set_params ( ** kwargs ) @property def _default_params ( self ) -> Dict [ str , Any ]: \"\"\"Get the default parameters for calling OpenAI API\"\"\" return { ** super () . _default_params , \"model\" : self . model , } def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the OpenAI LLM. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): Suffix to pass. Raises: UnsupportedOpenAIModelError: Unsupported model Returns: str: Response \"\"\" self . last_prompt = instruction . to_string () + suffix if self . model in self . _supported_chat_models : response = self . chat_completion ( self . last_prompt ) elif self . model in self . _supported_completion_models : response = self . completion ( self . last_prompt ) else : raise UnsupportedOpenAIModelError ( \"Unsupported model\" ) return response @property def type ( self ) -> str : return \"openai\"","title":"OpenAI"},{"location":"API/llms/#pandasai.llm.openai.OpenAI.__init__","text":"init method of OpenAI Class Parameters: api_token ( str , default: None ) \u2013 API Token for OpenAI platform. **kwargs \u2013 Extended Parameters inferred from BaseOpenAI class pandasai\\llm\\openai.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , api_token : Optional [ str ] = None , api_key_path : Optional [ str ] = None , ** kwargs , ): \"\"\" __init__ method of OpenAI Class Args: api_token (str): API Token for OpenAI platform. **kwargs: Extended Parameters inferred from BaseOpenAI class \"\"\" self . api_token = api_token or os . getenv ( \"OPENAI_API_KEY\" ) or None self . api_key_path = api_key_path if ( not self . api_token ) and ( not self . api_key_path ): raise APIKeyNotFoundError ( \"Either OpenAI API key or key path is required\" ) if self . api_token : openai . api_key = self . api_token else : openai . api_key_path = self . api_key_path self . openai_proxy = kwargs . get ( \"openai_proxy\" ) or os . getenv ( \"OPENAI_PROXY\" ) if self . openai_proxy : openai . proxy = { \"http\" : self . openai_proxy , \"https\" : self . openai_proxy } self . _set_params ( ** kwargs )","title":"__init__()"},{"location":"API/llms/#pandasai.llm.openai.OpenAI.call","text":"Call the OpenAI LLM. Parameters: instruction ( AbstractPrompt ) \u2013 A prompt object with instruction for LLM. suffix ( str , default: '' ) \u2013 Suffix to pass. Raises: UnsupportedOpenAIModelError \u2013 Unsupported model Returns: str ( str ) \u2013 Response pandasai\\llm\\openai.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the OpenAI LLM. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): Suffix to pass. Raises: UnsupportedOpenAIModelError: Unsupported model Returns: str: Response \"\"\" self . last_prompt = instruction . to_string () + suffix if self . model in self . _supported_chat_models : response = self . chat_completion ( self . last_prompt ) elif self . model in self . _supported_completion_models : response = self . completion ( self . last_prompt ) else : raise UnsupportedOpenAIModelError ( \"Unsupported model\" ) return response options: show_root_heading: true","title":"call()"},{"location":"API/llms/#starcoder-deprecated","text":"Starcoder wrapper extended through Base HuggingFace Class Note: Starcoder is deprecated and will be removed in future versions. Please use another LLM. Starcoder LLM This module is to run the StartCoder API hosted and maintained by HuggingFace.co. To generate HF_TOKEN go to https://huggingface.co/settings/tokens after creating Account on the platform. Example Use below example to call Starcoder Model from pandasai.llm.starcoder import Starcoder","title":"Starcoder (deprecated)"},{"location":"API/llms/#pandasai.llm.starcoder.Starcoder","text":"Bases: HuggingFaceLLM Starcoder LLM API (Deprecated: Kept for backwards compatibility) Source code in pandasai\\llm\\starcoder.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class Starcoder ( HuggingFaceLLM ): \"\"\"Starcoder LLM API (Deprecated: Kept for backwards compatibility)\"\"\" api_token : str _api_url : str = \"https://api-inference.huggingface.co/models/bigcode/starcoder\" _max_retries : int = 30 def __init__ ( self , ** kwargs ): warnings . warn ( \"\"\"Starcoder is deprecated and will be removed in a future release. Please use langchain.llms.HuggingFaceHub instead, although please be aware that it may perform poorly. \"\"\" ) super () . __init__ ( ** kwargs ) @property def type ( self ) -> str : return \"starcoder\" options: show_root_heading: true","title":"Starcoder"},{"location":"API/llms/#falcon-deprecated","text":"Falcon wrapper extended through Base HuggingFace Class Note: Falcon is deprecated and will be removed in future versions. Please use another LLM. Falcon LLM This module is to run the Falcon API hosted and maintained by HuggingFace.co. To generate HF_TOKEN go to https://huggingface.co/settings/tokens after creating Account on the platform. Example Use below example to call Falcon Model from pandasai.llm.falcon import Falcon","title":"Falcon (deprecated)"},{"location":"API/llms/#pandasai.llm.falcon.Falcon","text":"Bases: HuggingFaceLLM Falcon LLM API (Deprecated: Kept for backwards compatibility) Source code in pandasai\\llm\\falcon.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Falcon ( HuggingFaceLLM ): \"\"\"Falcon LLM API (Deprecated: Kept for backwards compatibility)\"\"\" api_token : str _api_url : str = ( \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\" ) _max_retries : int = 30 def __init__ ( self , ** kwargs ): warnings . warn ( \"\"\"Falcon is deprecated and will be removed in a future release. Please use langchain.llms.HuggingFaceHub instead, although please be aware that it may perform poorly. \"\"\" ) super () . __init__ ( ** kwargs ) @property def type ( self ) -> str : return \"falcon\" options: show_root_heading: true","title":"Falcon"},{"location":"API/llms/#azure-openai","text":"OpenAI API through Azure Platform wrapper OpenAI LLM via Microsoft Azure Cloud This module is to run the OpenAI API when using Microsoft Cloud infrastructure. Azure has implemented the openai API access to its platform. For details https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference. Example Use below example to call AzureOpenAI class from pandasai.llm.azure_openai import AzureOpenAI","title":"Azure OpenAI"},{"location":"API/llms/#pandasai.llm.azure_openai.AzureOpenAI","text":"Bases: BaseOpenAI OpenAI LLM via Microsoft Azure This class uses BaseOpenAI class to support Azure OpenAI features. Source code in pandasai\\llm\\azure_openai.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class AzureOpenAI ( BaseOpenAI ): \"\"\"OpenAI LLM via Microsoft Azure This class uses `BaseOpenAI` class to support Azure OpenAI features. \"\"\" api_base : str api_type : str = \"azure\" api_version : str engine : str def __init__ ( self , api_token : Optional [ str ] = None , api_base : Optional [ str ] = None , api_version : Optional [ str ] = None , deployment_name : str = None , is_chat_model : bool = True , ** kwargs , ): \"\"\" __init__ method of AzureOpenAI Class. Args: api_token (str): Azure OpenAI API token. api_base (str): Base url of the Azure endpoint. It should look like the following: <https://YOUR_RESOURCE_NAME.openai.azure.com/> api_version (str): Version of the Azure OpenAI API. Be aware the API version may change. deployment_name (str): Custom name of the deployed model is_chat_model (bool): Whether ``deployment_name`` corresponds to a Chat or a Completion model. **kwargs: Inference Parameters. \"\"\" self . api_token = api_token or os . getenv ( \"OPENAI_API_KEY\" ) or None self . api_base = api_base or os . getenv ( \"OPENAI_API_BASE\" ) or None self . api_version = api_version or os . getenv ( \"OPENAI_API_VERSION\" ) if self . api_token is None : raise APIKeyNotFoundError ( \"Azure OpenAI key is required. Please add an environment variable \" \"`OPENAI_API_KEY` or pass `api_token` as a named parameter\" ) if self . api_base is None : raise APIKeyNotFoundError ( \"Azure OpenAI base is required. Please add an environment variable \" \"`OPENAI_API_BASE` or pass `api_base` as a named parameter\" ) if self . api_version is None : raise APIKeyNotFoundError ( \"Azure OpenAI version is required. Please add an environment variable \" \"`OPENAI_API_VERSION` or pass `api_version` as a named parameter\" ) openai . api_key = self . api_token openai . api_base = self . api_base openai . api_version = self . api_version openai . api_type = self . api_type if deployment_name is None : raise UnsupportedOpenAIModelError ( \"Model deployment name is required.\" ) self . is_chat_model = is_chat_model self . engine = deployment_name self . openai_proxy = kwargs . get ( \"openai_proxy\" ) or os . getenv ( \"OPENAI_PROXY\" ) if self . openai_proxy : openai . proxy = { \"http\" : self . openai_proxy , \"https\" : self . openai_proxy } self . _set_params ( ** kwargs ) @property def _default_params ( self ) -> Dict [ str , Any ]: \"\"\" Get the default parameters for calling OpenAI API. Returns: dict: A dictionary containing Default Params. \"\"\" return { ** super () . _default_params , \"engine\" : self . engine } def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the Azure OpenAI LLM. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): Suffix to pass. Returns: str: LLM response. \"\"\" self . last_prompt = instruction . to_string () + suffix if self . is_chat_model : response = self . chat_completion ( self . last_prompt ) else : response = self . completion ( self . last_prompt ) return response @property def type ( self ) -> str : return \"azure-openai\"","title":"AzureOpenAI"},{"location":"API/llms/#pandasai.llm.azure_openai.AzureOpenAI.__init__","text":"init method of AzureOpenAI Class. Parameters: api_token ( str , default: None ) \u2013 Azure OpenAI API token. api_base ( str , default: None ) \u2013 Base url of the Azure endpoint. It should look like the following: https://YOUR_RESOURCE_NAME.openai.azure.com/ api_version ( str , default: None ) \u2013 Version of the Azure OpenAI API. Be aware the API version may change. deployment_name ( str , default: None ) \u2013 Custom name of the deployed model is_chat_model ( bool , default: True ) \u2013 Whether deployment_name corresponds to a Chat or a Completion model. **kwargs \u2013 Inference Parameters. pandasai\\llm\\azure_openai.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , api_token : Optional [ str ] = None , api_base : Optional [ str ] = None , api_version : Optional [ str ] = None , deployment_name : str = None , is_chat_model : bool = True , ** kwargs , ): \"\"\" __init__ method of AzureOpenAI Class. Args: api_token (str): Azure OpenAI API token. api_base (str): Base url of the Azure endpoint. It should look like the following: <https://YOUR_RESOURCE_NAME.openai.azure.com/> api_version (str): Version of the Azure OpenAI API. Be aware the API version may change. deployment_name (str): Custom name of the deployed model is_chat_model (bool): Whether ``deployment_name`` corresponds to a Chat or a Completion model. **kwargs: Inference Parameters. \"\"\" self . api_token = api_token or os . getenv ( \"OPENAI_API_KEY\" ) or None self . api_base = api_base or os . getenv ( \"OPENAI_API_BASE\" ) or None self . api_version = api_version or os . getenv ( \"OPENAI_API_VERSION\" ) if self . api_token is None : raise APIKeyNotFoundError ( \"Azure OpenAI key is required. Please add an environment variable \" \"`OPENAI_API_KEY` or pass `api_token` as a named parameter\" ) if self . api_base is None : raise APIKeyNotFoundError ( \"Azure OpenAI base is required. Please add an environment variable \" \"`OPENAI_API_BASE` or pass `api_base` as a named parameter\" ) if self . api_version is None : raise APIKeyNotFoundError ( \"Azure OpenAI version is required. Please add an environment variable \" \"`OPENAI_API_VERSION` or pass `api_version` as a named parameter\" ) openai . api_key = self . api_token openai . api_base = self . api_base openai . api_version = self . api_version openai . api_type = self . api_type if deployment_name is None : raise UnsupportedOpenAIModelError ( \"Model deployment name is required.\" ) self . is_chat_model = is_chat_model self . engine = deployment_name self . openai_proxy = kwargs . get ( \"openai_proxy\" ) or os . getenv ( \"OPENAI_PROXY\" ) if self . openai_proxy : openai . proxy = { \"http\" : self . openai_proxy , \"https\" : self . openai_proxy } self . _set_params ( ** kwargs )","title":"__init__()"},{"location":"API/llms/#pandasai.llm.azure_openai.AzureOpenAI.call","text":"Call the Azure OpenAI LLM. Parameters: instruction ( AbstractPrompt ) \u2013 A prompt object with instruction for LLM. suffix ( str , default: '' ) \u2013 Suffix to pass. Returns: str ( str ) \u2013 LLM response. pandasai\\llm\\azure_openai.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : \"\"\" Call the Azure OpenAI LLM. Args: instruction (AbstractPrompt): A prompt object with instruction for LLM. suffix (str): Suffix to pass. Returns: str: LLM response. \"\"\" self . last_prompt = instruction . to_string () + suffix if self . is_chat_model : response = self . chat_completion ( self . last_prompt ) else : response = self . completion ( self . last_prompt ) return response options: show_root_heading: true","title":"call()"},{"location":"API/llms/#googlepalm","text":"GooglePalm class extended through BaseGoogle Class Google Palm LLM This module is to run the Google PaLM API hosted and maintained by Google. To read more on Google PaLM follow https://developers.generativeai.google/products/palm. Example Use below example to call GooglePalm Model from pandasai.llm.google_palm import GooglePalm","title":"GooglePalm"},{"location":"API/llms/#pandasai.llm.google_palm.GooglePalm","text":"Bases: BaseGoogle Google Palm LLM BaseGoogle class is extended for Google Palm model. The default and only model support at the moment is models/text-bison-001. Source code in pandasai\\llm\\google_palm.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class GooglePalm ( BaseGoogle ): \"\"\"Google Palm LLM BaseGoogle class is extended for Google Palm model. The default and only model support at the moment is models/text-bison-001. \"\"\" model : str = \"models/text-bison-001\" google_palm : Any def __init__ ( self , api_key : str , ** kwargs ): \"\"\" __init__ method of GooglePalm Class Args: api_key (str): API Key **kwargs: Extended Parameters inferred from BaseGoogle Class \"\"\" self . _configure ( api_key = api_key ) self . _set_params ( ** kwargs ) def _configure ( self , api_key : str ): \"\"\" Configure Google Palm API Key Args: api_key (str): A string of API keys generated from Google Cloud. Returns: None. \"\"\" if not api_key : raise APIKeyNotFoundError ( \"Google Palm API key is required\" ) err_msg = \"Install google-generativeai >= 0.1 for Google Palm API\" google_palm = import_dependency ( \"google.generativeai\" , extra = err_msg ) google_palm . configure ( api_key = api_key ) self . google_palm = google_palm def _valid_params ( self ): \"\"\"Returns if the Parameters are valid or Not\"\"\" return super () . _valid_params () + [ \"model\" ] def _validate ( self ): \"\"\" A method to Validate the Model \"\"\" super () . _validate () if not self . model : raise ValueError ( \"model is required.\" ) def _generate_text ( self , prompt : str ) -> str : \"\"\" Generates text for prompt. Args: prompt (str): A string representation of the prompt. Returns: str: LLM response. \"\"\" self . _validate () completion = self . google_palm . generate_text ( model = self . model , prompt = prompt , temperature = self . temperature , top_p = self . top_p , top_k = self . top_k , max_output_tokens = self . max_output_tokens , ) return completion . result @property def type ( self ) -> str : return \"google-palm\"","title":"GooglePalm"},{"location":"API/llms/#pandasai.llm.google_palm.GooglePalm.__init__","text":"init method of GooglePalm Class Args: api_key (str): API Key **kwargs: Extended Parameters inferred from BaseGoogle Class pandasai\\llm\\google_palm.py 28 29 30 31 32 33 34 35 36 def __init__ ( self , api_key : str , ** kwargs ): \"\"\" __init__ method of GooglePalm Class Args: api_key (str): API Key **kwargs: Extended Parameters inferred from BaseGoogle Class \"\"\" self . _configure ( api_key = api_key ) self . _set_params ( ** kwargs ) options: show_root_heading: true","title":"__init__()"},{"location":"API/llms/#fake","text":"A test fake class Fake LLM","title":"Fake"},{"location":"API/llms/#pandasai.llm.fake.FakeLLM","text":"Bases: LLM Fake LLM Source code in pandasai\\llm\\fake.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class FakeLLM ( LLM ): \"\"\"Fake LLM\"\"\" _output : str = \"\"\"def analyze_data(dfs): return { 'type': 'text', 'value': \"Hello World\" }\"\"\" def __init__ ( self , output : Optional [ str ] = None ): if output is not None : self . _output = output def call ( self , instruction : AbstractPrompt , suffix : str = \"\" ) -> str : self . last_prompt = instruction . to_string () + suffix return self . _output @property def type ( self ) -> str : return \"fake\" options: show_root_heading: true","title":"FakeLLM"},{"location":"API/pandasai/","text":"PANDASAI This Section of API covers the BaseModule Implementation along with some Package Constants and Exceptions. Main The init of pandasai module contains the a high level wrapper to run the package. pandasai PandasAI is a wrapper around a LLM to make dataframes conversational This module includes the implementation of basis PandasAI class with methods to run the LLMs models on Pandas dataframes. Following LLMs are implemented so far. Example: This module is the Entry point of the `pandasai` package. Following is an example of how to use this Class. ```python import pandas as pd from pandasai import PandasAI # Sample DataFrame df = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) # Instantiate a LLM from pandasai.llm.openai import OpenAI llm = OpenAI(api_token=\"YOUR_API_TOKEN\") pandas_ai = PandasAI(llm) pandas_ai(df, prompt='Which are the 5 happiest countries?') ``` Agent Agent class to improve the conversational experience in PandasAI Source code in pandasai\\agent\\__init__.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 class Agent : \"\"\" Agent class to improve the conversational experience in PandasAI \"\"\" _lake : SmartDatalake = None _logger : Optional [ Logger ] = None def __init__ ( self , dfs : Union [ DataFrameType , List [ DataFrameType ]], config : Optional [ Union [ Config , dict ]] = None , logger : Optional [ Logger ] = None , memory_size : int = 1 , ): \"\"\" Args: df (Union[DataFrameType, List[DataFrameType]]): DataFrame can be Pandas, Polars or Database connectors memory_size (int, optional): Conversation history to use during chat. Defaults to 1. \"\"\" if not isinstance ( dfs , list ): dfs = [ dfs ] self . _lake = SmartDatalake ( dfs , config , logger , memory = Memory ( memory_size )) self . _logger = self . _lake . logger def _call_llm_with_prompt ( self , prompt : AbstractPrompt ): \"\"\" Call LLM with prompt using error handling to retry based on config Args: prompt (AbstractPrompt): AbstractPrompt to pass to LLM's \"\"\" retry_count = 0 while retry_count < self . _lake . config . max_retries : try : result : str = self . _lake . llm . call ( prompt ) if prompt . validate ( result ): return result else : raise Exception ( \"Response validation failed!\" ) except Exception : if ( not self . _lake . use_error_correction_framework or retry_count >= self . _lake . config . max_retries - 1 ): raise retry_count += 1 def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Simulate a chat interaction with the assistant on Dataframe. \"\"\" try : result = self . _lake . chat ( query , output_type = output_type ) return result except Exception as exception : return ( \"Unfortunately, I was not able to get your answers, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) def clarification_questions ( self , query : str ) -> List [ str ]: \"\"\" Generate clarification questions based on the data \"\"\" prompt = ClarificationQuestionPrompt ( dataframes = self . _lake . dfs , conversation = self . _lake . _memory . get_conversation (), query = query , ) result = self . _call_llm_with_prompt ( prompt ) self . _logger . log ( f \"\"\"Clarification Questions: { result } \"\"\" ) questions : list [ str ] = json . loads ( result ) return questions [: 3 ] def start_new_conversation ( self ): \"\"\" Clears the previous conversation \"\"\" self . _lake . clear_memory () def explain ( self ) -> str : \"\"\" Returns the explanation of the code how it reached to the solution \"\"\" try : prompt = ExplainPrompt ( conversation = self . _lake . _memory . get_conversation (), code = self . _lake . last_code_executed , ) response = self . _call_llm_with_prompt ( prompt ) self . _logger . log ( f \"\"\"Explaination: { response } \"\"\" ) return response except Exception as exception : return ( \"Unfortunately, I was not able to explain, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) def rephrase_query ( self , query : str ): try : prompt = RephraseQueryPrompt ( query = query , dataframes = self . _lake . dfs , conversation = self . _lake . _memory . get_conversation (), ) response = self . _call_llm_with_prompt ( prompt ) self . _logger . log ( f \"\"\"Rephrased Response: { response } \"\"\" ) return response except Exception as exception : return ( \"Unfortunately, I was not able to repharse query, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) __init__ ( dfs , config = None , logger = None , memory_size = 1 ) Parameters: df ( Union [ DataFrameType , List [ DataFrameType ]] ) \u2013 DataFrame can be Pandas, memory_size ( int , default: 1 ) \u2013 Conversation history to use during chat. pandasai\\agent\\__init__.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , dfs : Union [ DataFrameType , List [ DataFrameType ]], config : Optional [ Union [ Config , dict ]] = None , logger : Optional [ Logger ] = None , memory_size : int = 1 , ): \"\"\" Args: df (Union[DataFrameType, List[DataFrameType]]): DataFrame can be Pandas, Polars or Database connectors memory_size (int, optional): Conversation history to use during chat. Defaults to 1. \"\"\" if not isinstance ( dfs , list ): dfs = [ dfs ] self . _lake = SmartDatalake ( dfs , config , logger , memory = Memory ( memory_size )) self . _logger = self . _lake . logger chat ( query , output_type = None ) Simulate a chat interaction with the assistant on Dataframe. pandasai\\agent\\__init__.py 65 66 67 68 69 70 71 72 73 74 75 76 77 def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Simulate a chat interaction with the assistant on Dataframe. \"\"\" try : result = self . _lake . chat ( query , output_type = output_type ) return result except Exception as exception : return ( \"Unfortunately, I was not able to get your answers, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) clarification_questions ( query ) Generate clarification questions based on the data pandasai\\agent\\__init__.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def clarification_questions ( self , query : str ) -> List [ str ]: \"\"\" Generate clarification questions based on the data \"\"\" prompt = ClarificationQuestionPrompt ( dataframes = self . _lake . dfs , conversation = self . _lake . _memory . get_conversation (), query = query , ) result = self . _call_llm_with_prompt ( prompt ) self . _logger . log ( f \"\"\"Clarification Questions: { result } \"\"\" ) questions : list [ str ] = json . loads ( result ) return questions [: 3 ] explain () Returns the explanation of the code how it reached to the solution pandasai\\agent\\__init__.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def explain ( self ) -> str : \"\"\" Returns the explanation of the code how it reached to the solution \"\"\" try : prompt = ExplainPrompt ( conversation = self . _lake . _memory . get_conversation (), code = self . _lake . last_code_executed , ) response = self . _call_llm_with_prompt ( prompt ) self . _logger . log ( f \"\"\"Explaination: { response } \"\"\" ) return response except Exception as exception : return ( \"Unfortunately, I was not able to explain, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) start_new_conversation () Clears the previous conversation pandasai\\agent\\__init__.py 97 98 99 100 101 def start_new_conversation ( self ): \"\"\" Clears the previous conversation \"\"\" self . _lake . clear_memory () PandasAI PandasAI is a wrapper around a LLM to make dataframes conversational. This is an entry point of pandasai object. This class consists of methods to interface the LLMs with Pandas dataframes. A pandas dataframe metadata i.e. df.head() and prompt is passed on to chosen LLMs API end point to generate a Python code to answer the questions asked. The resultant python code is run on actual data and answer is converted into a conversational form. Note Do not include the self parameter in the Args section. Args: _llm (obj): LLMs option to be used for API access _verbose (bool, optional): To show the intermediate outputs e.g. python code generated and execution step on the prompt. Default to False _enforce_privacy (bool, optional): Do not display the data on prompt in case of Sensitive data. Default to False _max_retries (int, optional): max no. of tries to generate code on failure. Default to 3 _original_instructions (dict, optional): The dict of instruction to run. Default to None _cache (Cache, optional): Cache object to store the results. Default to None _enable_cache (bool, optional): Whether to enable cache. Default to True _logger (logging.Logger, optional): Logger object to log the messages. Default to None _logs (List[dict], optional): List of logs to be stored. Default to [] _prompt_id (str, optional): Unique ID to differentiate calls. Default to None _middlewares (List[Middleware], optional): List of middlewares to run. Default to [ChartsMiddleware()] _additional_dependencies (List[dict], optional): List of additional dependencies to be added. Default to [] _custom_whitelisted_dependencies (List[str], optional): List of custom whitelisted dependencies. Default to [] last_code_generated (str, optional): Pass last Code if generated. Default to None last_code_executed (str, optional): Pass the last execution / run. Default to None code_output (str, optional): The code output if any. Default to None last_error (str, optional): Error of running code last time. Default to None prompt_id (str, optional): Unique ID to differentiate calls. Default to None Returns (str): Response to a Question related to Data Source code in pandasai\\__init__.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class PandasAI : \"\"\" PandasAI is a wrapper around a LLM to make dataframes conversational. This is an entry point of `pandasai` object. This class consists of methods to interface the LLMs with Pandas dataframes. A pandas dataframe metadata i.e. df.head() and prompt is passed on to chosen LLMs API end point to generate a Python code to answer the questions asked. The resultant python code is run on actual data and answer is converted into a conversational form. Note: Do not include the `self` parameter in the ``Args`` section. Args: _llm (obj): LLMs option to be used for API access _verbose (bool, optional): To show the intermediate outputs e.g. python code generated and execution step on the prompt. Default to False _enforce_privacy (bool, optional): Do not display the data on prompt in case of Sensitive data. Default to False _max_retries (int, optional): max no. of tries to generate code on failure. Default to 3 _original_instructions (dict, optional): The dict of instruction to run. Default to None _cache (Cache, optional): Cache object to store the results. Default to None _enable_cache (bool, optional): Whether to enable cache. Default to True _logger (logging.Logger, optional): Logger object to log the messages. Default to None _logs (List[dict], optional): List of logs to be stored. Default to [] _prompt_id (str, optional): Unique ID to differentiate calls. Default to None _middlewares (List[Middleware], optional): List of middlewares to run. Default to [ChartsMiddleware()] _additional_dependencies (List[dict], optional): List of additional dependencies to be added. Default to [] _custom_whitelisted_dependencies (List[str], optional): List of custom whitelisted dependencies. Default to [] last_code_generated (str, optional): Pass last Code if generated. Default to None last_code_executed (str, optional): Pass the last execution / run. Default to None code_output (str, optional): The code output if any. Default to None last_error (str, optional): Error of running code last time. Default to None prompt_id (str, optional): Unique ID to differentiate calls. Default to None Returns (str): Response to a Question related to Data \"\"\" _dl : SmartDatalake = None _config : Union [ Config , dict ] def __init__ ( self , llm = None , conversational = False , verbose = False , enforce_privacy = False , save_charts = False , save_charts_path = \"\" , enable_cache = True , middlewares = None , custom_whitelisted_dependencies = None , enable_logging = True , non_default_prompts : Optional [ Dict [ str , Type [ AbstractPrompt ]]] = None , callback : Optional [ BaseCallback ] = None , ): \"\"\" __init__ method of the Class PandasAI Args: llm (object): LLMs option to be used for API access. Default is None conversational (bool): Whether to return answer in conversational form. Default to False verbose (bool): To show the intermediate outputs e.g. python code generated and execution step on the prompt. Default to False enforce_privacy (bool): Execute the codes with Privacy Mode ON. Default to False save_charts (bool): Save the charts generated in the notebook. Default to False enable_cache (bool): Enable the cache to store the results. Default to True middlewares (list): List of middlewares to be used. Default to None custom_whitelisted_dependencies (list): List of custom dependencies to be used. Default to None enable_logging (bool): Enable the logging. Default to True non_default_prompts (dict): Mapping from keys to replacement prompt classes. Used to override specific types of prompts. Defaults to None. \"\"\" # configure the logging # noinspection PyArgumentList # https://stackoverflow.com/questions/61226587/pycharm-does-not-recognize-logging-basicconfig-handlers-argument warnings . warn ( \"`PandasAI` (class) is deprecated since v1.0 and will be removed \" \"in a future release. Please use `SmartDataframe` instead.\" ) self . _config = Config ( conversational = conversational , verbose = verbose , enforce_privacy = enforce_privacy , save_charts = save_charts , save_charts_path = save_charts_path , enable_cache = enable_cache , middlewares = middlewares or [], custom_whitelisted_dependencies = custom_whitelisted_dependencies or [], enable_logging = enable_logging , non_default_prompts = non_default_prompts , llm = llm , callback = callback , ) def run ( self , data_frame : Union [ pd . DataFrame , List [ pd . DataFrame ]], prompt : str , show_code : bool = False , anonymize_df : bool = True , use_error_correction_framework : bool = True , ) -> Union [ str , pd . DataFrame ]: \"\"\" Run the PandasAI to make Dataframes Conversational. Args: data_frame (Union[pd.DataFrame, List[pd.DataFrame]]): A pandas Dataframe prompt (str): A prompt to query about the Dataframe show_code (bool): To show the intermediate python code generated on the prompt. Default to False anonymize_df (bool): Running the code with Sensitive Data. Default to True use_error_correction_framework (bool): Turn on Error Correction mechanism. Default to True Returns (str): Answer to the Input Questions about the DataFrame \"\"\" new_config = self . _config . dict () new_config [ \"show_code\" ] = show_code new_config [ \"anonymize_df\" ] = anonymize_df new_config [ \"use_error_correction_framework\" ] = use_error_correction_framework config = Config ( ** new_config ) . dict () if not isinstance ( data_frame , list ): data_frame = [ data_frame ] self . _dl = SmartDatalake ( data_frame , config ) return self . _dl . chat ( prompt ) def __call__ ( self , data_frame : Union [ pd . DataFrame , List [ pd . DataFrame ]], prompt : str , show_code : bool = False , anonymize_df : bool = True , use_error_correction_framework : bool = True , ) -> Union [ str , pd . DataFrame ]: \"\"\" __call__ method of PandasAI class. It calls the `run` method. Args: data_frame: prompt: show_code: anonymize_df: use_error_correction_framework: Returns (str): Answer to the Input Questions about the DataFrame. \"\"\" return self . run ( data_frame , prompt , show_code , anonymize_df , use_error_correction_framework , ) @property def logs ( self ) -> List [ dict [ str , str ]]: \"\"\"Return the logs\"\"\" if self . _dl is None : return [] return self . _dl . logs @property def last_prompt_id ( self ) -> str : \"\"\"Return the id of the last prompt that was run.\"\"\" if self . _dl is None : return None return self . _dl . last_prompt_id @property def last_prompt ( self ) -> str : \"\"\"Return the last prompt that was executed.\"\"\" if self . _dl is None : return None return self . _dl . last_prompt last_prompt : str property Return the last prompt that was executed. last_prompt_id : str property Return the id of the last prompt that was run. logs : List [ dict [ str , str ]] property Return the logs __call__ ( data_frame , prompt , show_code = False , anonymize_df = True , use_error_correction_framework = True ) call method of PandasAI class. It calls the run method. Parameters: data_frame ( Union [ DataFrame , List [ DataFrame ]] ) \u2013 prompt ( str ) \u2013 show_code ( bool , default: False ) \u2013 anonymize_df ( bool , default: True ) \u2013 use_error_correction_framework ( bool , default: True ) \u2013 Returns (str): Answer to the Input Questions about the DataFrame. pandasai\\__init__.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def __call__ ( self , data_frame : Union [ pd . DataFrame , List [ pd . DataFrame ]], prompt : str , show_code : bool = False , anonymize_df : bool = True , use_error_correction_framework : bool = True , ) -> Union [ str , pd . DataFrame ]: \"\"\" __call__ method of PandasAI class. It calls the `run` method. Args: data_frame: prompt: show_code: anonymize_df: use_error_correction_framework: Returns (str): Answer to the Input Questions about the DataFrame. \"\"\" return self . run ( data_frame , prompt , show_code , anonymize_df , use_error_correction_framework , ) __init__ ( llm = None , conversational = False , verbose = False , enforce_privacy = False , save_charts = False , save_charts_path = '' , enable_cache = True , middlewares = None , custom_whitelisted_dependencies = None , enable_logging = True , non_default_prompts = None , callback = None ) init method of the Class PandasAI Parameters: llm ( object , default: None ) \u2013 LLMs option to be used for API access. Default is None conversational ( bool , default: False ) \u2013 Whether to return answer in conversational form. verbose ( bool , default: False ) \u2013 To show the intermediate outputs e.g. python code enforce_privacy ( bool , default: False ) \u2013 Execute the codes with Privacy Mode ON. save_charts ( bool , default: False ) \u2013 Save the charts generated in the notebook. enable_cache ( bool , default: True ) \u2013 Enable the cache to store the results. middlewares ( list , default: None ) \u2013 List of middlewares to be used. Default to None custom_whitelisted_dependencies ( list , default: None ) \u2013 List of custom dependencies to enable_logging ( bool , default: True ) \u2013 Enable the logging. Default to True non_default_prompts ( dict , default: None ) \u2013 Mapping from keys to replacement prompt classes. pandasai\\__init__.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def __init__ ( self , llm = None , conversational = False , verbose = False , enforce_privacy = False , save_charts = False , save_charts_path = \"\" , enable_cache = True , middlewares = None , custom_whitelisted_dependencies = None , enable_logging = True , non_default_prompts : Optional [ Dict [ str , Type [ AbstractPrompt ]]] = None , callback : Optional [ BaseCallback ] = None , ): \"\"\" __init__ method of the Class PandasAI Args: llm (object): LLMs option to be used for API access. Default is None conversational (bool): Whether to return answer in conversational form. Default to False verbose (bool): To show the intermediate outputs e.g. python code generated and execution step on the prompt. Default to False enforce_privacy (bool): Execute the codes with Privacy Mode ON. Default to False save_charts (bool): Save the charts generated in the notebook. Default to False enable_cache (bool): Enable the cache to store the results. Default to True middlewares (list): List of middlewares to be used. Default to None custom_whitelisted_dependencies (list): List of custom dependencies to be used. Default to None enable_logging (bool): Enable the logging. Default to True non_default_prompts (dict): Mapping from keys to replacement prompt classes. Used to override specific types of prompts. Defaults to None. \"\"\" # configure the logging # noinspection PyArgumentList # https://stackoverflow.com/questions/61226587/pycharm-does-not-recognize-logging-basicconfig-handlers-argument warnings . warn ( \"`PandasAI` (class) is deprecated since v1.0 and will be removed \" \"in a future release. Please use `SmartDataframe` instead.\" ) self . _config = Config ( conversational = conversational , verbose = verbose , enforce_privacy = enforce_privacy , save_charts = save_charts , save_charts_path = save_charts_path , enable_cache = enable_cache , middlewares = middlewares or [], custom_whitelisted_dependencies = custom_whitelisted_dependencies or [], enable_logging = enable_logging , non_default_prompts = non_default_prompts , llm = llm , callback = callback , ) run ( data_frame , prompt , show_code = False , anonymize_df = True , use_error_correction_framework = True ) Run the PandasAI to make Dataframes Conversational. Parameters: data_frame ( Union [ DataFrame , List [ DataFrame ]] ) \u2013 A pandas Dataframe prompt ( str ) \u2013 A prompt to query about the Dataframe show_code ( bool , default: False ) \u2013 To show the intermediate python code generated on the anonymize_df ( bool , default: True ) \u2013 Running the code with Sensitive Data. Default to True use_error_correction_framework ( bool , default: True ) \u2013 Turn on Error Correction mechanism. Returns (str): Answer to the Input Questions about the DataFrame pandasai\\__init__.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def run ( self , data_frame : Union [ pd . DataFrame , List [ pd . DataFrame ]], prompt : str , show_code : bool = False , anonymize_df : bool = True , use_error_correction_framework : bool = True , ) -> Union [ str , pd . DataFrame ]: \"\"\" Run the PandasAI to make Dataframes Conversational. Args: data_frame (Union[pd.DataFrame, List[pd.DataFrame]]): A pandas Dataframe prompt (str): A prompt to query about the Dataframe show_code (bool): To show the intermediate python code generated on the prompt. Default to False anonymize_df (bool): Running the code with Sensitive Data. Default to True use_error_correction_framework (bool): Turn on Error Correction mechanism. Default to True Returns (str): Answer to the Input Questions about the DataFrame \"\"\" new_config = self . _config . dict () new_config [ \"show_code\" ] = show_code new_config [ \"anonymize_df\" ] = anonymize_df new_config [ \"use_error_correction_framework\" ] = use_error_correction_framework config = Config ( ** new_config ) . dict () if not isinstance ( data_frame , list ): data_frame = [ data_frame ] self . _dl = SmartDatalake ( data_frame , config ) return self . _dl . chat ( prompt ) SmartDataframe Bases: DataframeAbstract , Shortcuts Source code in pandasai\\smart_dataframe\\__init__.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 class SmartDataframe ( DataframeAbstract , Shortcuts ): _table_name : str _table_description : str _sample_head : str = None _original_import : any _core : SmartDataframeCore _lake : SmartDatalake def __init__ ( self , df : DataFrameType , name : str = None , description : str = None , sample_head : pd . DataFrame = None , config : Config = None , logger : Logger = None , ): \"\"\" Args: df (Union[pd.DataFrame, pl.DataFrame]): Pandas or Polars dataframe name (str, optional): Name of the dataframe. Defaults to None. description (str, optional): Description of the dataframe. Defaults to \"\". sample_head (pd.DataFrame, optional): Sample head of the dataframe. config (Config, optional): Config to be used. Defaults to None. logger (Logger, optional): Logger to be used. Defaults to None. \"\"\" self . _original_import = df if isinstance ( df , str ): if not ( df . endswith ( \".csv\" ) or df . endswith ( \".parquet\" ) or df . endswith ( \".xlsx\" ) or df . startswith ( \"https://docs.google.com/spreadsheets/\" ) ): df_config = self . _load_from_config ( df ) if df_config : if \"://\" in df_config [ \"import_path\" ]: connector_name = df_config [ \"import_path\" ] . split ( \"://\" )[ 0 ] connector_path = df_config [ \"import_path\" ] . split ( \"://\" )[ 1 ] connector_host = connector_path . split ( \":\" )[ 0 ] connector_port = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 0 ] connector_database = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 1 ] connector_table = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 2 ] connector_data = { \"host\" : connector_host , \"database\" : connector_database , \"table\" : connector_table , } if connector_port : connector_data [ \"port\" ] = connector_port # instantiate the connector df = getattr ( __import__ ( \"pandasai.connectors\" , fromlist = [ connector_name ] ), connector_name , )( config = connector_data ) else : df = df_config [ \"import_path\" ] if name is None : name = df_config [ \"name\" ] if description is None : description = df_config [ \"description\" ] else : raise ValueError ( \"Could not find a saved dataframe configuration \" \"with the given name.\" ) self . _core = SmartDataframeCore ( df , logger ) self . _table_name = name self . _table_description = description self . _lake = SmartDatalake ([ self ], config , logger ) # If no name is provided, use the fallback name provided the connector if self . _table_name is None and self . connector : self . _table_name = self . connector . fallback_name if sample_head is not None : self . _sample_head = sample_head . to_csv ( index = False ) def add_middlewares ( self , * middlewares : Optional [ Middleware ]): \"\"\" Add middlewares to PandasAI instance. Args: *middlewares: Middlewares to be added \"\"\" self . lake . add_middlewares ( * middlewares ) def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Run a query on the dataframe. Args: query (str): Query to run on the dataframe output_type (Optional[str]): Add a hint for LLM of which type should be returned by `analyze_data()` in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object Raises: ValueError: If the query is empty \"\"\" return self . lake . chat ( query , output_type ) def column_hash ( self ) -> str : \"\"\" Get the hash of the columns of the dataframe. Returns: str: Hash of the columns of the dataframe \"\"\" if not self . _core . _df_loaded and self . connector : return self . connector . column_hash columns_str = \"\" . join ( self . dataframe . columns ) hash_object = hashlib . sha256 ( columns_str . encode ()) return hash_object . hexdigest () def save ( self , name : str = None ): \"\"\" Saves the dataframe configuration to be used for later Args: name (str, optional): Name of the dataframe configuration. Defaults to None. \"\"\" config_manager = DfConfigManager ( self ) config_manager . save ( name ) def load_connector ( self , temporary : bool = False ): \"\"\" Load a connector into the smart dataframe Args: temporary (bool, optional): Whether the connector is temporary or not. Defaults to False. \"\"\" self . _core . load_connector ( temporary ) def _truncate_head_columns ( self , df : DataFrameType , max_size = 25 ) -> DataFrameType : \"\"\" Truncate the columns of the dataframe to a maximum of 20 characters. Args: df (DataFrameType): Pandas or Polars dataframe Returns: DataFrameType: Pandas or Polars dataframe \"\"\" if df_type ( df ) == \"pandas\" : df_trunc = df . copy () for col in df . columns : if df [ col ] . dtype == \"object\" : first_val = df [ col ] . iloc [ 0 ] if isinstance ( first_val , str ) and len ( first_val ) > max_size : df_trunc [ col ] = df_trunc [ col ] . str . slice ( 0 , max_size - 3 ) + \"...\" elif df_type ( df ) == \"polars\" : try : import polars as pl df_trunc = df . clone () for col in df . columns : if df [ col ] . dtype == pl . Utf8 : first_val = df [ col ][ 0 ] if isinstance ( first_val , str ) and len ( df_trunc [ col ]) > max_size : df_trunc [ col ] = ( df_trunc [ col ] . str . slice ( 0 , max_size - 3 ) + \"...\" ) except ImportError : raise ImportError ( \"Polars is not installed. \" \"Please install Polars to use this feature.\" ) return df_trunc def _get_sample_head ( self ) -> DataFrameType : head = None rows_to_display = 0 if self . lake . config . enforce_privacy else 5 if not self . _core . _df_loaded and self . connector : head = self . connector . head () else : head = self . dataframe . head ( rows_to_display ) if head is None : return None sampler = DataSampler ( head ) sampled_head = sampler . sample ( rows_to_display ) return self . _truncate_head_columns ( sampled_head ) def _load_from_config ( self , name : str ): \"\"\" Loads a saved dataframe configuration \"\"\" config_manager = DfConfigManager ( self ) return config_manager . load ( name ) @property def dataframe ( self ) -> DataFrameType : return self . _core . dataframe @property def engine ( self ): return self . _core . engine @property def connector ( self ): return self . _core . connector @connector . setter def connector ( self , connector : BaseConnector ): connector . logger = self . logger self . _core . connector = connector def validate ( self , schema : pydantic . BaseModel ): \"\"\" Validates Dataframe rows on the basis Pydantic schema input (Args): schema: Pydantic schema class verbose: Print Errors \"\"\" df_validator = DfValidator ( self . dataframe ) return df_validator . validate ( schema ) @property def lake ( self ) -> SmartDatalake : return self . _lake @lake . setter def lake ( self , lake : SmartDatalake ): self . _lake = lake @property def rows_count ( self ): if self . _core . _df_loaded : return self . dataframe . shape [ 0 ] elif self . connector is not None : return self . connector . rows_count else : raise ValueError ( \"Cannot determine rows_count. No dataframe or connector loaded.\" ) @property def columns_count ( self ): if self . _core . _df_loaded : return self . dataframe . shape [ 1 ] elif self . connector is not None : return self . connector . columns_count else : raise ValueError ( \"Cannot determine columns_count. No dataframe or connector loaded.\" ) @cached_property def head_df ( self ): \"\"\" Get the head of the dataframe as a dataframe. Returns: DataFrameType: Pandas or Polars dataframe \"\"\" return self . _get_sample_head () @cached_property def head_csv ( self ): \"\"\" Get the head of the dataframe as a CSV string. Returns: str: CSV string \"\"\" df_head = self . _get_sample_head () return df_head . to_csv ( index = False ) @property def last_prompt ( self ): return self . lake . last_prompt @property def last_prompt_id ( self ) -> str : return self . lake . last_prompt_id @property def last_code_generated ( self ): return self . lake . last_code_executed @property def last_code_executed ( self ): return self . lake . last_code_executed @property def last_result ( self ): return self . lake . last_result @property def last_error ( self ): return self . lake . last_error @property def cache ( self ): return self . lake . cache @property def middlewares ( self ): return self . lake . middlewares def original_import ( self ): return self . _original_import @property def logger ( self ): return self . lake . logger @logger . setter def logger ( self , logger : Logger ): self . lake . logger = logger @property def logs ( self ): return self . lake . logs @property def verbose ( self ): return self . lake . verbose @verbose . setter def verbose ( self , verbose : bool ): self . lake . verbose = verbose @property def save_logs ( self ): return self . lake . save_logs @save_logs . setter def save_logs ( self , save_logs : bool ): self . lake . save_logs = save_logs @property def callback ( self ): return self . lake . callback @callback . setter def callback ( self , callback : BaseCallback ): self . lake . callback = callback @property def enforce_privacy ( self ): return self . lake . enforce_privacy @enforce_privacy . setter def enforce_privacy ( self , enforce_privacy : bool ): self . lake . enforce_privacy = enforce_privacy @property def enable_cache ( self ): return self . lake . enable_cache @enable_cache . setter def enable_cache ( self , enable_cache : bool ): self . lake . enable_cache = enable_cache @property def use_error_correction_framework ( self ): return self . lake . use_error_correction_framework @use_error_correction_framework . setter def use_error_correction_framework ( self , use_error_correction_framework : bool ): self . lake . use_error_correction_framework = use_error_correction_framework @property def custom_prompts ( self ): return self . lake . custom_prompts @custom_prompts . setter def custom_prompts ( self , custom_prompts : dict ): self . lake . custom_prompts = custom_prompts @property def save_charts ( self ): return self . lake . save_charts @save_charts . setter def save_charts ( self , save_charts : bool ): self . lake . save_charts = save_charts @property def save_charts_path ( self ): return self . lake . save_charts_path @save_charts_path . setter def save_charts_path ( self , save_charts_path : str ): self . lake . save_charts_path = save_charts_path @property def custom_whitelisted_dependencies ( self ): return self . lake . custom_whitelisted_dependencies @custom_whitelisted_dependencies . setter def custom_whitelisted_dependencies ( self , custom_whitelisted_dependencies : List [ str ] ): self . lake . custom_whitelisted_dependencies = custom_whitelisted_dependencies @property def max_retries ( self ): return self . lake . max_retries @max_retries . setter def max_retries ( self , max_retries : int ): self . lake . max_retries = max_retries @property def llm ( self ): return self . lake . llm @llm . setter def llm ( self , llm : Union [ LLM , LangchainLLM ]): self . lake . llm = llm @property def table_name ( self ): return self . _table_name @property def table_description ( self ): return self . _table_description @property def sample_head ( self ): data = StringIO ( self . _sample_head ) return pd . read_csv ( data ) @sample_head . setter def sample_head ( self , sample_head : pd . DataFrame ): self . _sample_head = sample_head . to_csv ( index = False ) def __getattr__ ( self , name ): if name in self . _core . __dir__ (): return getattr ( self . _core , name ) elif name in self . dataframe . __dir__ (): return getattr ( self . dataframe , name ) else : return self . __getattribute__ ( name ) def __getitem__ ( self , key ): return self . dataframe . __getitem__ ( key ) def __setitem__ ( self , key , value ): return self . dataframe . __setitem__ ( key , value ) def __dir__ ( self ): return dir ( self . _core ) + dir ( self . dataframe ) + dir ( self . __class__ ) def __repr__ ( self ): return self . dataframe . __repr__ () def __len__ ( self ): return len ( self . dataframe ) head_csv cached property Get the head of the dataframe as a CSV string. Returns: str \u2013 CSV string head_df cached property Get the head of the dataframe as a dataframe. Returns: DataFrameType \u2013 Pandas or Polars dataframe __init__ ( df , name = None , description = None , sample_head = None , config = None , logger = None ) Parameters: df ( Union [ DataFrame , DataFrame ] ) \u2013 Pandas or Polars dataframe name ( str , default: None ) \u2013 Name of the dataframe. Defaults to None. description ( str , default: None ) \u2013 Description of the dataframe. Defaults to \"\". sample_head ( DataFrame , default: None ) \u2013 Sample head of the dataframe. config ( Config , default: None ) \u2013 Config to be used. Defaults to None. logger ( Logger , default: None ) \u2013 Logger to be used. Defaults to None. pandasai\\smart_dataframe\\__init__.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def __init__ ( self , df : DataFrameType , name : str = None , description : str = None , sample_head : pd . DataFrame = None , config : Config = None , logger : Logger = None , ): \"\"\" Args: df (Union[pd.DataFrame, pl.DataFrame]): Pandas or Polars dataframe name (str, optional): Name of the dataframe. Defaults to None. description (str, optional): Description of the dataframe. Defaults to \"\". sample_head (pd.DataFrame, optional): Sample head of the dataframe. config (Config, optional): Config to be used. Defaults to None. logger (Logger, optional): Logger to be used. Defaults to None. \"\"\" self . _original_import = df if isinstance ( df , str ): if not ( df . endswith ( \".csv\" ) or df . endswith ( \".parquet\" ) or df . endswith ( \".xlsx\" ) or df . startswith ( \"https://docs.google.com/spreadsheets/\" ) ): df_config = self . _load_from_config ( df ) if df_config : if \"://\" in df_config [ \"import_path\" ]: connector_name = df_config [ \"import_path\" ] . split ( \"://\" )[ 0 ] connector_path = df_config [ \"import_path\" ] . split ( \"://\" )[ 1 ] connector_host = connector_path . split ( \":\" )[ 0 ] connector_port = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 0 ] connector_database = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 1 ] connector_table = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 2 ] connector_data = { \"host\" : connector_host , \"database\" : connector_database , \"table\" : connector_table , } if connector_port : connector_data [ \"port\" ] = connector_port # instantiate the connector df = getattr ( __import__ ( \"pandasai.connectors\" , fromlist = [ connector_name ] ), connector_name , )( config = connector_data ) else : df = df_config [ \"import_path\" ] if name is None : name = df_config [ \"name\" ] if description is None : description = df_config [ \"description\" ] else : raise ValueError ( \"Could not find a saved dataframe configuration \" \"with the given name.\" ) self . _core = SmartDataframeCore ( df , logger ) self . _table_name = name self . _table_description = description self . _lake = SmartDatalake ([ self ], config , logger ) # If no name is provided, use the fallback name provided the connector if self . _table_name is None and self . connector : self . _table_name = self . connector . fallback_name if sample_head is not None : self . _sample_head = sample_head . to_csv ( index = False ) add_middlewares ( * middlewares ) Add middlewares to PandasAI instance. Parameters: *middlewares ( Optional [ Middleware ] , default: () ) \u2013 Middlewares to be added pandasai\\smart_dataframe\\__init__.py 312 313 314 315 316 317 318 319 320 def add_middlewares ( self , * middlewares : Optional [ Middleware ]): \"\"\" Add middlewares to PandasAI instance. Args: *middlewares: Middlewares to be added \"\"\" self . lake . add_middlewares ( * middlewares ) chat ( query , output_type = None ) Run a query on the dataframe. Parameters: query ( str ) \u2013 Query to run on the dataframe output_type ( Optional [ str ] , default: None ) \u2013 Add a hint for LLM of which type should be returned by analyze_data() in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object Raises: ValueError \u2013 If the query is empty pandasai\\smart_dataframe\\__init__.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Run a query on the dataframe. Args: query (str): Query to run on the dataframe output_type (Optional[str]): Add a hint for LLM of which type should be returned by `analyze_data()` in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object Raises: ValueError: If the query is empty \"\"\" return self . lake . chat ( query , output_type ) column_hash () Get the hash of the columns of the dataframe. Returns: str ( str ) \u2013 Hash of the columns of the dataframe pandasai\\smart_dataframe\\__init__.py 345 346 347 348 349 350 351 352 353 354 355 356 357 def column_hash ( self ) -> str : \"\"\" Get the hash of the columns of the dataframe. Returns: str: Hash of the columns of the dataframe \"\"\" if not self . _core . _df_loaded and self . connector : return self . connector . column_hash columns_str = \"\" . join ( self . dataframe . columns ) hash_object = hashlib . sha256 ( columns_str . encode ()) return hash_object . hexdigest () load_connector ( temporary = False ) Load a connector into the smart dataframe Parameters: temporary ( bool , default: False ) \u2013 Whether the connector is temporary or not. pandasai\\smart_dataframe\\__init__.py 370 371 372 373 374 375 376 377 378 def load_connector ( self , temporary : bool = False ): \"\"\" Load a connector into the smart dataframe Args: temporary (bool, optional): Whether the connector is temporary or not. Defaults to False. \"\"\" self . _core . load_connector ( temporary ) save ( name = None ) Saves the dataframe configuration to be used for later Parameters: name ( str , default: None ) \u2013 Name of the dataframe configuration. Defaults to None. pandasai\\smart_dataframe\\__init__.py 359 360 361 362 363 364 365 366 367 368 def save ( self , name : str = None ): \"\"\" Saves the dataframe configuration to be used for later Args: name (str, optional): Name of the dataframe configuration. Defaults to None. \"\"\" config_manager = DfConfigManager ( self ) config_manager . save ( name ) validate ( schema ) Validates Dataframe rows on the basis Pydantic schema input (Args): schema: Pydantic schema class verbose: Print Errors pandasai\\smart_dataframe\\__init__.py 461 462 463 464 465 466 467 468 469 def validate ( self , schema : pydantic . BaseModel ): \"\"\" Validates Dataframe rows on the basis Pydantic schema input (Args): schema: Pydantic schema class verbose: Print Errors \"\"\" df_validator = DfValidator ( self . dataframe ) return df_validator . validate ( schema ) SmartDatalake Source code in pandasai\\smart_datalake\\__init__.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 class SmartDatalake : _dfs : List [ DataFrameType ] _config : Union [ Config , dict ] _llm : LLM _cache : Cache = None _logger : Logger _start_time : float _last_prompt_id : uuid . UUID _code_manager : CodeManager _memory : Memory _last_code_generated : str _last_result : str = None _last_error : str = None def __init__ ( self , dfs : List [ Union [ DataFrameType , Any ]], config : Optional [ Union [ Config , dict ]] = None , logger : Logger = None , memory : Memory = None , cache : Cache = None , ): \"\"\" Args: dfs (List[Union[DataFrameType, Any]]): List of dataframes to be used config (Union[Config, dict], optional): Config to be used. Defaults to None. logger (Logger, optional): Logger to be used. Defaults to None. \"\"\" self . initialize () self . _load_config ( config ) if logger : self . logger = logger else : self . logger = Logger ( save_logs = self . _config . save_logs , verbose = self . _config . verbose ) self . _load_dfs ( dfs ) if memory : self . _memory = memory else : self . _memory = Memory () self . _code_manager = CodeManager ( dfs = self . _dfs , config = self . _config , logger = self . logger , ) if cache : self . _cache = cache elif self . _config . enable_cache : self . _cache = Cache () context = Context ( self . _config , self . logger , self . engine ) if self . _config . response_parser : self . _response_parser = self . _config . response_parser ( context ) else : self . _response_parser = ResponseParser ( context ) def initialize ( self ): \"\"\"Initialize the SmartDatalake\"\"\" # Create exports/charts folder if it doesn't exist try : charts_dir = os . path . join (( find_project_root ()), \"exports\" , \"charts\" ) except ValueError : charts_dir = os . path . join ( os . getcwd (), \"exports\" , \"charts\" ) os . makedirs ( charts_dir , mode = 0o777 , exist_ok = True ) # Create /cache folder if it doesn't exist try : cache_dir = os . path . join (( find_project_root ()), \"cache\" ) except ValueError : cache_dir = os . path . join ( os . getcwd (), \"cache\" ) os . makedirs ( cache_dir , mode = 0o777 , exist_ok = True ) def _load_dfs ( self , dfs : List [ Union [ DataFrameType , Any ]]): \"\"\" Load all the dataframes to be used in the smart datalake. Args: dfs (List[Union[DataFrameType, Any]]): List of dataframes to be used \"\"\" from ..smart_dataframe import SmartDataframe smart_dfs = [] for df in dfs : if not isinstance ( df , SmartDataframe ): smart_dfs . append ( SmartDataframe ( df , config = self . _config , logger = self . logger ) ) else : smart_dfs . append ( df ) self . _dfs = smart_dfs def _load_config ( self , config : Union [ Config , dict ]): \"\"\" Load a config to be used to run the queries. Args: config (Union[Config, dict]): Config to be used \"\"\" config = load_config ( config ) if config . get ( \"llm\" ): self . _load_llm ( config [ \"llm\" ]) config [ \"llm\" ] = self . _llm self . _config = Config ( ** config ) def _load_llm ( self , llm : LLM ): \"\"\" Load a LLM to be used to run the queries. Check if it is a PandasAI LLM or a Langchain LLM. If it is a Langchain LLM, wrap it in a PandasAI LLM. Args: llm (object): LLMs option to be used for API access Raises: BadImportError: If the LLM is a Langchain LLM but the langchain package is not installed \"\"\" if hasattr ( llm , \"_llm_type\" ): llm = LangchainLLM ( llm ) self . _llm = llm def add_middlewares ( self , * middlewares : Optional [ Middleware ]): \"\"\" Add middlewares to PandasAI instance. Args: *middlewares: Middlewares to be added \"\"\" self . _code_manager . add_middlewares ( * middlewares ) def _start_timer ( self ): \"\"\"Start the timer\"\"\" self . _start_time = time . time () def _assign_prompt_id ( self ): \"\"\"Assign a prompt ID\"\"\" self . _last_prompt_id = uuid . uuid4 () if self . logger : self . logger . log ( f \"Prompt ID: { self . _last_prompt_id } \" ) def _get_prompt ( self , key : str , default_prompt : Type [ AbstractPrompt ], default_values : Optional [ dict ] = None , ) -> AbstractPrompt : \"\"\" Return a prompt by key. Args: key (str): The key of the prompt default_prompt (Type[AbstractPrompt]): The default prompt to use default_values (Optional[dict], optional): The default values to use for the prompt. Defaults to None. Returns: AbstractPrompt: The prompt \"\"\" if default_values is None : default_values = {} custom_prompt = self . _config . custom_prompts . get ( key ) prompt = custom_prompt if custom_prompt else default_prompt () # set default values for the prompt if \"dfs\" not in default_values : prompt . set_var ( \"dfs\" , self . _dfs ) if \"conversation\" not in default_values : prompt . set_var ( \"conversation\" , self . _memory . get_conversation ()) for key , value in default_values . items (): prompt . set_var ( key , value ) self . logger . log ( f \"Using prompt: { prompt } \" ) return prompt def _get_cache_key ( self ) -> str : \"\"\" Return the cache key for the current conversation. Returns: str: The cache key for the current conversation \"\"\" cache_key = self . _memory . get_conversation () # make the cache key unique for each combination of dfs for df in self . _dfs : hash = df . column_hash () cache_key += str ( hash ) return cache_key def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Run a query on the dataframe. Args: query (str): Query to run on the dataframe output_type (Optional[str]): Add a hint for LLM which type should be returned by `analyze_data()` in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object If none `output_type` is specified, the type can be any of the above or \"text\". Raises: ValueError: If the query is empty \"\"\" self . _start_timer () self . logger . log ( f \"Question: { query } \" ) self . logger . log ( f \"Running PandasAI with { self . _llm . type } LLM...\" ) self . _assign_prompt_id () self . _memory . add ( query , True ) try : output_type_helper = output_type_factory ( output_type , logger = self . logger ) if ( self . _config . enable_cache and self . _cache and self . _cache . get ( self . _get_cache_key ()) ): self . logger . log ( \"Using cached response\" ) code = self . _cache . get ( self . _get_cache_key ()) else : default_values = { # TODO: find a better way to determine the engine, \"engine\" : self . _dfs [ 0 ] . engine , \"output_type_hint\" : output_type_helper . template_hint , } generate_python_code_instruction = self . _get_prompt ( \"generate_python_code\" , default_prompt = GeneratePythonCodePrompt , default_values = default_values , ) code = self . _llm . generate_code ( generate_python_code_instruction ) if self . _config . enable_cache and self . _cache : self . _cache . set ( self . _get_cache_key (), code ) if self . _config . callback is not None : self . _config . callback . on_code ( code ) self . last_code_generated = code self . logger . log ( f \"\"\"Code generated: ``` { code } ``` \"\"\" ) retry_count = 0 code_to_run = code result = None while retry_count < self . _config . max_retries : try : # Execute the code result = self . _code_manager . execute_code ( code = code_to_run , prompt_id = self . _last_prompt_id , ) break except Exception as e : if ( not self . _config . use_error_correction_framework or retry_count >= self . _config . max_retries - 1 ): raise e retry_count += 1 self . _logger . log ( f \"Failed to execute code with a correction framework \" f \"[retry number: { retry_count } ]\" , level = logging . WARNING , ) traceback_error = traceback . format_exc () code_to_run = self . _retry_run_code ( code , traceback_error ) if result is not None : if isinstance ( result , dict ): validation_ok , validation_logs = output_type_helper . validate ( result ) if not validation_ok : self . logger . log ( \" \\n \" . join ( validation_logs ), level = logging . WARNING ) self . last_result = result self . logger . log ( f \"Answer: { result } \" ) except Exception as exception : self . last_error = str ( exception ) return ( \"Unfortunately, I was not able to answer your question, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) self . logger . log ( f \"Executed in: { time . time () - self . _start_time } s\" ) self . _add_result_to_memory ( result ) return self . _response_parser . parse ( result ) def _add_result_to_memory ( self , result : dict ): \"\"\" Add the result to the memory. Args: result (dict): The result to add to the memory \"\"\" if result is None : return if result [ \"type\" ] == \"string\" or result [ \"type\" ] == \"number\" : self . _memory . add ( result [ \"value\" ], False ) elif result [ \"type\" ] == \"dataframe\" or result [ \"type\" ] == \"plot\" : self . _memory . add ( \"Ok here it is\" , False ) def _retry_run_code ( self , code : str , e : Exception ): \"\"\" A method to retry the code execution with error correction framework. Args: code (str): A python code e (Exception): An exception dataframes Returns (str): A python code \"\"\" self . logger . log ( f \"Failed with error: { e } . Retrying\" , logging . ERROR ) default_values = { \"engine\" : self . _dfs [ 0 ] . engine , \"code\" : code , \"error_returned\" : e , } error_correcting_instruction = self . _get_prompt ( \"correct_error\" , default_prompt = CorrectErrorPrompt , default_values = default_values , ) code = self . _llm . generate_code ( error_correcting_instruction ) if self . _config . callback is not None : self . _config . callback . on_code ( code ) return code def clear_memory ( self ): \"\"\" Clears the memory \"\"\" self . _memory . clear () @property def engine ( self ): return self . _dfs [ 0 ] . engine @property def last_prompt ( self ): return self . _llm . last_prompt @property def last_prompt_id ( self ) -> str : \"\"\"Return the id of the last prompt that was run.\"\"\" if self . _last_prompt_id is None : raise ValueError ( \"Pandas AI has not been run yet.\" ) return self . _last_prompt_id @property def logs ( self ): return self . logger . logs @property def logger ( self ): return self . _logger @logger . setter def logger ( self , logger ): self . _logger = logger @property def config ( self ): return self . _config @property def cache ( self ): return self . _cache @property def middlewares ( self ): return self . _code_manager . middlewares @property def verbose ( self ): return self . _config . verbose @verbose . setter def verbose ( self , verbose : bool ): self . _config . verbose = verbose self . _logger . verbose = verbose @property def save_logs ( self ): return self . _config . save_logs @save_logs . setter def save_logs ( self , save_logs : bool ): self . _config . save_logs = save_logs self . _logger . save_logs = save_logs @property def callback ( self ): return self . _config . callback @callback . setter def callback ( self , callback : Any ): self . config . callback = callback @property def enforce_privacy ( self ): return self . _config . enforce_privacy @enforce_privacy . setter def enforce_privacy ( self , enforce_privacy : bool ): self . _config . enforce_privacy = enforce_privacy @property def enable_cache ( self ): return self . _config . enable_cache @enable_cache . setter def enable_cache ( self , enable_cache : bool ): self . _config . enable_cache = enable_cache if enable_cache : if self . cache is None : self . _cache = Cache () else : self . _cache = None @property def use_error_correction_framework ( self ): return self . _config . use_error_correction_framework @use_error_correction_framework . setter def use_error_correction_framework ( self , use_error_correction_framework : bool ): self . _config . use_error_correction_framework = use_error_correction_framework @property def custom_prompts ( self ): return self . _config . custom_prompts @custom_prompts . setter def custom_prompts ( self , custom_prompts : dict ): self . _config . custom_prompts = custom_prompts @property def save_charts ( self ): return self . _config . save_charts @save_charts . setter def save_charts ( self , save_charts : bool ): self . _config . save_charts = save_charts @property def save_charts_path ( self ): return self . _config . save_charts_path @save_charts_path . setter def save_charts_path ( self , save_charts_path : str ): self . _config . save_charts_path = save_charts_path @property def custom_whitelisted_dependencies ( self ): return self . _config . custom_whitelisted_dependencies @custom_whitelisted_dependencies . setter def custom_whitelisted_dependencies ( self , custom_whitelisted_dependencies : List [ str ] ): self . _config . custom_whitelisted_dependencies = custom_whitelisted_dependencies @property def max_retries ( self ): return self . _config . max_retries @max_retries . setter def max_retries ( self , max_retries : int ): self . _config . max_retries = max_retries @property def llm ( self ): return self . _llm @llm . setter def llm ( self , llm : LLM ): self . _load_llm ( llm ) @property def last_code_generated ( self ): return self . _last_code_generated @last_code_generated . setter def last_code_generated ( self , last_code_generated : str ): self . _last_code_generated = last_code_generated @property def last_code_executed ( self ): return self . _code_manager . last_code_executed @property def last_result ( self ): return self . _last_result @last_result . setter def last_result ( self , last_result : str ): self . _last_result = last_result @property def last_error ( self ): return self . _last_error @last_error . setter def last_error ( self , last_error : str ): self . _last_error = last_error @property def dfs ( self ): return self . _dfs @property def memory ( self ): return self . _memory last_prompt_id : str property Return the id of the last prompt that was run. __init__ ( dfs , config = None , logger = None , memory = None , cache = None ) Parameters: dfs ( List [ Union [ DataFrameType , Any ]] ) \u2013 List of dataframes to be used config ( Union [ Config , dict ] , default: None ) \u2013 Config to be used. Defaults to None. logger ( Logger , default: None ) \u2013 Logger to be used. Defaults to None. pandasai\\smart_datalake\\__init__.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def __init__ ( self , dfs : List [ Union [ DataFrameType , Any ]], config : Optional [ Union [ Config , dict ]] = None , logger : Logger = None , memory : Memory = None , cache : Cache = None , ): \"\"\" Args: dfs (List[Union[DataFrameType, Any]]): List of dataframes to be used config (Union[Config, dict], optional): Config to be used. Defaults to None. logger (Logger, optional): Logger to be used. Defaults to None. \"\"\" self . initialize () self . _load_config ( config ) if logger : self . logger = logger else : self . logger = Logger ( save_logs = self . _config . save_logs , verbose = self . _config . verbose ) self . _load_dfs ( dfs ) if memory : self . _memory = memory else : self . _memory = Memory () self . _code_manager = CodeManager ( dfs = self . _dfs , config = self . _config , logger = self . logger , ) if cache : self . _cache = cache elif self . _config . enable_cache : self . _cache = Cache () context = Context ( self . _config , self . logger , self . engine ) if self . _config . response_parser : self . _response_parser = self . _config . response_parser ( context ) else : self . _response_parser = ResponseParser ( context ) add_middlewares ( * middlewares ) Add middlewares to PandasAI instance. Parameters: *middlewares ( Optional [ Middleware ] , default: () ) \u2013 Middlewares to be added pandasai\\smart_datalake\\__init__.py 184 185 186 187 188 189 190 191 def add_middlewares ( self , * middlewares : Optional [ Middleware ]): \"\"\" Add middlewares to PandasAI instance. Args: *middlewares: Middlewares to be added \"\"\" self . _code_manager . add_middlewares ( * middlewares ) chat ( query , output_type = None ) Run a query on the dataframe. Parameters: query ( str ) \u2013 Query to run on the dataframe output_type ( Optional [ str ] , default: None ) \u2013 Add a hint for LLM which type should be returned by analyze_data() in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object If none output_type is specified, the type can be any of the above or \"text\". Raises: ValueError \u2013 If the query is empty pandasai\\smart_datalake\\__init__.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Run a query on the dataframe. Args: query (str): Query to run on the dataframe output_type (Optional[str]): Add a hint for LLM which type should be returned by `analyze_data()` in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object If none `output_type` is specified, the type can be any of the above or \"text\". Raises: ValueError: If the query is empty \"\"\" self . _start_timer () self . logger . log ( f \"Question: { query } \" ) self . logger . log ( f \"Running PandasAI with { self . _llm . type } LLM...\" ) self . _assign_prompt_id () self . _memory . add ( query , True ) try : output_type_helper = output_type_factory ( output_type , logger = self . logger ) if ( self . _config . enable_cache and self . _cache and self . _cache . get ( self . _get_cache_key ()) ): self . logger . log ( \"Using cached response\" ) code = self . _cache . get ( self . _get_cache_key ()) else : default_values = { # TODO: find a better way to determine the engine, \"engine\" : self . _dfs [ 0 ] . engine , \"output_type_hint\" : output_type_helper . template_hint , } generate_python_code_instruction = self . _get_prompt ( \"generate_python_code\" , default_prompt = GeneratePythonCodePrompt , default_values = default_values , ) code = self . _llm . generate_code ( generate_python_code_instruction ) if self . _config . enable_cache and self . _cache : self . _cache . set ( self . _get_cache_key (), code ) if self . _config . callback is not None : self . _config . callback . on_code ( code ) self . last_code_generated = code self . logger . log ( f \"\"\"Code generated: ``` { code } ``` \"\"\" ) retry_count = 0 code_to_run = code result = None while retry_count < self . _config . max_retries : try : # Execute the code result = self . _code_manager . execute_code ( code = code_to_run , prompt_id = self . _last_prompt_id , ) break except Exception as e : if ( not self . _config . use_error_correction_framework or retry_count >= self . _config . max_retries - 1 ): raise e retry_count += 1 self . _logger . log ( f \"Failed to execute code with a correction framework \" f \"[retry number: { retry_count } ]\" , level = logging . WARNING , ) traceback_error = traceback . format_exc () code_to_run = self . _retry_run_code ( code , traceback_error ) if result is not None : if isinstance ( result , dict ): validation_ok , validation_logs = output_type_helper . validate ( result ) if not validation_ok : self . logger . log ( \" \\n \" . join ( validation_logs ), level = logging . WARNING ) self . last_result = result self . logger . log ( f \"Answer: { result } \" ) except Exception as exception : self . last_error = str ( exception ) return ( \"Unfortunately, I was not able to answer your question, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) self . logger . log ( f \"Executed in: { time . time () - self . _start_time } s\" ) self . _add_result_to_memory ( result ) return self . _response_parser . parse ( result ) clear_memory () Clears the memory pandasai\\smart_datalake\\__init__.py 429 430 431 432 433 def clear_memory ( self ): \"\"\" Clears the memory \"\"\" self . _memory . clear () initialize () Initialize the SmartDatalake pandasai\\smart_datalake\\__init__.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def initialize ( self ): \"\"\"Initialize the SmartDatalake\"\"\" # Create exports/charts folder if it doesn't exist try : charts_dir = os . path . join (( find_project_root ()), \"exports\" , \"charts\" ) except ValueError : charts_dir = os . path . join ( os . getcwd (), \"exports\" , \"charts\" ) os . makedirs ( charts_dir , mode = 0o777 , exist_ok = True ) # Create /cache folder if it doesn't exist try : cache_dir = os . path . join (( find_project_root ()), \"cache\" ) except ValueError : cache_dir = os . path . join ( os . getcwd (), \"cache\" ) os . makedirs ( cache_dir , mode = 0o777 , exist_ok = True ) clear_cache ( filename = None ) Clear the cache pandasai\\__init__.py 254 255 256 257 def clear_cache ( filename : str = None ): \"\"\"Clear the cache\"\"\" cache = Cache ( filename or \"cache_db\" ) cache . clear () Constants Some of the package level constants are defined here. pandasai.constants Constants used in the pandasai package. It includes Start & End Code tags, Whitelisted Python Packages and While List Builtin Methods. Exception Handling The pandasai specific Exception handling mechanism defined here. pandasai.exceptions PandasAI's custom exceptions. This module contains the implementation of Custom Exceptions. APIKeyNotFoundError Bases: Exception Raised when the API key is not defined/declared. Parameters: Exception ( Exception ) \u2013 APIKeyNotFoundError Source code in pandasai\\exceptions.py 8 9 10 11 12 13 14 15 class APIKeyNotFoundError ( Exception ): \"\"\" Raised when the API key is not defined/declared. Args: Exception (Exception): APIKeyNotFoundError \"\"\" BadImportError Bases: Exception Raised when a library not in the whitelist is imported. Parameters: Exception ( Exception ) \u2013 BadImportError Source code in pandasai\\exceptions.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class BadImportError ( Exception ): \"\"\" Raised when a library not in the whitelist is imported. Args: Exception (Exception): BadImportError \"\"\" def __init__ ( self , library_name ): \"\"\" __init__ method of BadImportError Class Args: library_name (str): Name of the library that is not in the whitelist. \"\"\" self . library_name = library_name super () . __init__ ( f \"Generated code includes import of { library_name } which\" \" is not in whitelist.\" ) __init__ ( library_name ) init method of BadImportError Class Parameters: library_name ( str ) \u2013 Name of the library that is not in the whitelist. pandasai\\exceptions.py 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , library_name ): \"\"\" __init__ method of BadImportError Class Args: library_name (str): Name of the library that is not in the whitelist. \"\"\" self . library_name = library_name super () . __init__ ( f \"Generated code includes import of { library_name } which\" \" is not in whitelist.\" ) LLMNotFoundError Bases: Exception Raised when the LLM is not provided. Parameters: Exception ( Exception ) \u2013 LLMNotFoundError Source code in pandasai\\exceptions.py 18 19 20 21 22 23 24 class LLMNotFoundError ( Exception ): \"\"\" Raised when the LLM is not provided. Args: Exception (Exception): LLMNotFoundError \"\"\" MethodNotImplementedError Bases: Exception Raised when a method is not implemented. Parameters: Exception ( Exception ) \u2013 MethodNotImplementedError Source code in pandasai\\exceptions.py 36 37 38 39 40 41 42 class MethodNotImplementedError ( Exception ): \"\"\" Raised when a method is not implemented. Args: Exception (Exception): MethodNotImplementedError \"\"\" NoCodeFoundError Bases: Exception Raised when no code is found in the response. Parameters: Exception ( Exception ) \u2013 NoCodeFoundError Source code in pandasai\\exceptions.py 27 28 29 30 31 32 33 class NoCodeFoundError ( Exception ): \"\"\" Raised when no code is found in the response. Args: Exception (Exception): NoCodeFoundError \"\"\" TemplateFileNotFoundError Bases: FileNotFoundError Raised when a template file cannot be found. Source code in pandasai\\exceptions.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class TemplateFileNotFoundError ( FileNotFoundError ): \"\"\" Raised when a template file cannot be found. \"\"\" def __init__ ( self , template_path , prompt_name = \"Unknown\" ): \"\"\" __init__ method of TemplateFileNotFoundError Class Args: template_path (str): Path for template file. prompt_name (str): Prompt name. Defaults to \"Unknown\". \"\"\" self . template_path = template_path super () . __init__ ( f \"Unable to find a file with template at ' { template_path } ' \" f \"for ' { prompt_name } ' prompt.\" ) __init__ ( template_path , prompt_name = 'Unknown' ) init method of TemplateFileNotFoundError Class Parameters: template_path ( str ) \u2013 Path for template file. prompt_name ( str , default: 'Unknown' ) \u2013 Prompt name. Defaults to \"Unknown\". pandasai\\exceptions.py 81 82 83 84 85 86 87 88 89 90 91 92 93 def __init__ ( self , template_path , prompt_name = \"Unknown\" ): \"\"\" __init__ method of TemplateFileNotFoundError Class Args: template_path (str): Path for template file. prompt_name (str): Prompt name. Defaults to \"Unknown\". \"\"\" self . template_path = template_path super () . __init__ ( f \"Unable to find a file with template at ' { template_path } ' \" f \"for ' { prompt_name } ' prompt.\" ) UnsupportedOpenAIModelError Bases: Exception Raised when an unsupported OpenAI model is used. Parameters: Exception ( Exception ) \u2013 UnsupportedOpenAIModelError Source code in pandasai\\exceptions.py 45 46 47 48 49 50 51 class UnsupportedOpenAIModelError ( Exception ): \"\"\" Raised when an unsupported OpenAI model is used. Args: Exception (Exception): UnsupportedOpenAIModelError \"\"\"","title":"Pandasai"},{"location":"API/pandasai/#pandasai_1","text":"This Section of API covers the BaseModule Implementation along with some Package Constants and Exceptions.","title":"PANDASAI"},{"location":"API/pandasai/#main","text":"The init of pandasai module contains the a high level wrapper to run the package.","title":"Main"},{"location":"API/pandasai/#pandasai","text":"PandasAI is a wrapper around a LLM to make dataframes conversational This module includes the implementation of basis PandasAI class with methods to run the LLMs models on Pandas dataframes. Following LLMs are implemented so far. Example: This module is the Entry point of the `pandasai` package. Following is an example of how to use this Class. ```python import pandas as pd from pandasai import PandasAI # Sample DataFrame df = pd.DataFrame({ \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"], \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064], \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12] }) # Instantiate a LLM from pandasai.llm.openai import OpenAI llm = OpenAI(api_token=\"YOUR_API_TOKEN\") pandas_ai = PandasAI(llm) pandas_ai(df, prompt='Which are the 5 happiest countries?') ```","title":"pandasai"},{"location":"API/pandasai/#pandasai.Agent","text":"Agent class to improve the conversational experience in PandasAI Source code in pandasai\\agent\\__init__.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 class Agent : \"\"\" Agent class to improve the conversational experience in PandasAI \"\"\" _lake : SmartDatalake = None _logger : Optional [ Logger ] = None def __init__ ( self , dfs : Union [ DataFrameType , List [ DataFrameType ]], config : Optional [ Union [ Config , dict ]] = None , logger : Optional [ Logger ] = None , memory_size : int = 1 , ): \"\"\" Args: df (Union[DataFrameType, List[DataFrameType]]): DataFrame can be Pandas, Polars or Database connectors memory_size (int, optional): Conversation history to use during chat. Defaults to 1. \"\"\" if not isinstance ( dfs , list ): dfs = [ dfs ] self . _lake = SmartDatalake ( dfs , config , logger , memory = Memory ( memory_size )) self . _logger = self . _lake . logger def _call_llm_with_prompt ( self , prompt : AbstractPrompt ): \"\"\" Call LLM with prompt using error handling to retry based on config Args: prompt (AbstractPrompt): AbstractPrompt to pass to LLM's \"\"\" retry_count = 0 while retry_count < self . _lake . config . max_retries : try : result : str = self . _lake . llm . call ( prompt ) if prompt . validate ( result ): return result else : raise Exception ( \"Response validation failed!\" ) except Exception : if ( not self . _lake . use_error_correction_framework or retry_count >= self . _lake . config . max_retries - 1 ): raise retry_count += 1 def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Simulate a chat interaction with the assistant on Dataframe. \"\"\" try : result = self . _lake . chat ( query , output_type = output_type ) return result except Exception as exception : return ( \"Unfortunately, I was not able to get your answers, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) def clarification_questions ( self , query : str ) -> List [ str ]: \"\"\" Generate clarification questions based on the data \"\"\" prompt = ClarificationQuestionPrompt ( dataframes = self . _lake . dfs , conversation = self . _lake . _memory . get_conversation (), query = query , ) result = self . _call_llm_with_prompt ( prompt ) self . _logger . log ( f \"\"\"Clarification Questions: { result } \"\"\" ) questions : list [ str ] = json . loads ( result ) return questions [: 3 ] def start_new_conversation ( self ): \"\"\" Clears the previous conversation \"\"\" self . _lake . clear_memory () def explain ( self ) -> str : \"\"\" Returns the explanation of the code how it reached to the solution \"\"\" try : prompt = ExplainPrompt ( conversation = self . _lake . _memory . get_conversation (), code = self . _lake . last_code_executed , ) response = self . _call_llm_with_prompt ( prompt ) self . _logger . log ( f \"\"\"Explaination: { response } \"\"\" ) return response except Exception as exception : return ( \"Unfortunately, I was not able to explain, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) def rephrase_query ( self , query : str ): try : prompt = RephraseQueryPrompt ( query = query , dataframes = self . _lake . dfs , conversation = self . _lake . _memory . get_conversation (), ) response = self . _call_llm_with_prompt ( prompt ) self . _logger . log ( f \"\"\"Rephrased Response: { response } \"\"\" ) return response except Exception as exception : return ( \"Unfortunately, I was not able to repharse query, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" )","title":"Agent"},{"location":"API/pandasai/#pandasai.Agent.__init__","text":"Parameters: df ( Union [ DataFrameType , List [ DataFrameType ]] ) \u2013 DataFrame can be Pandas, memory_size ( int , default: 1 ) \u2013 Conversation history to use during chat. pandasai\\agent\\__init__.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , dfs : Union [ DataFrameType , List [ DataFrameType ]], config : Optional [ Union [ Config , dict ]] = None , logger : Optional [ Logger ] = None , memory_size : int = 1 , ): \"\"\" Args: df (Union[DataFrameType, List[DataFrameType]]): DataFrame can be Pandas, Polars or Database connectors memory_size (int, optional): Conversation history to use during chat. Defaults to 1. \"\"\" if not isinstance ( dfs , list ): dfs = [ dfs ] self . _lake = SmartDatalake ( dfs , config , logger , memory = Memory ( memory_size )) self . _logger = self . _lake . logger","title":"__init__()"},{"location":"API/pandasai/#pandasai.Agent.chat","text":"Simulate a chat interaction with the assistant on Dataframe. pandasai\\agent\\__init__.py 65 66 67 68 69 70 71 72 73 74 75 76 77 def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Simulate a chat interaction with the assistant on Dataframe. \"\"\" try : result = self . _lake . chat ( query , output_type = output_type ) return result except Exception as exception : return ( \"Unfortunately, I was not able to get your answers, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" )","title":"chat()"},{"location":"API/pandasai/#pandasai.Agent.clarification_questions","text":"Generate clarification questions based on the data pandasai\\agent\\__init__.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def clarification_questions ( self , query : str ) -> List [ str ]: \"\"\" Generate clarification questions based on the data \"\"\" prompt = ClarificationQuestionPrompt ( dataframes = self . _lake . dfs , conversation = self . _lake . _memory . get_conversation (), query = query , ) result = self . _call_llm_with_prompt ( prompt ) self . _logger . log ( f \"\"\"Clarification Questions: { result } \"\"\" ) questions : list [ str ] = json . loads ( result ) return questions [: 3 ]","title":"clarification_questions()"},{"location":"API/pandasai/#pandasai.Agent.explain","text":"Returns the explanation of the code how it reached to the solution pandasai\\agent\\__init__.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def explain ( self ) -> str : \"\"\" Returns the explanation of the code how it reached to the solution \"\"\" try : prompt = ExplainPrompt ( conversation = self . _lake . _memory . get_conversation (), code = self . _lake . last_code_executed , ) response = self . _call_llm_with_prompt ( prompt ) self . _logger . log ( f \"\"\"Explaination: { response } \"\"\" ) return response except Exception as exception : return ( \"Unfortunately, I was not able to explain, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" )","title":"explain()"},{"location":"API/pandasai/#pandasai.Agent.start_new_conversation","text":"Clears the previous conversation pandasai\\agent\\__init__.py 97 98 99 100 101 def start_new_conversation ( self ): \"\"\" Clears the previous conversation \"\"\" self . _lake . clear_memory ()","title":"start_new_conversation()"},{"location":"API/pandasai/#pandasai.PandasAI","text":"PandasAI is a wrapper around a LLM to make dataframes conversational. This is an entry point of pandasai object. This class consists of methods to interface the LLMs with Pandas dataframes. A pandas dataframe metadata i.e. df.head() and prompt is passed on to chosen LLMs API end point to generate a Python code to answer the questions asked. The resultant python code is run on actual data and answer is converted into a conversational form. Note Do not include the self parameter in the Args section. Args: _llm (obj): LLMs option to be used for API access _verbose (bool, optional): To show the intermediate outputs e.g. python code generated and execution step on the prompt. Default to False _enforce_privacy (bool, optional): Do not display the data on prompt in case of Sensitive data. Default to False _max_retries (int, optional): max no. of tries to generate code on failure. Default to 3 _original_instructions (dict, optional): The dict of instruction to run. Default to None _cache (Cache, optional): Cache object to store the results. Default to None _enable_cache (bool, optional): Whether to enable cache. Default to True _logger (logging.Logger, optional): Logger object to log the messages. Default to None _logs (List[dict], optional): List of logs to be stored. Default to [] _prompt_id (str, optional): Unique ID to differentiate calls. Default to None _middlewares (List[Middleware], optional): List of middlewares to run. Default to [ChartsMiddleware()] _additional_dependencies (List[dict], optional): List of additional dependencies to be added. Default to [] _custom_whitelisted_dependencies (List[str], optional): List of custom whitelisted dependencies. Default to [] last_code_generated (str, optional): Pass last Code if generated. Default to None last_code_executed (str, optional): Pass the last execution / run. Default to None code_output (str, optional): The code output if any. Default to None last_error (str, optional): Error of running code last time. Default to None prompt_id (str, optional): Unique ID to differentiate calls. Default to None Returns (str): Response to a Question related to Data Source code in pandasai\\__init__.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class PandasAI : \"\"\" PandasAI is a wrapper around a LLM to make dataframes conversational. This is an entry point of `pandasai` object. This class consists of methods to interface the LLMs with Pandas dataframes. A pandas dataframe metadata i.e. df.head() and prompt is passed on to chosen LLMs API end point to generate a Python code to answer the questions asked. The resultant python code is run on actual data and answer is converted into a conversational form. Note: Do not include the `self` parameter in the ``Args`` section. Args: _llm (obj): LLMs option to be used for API access _verbose (bool, optional): To show the intermediate outputs e.g. python code generated and execution step on the prompt. Default to False _enforce_privacy (bool, optional): Do not display the data on prompt in case of Sensitive data. Default to False _max_retries (int, optional): max no. of tries to generate code on failure. Default to 3 _original_instructions (dict, optional): The dict of instruction to run. Default to None _cache (Cache, optional): Cache object to store the results. Default to None _enable_cache (bool, optional): Whether to enable cache. Default to True _logger (logging.Logger, optional): Logger object to log the messages. Default to None _logs (List[dict], optional): List of logs to be stored. Default to [] _prompt_id (str, optional): Unique ID to differentiate calls. Default to None _middlewares (List[Middleware], optional): List of middlewares to run. Default to [ChartsMiddleware()] _additional_dependencies (List[dict], optional): List of additional dependencies to be added. Default to [] _custom_whitelisted_dependencies (List[str], optional): List of custom whitelisted dependencies. Default to [] last_code_generated (str, optional): Pass last Code if generated. Default to None last_code_executed (str, optional): Pass the last execution / run. Default to None code_output (str, optional): The code output if any. Default to None last_error (str, optional): Error of running code last time. Default to None prompt_id (str, optional): Unique ID to differentiate calls. Default to None Returns (str): Response to a Question related to Data \"\"\" _dl : SmartDatalake = None _config : Union [ Config , dict ] def __init__ ( self , llm = None , conversational = False , verbose = False , enforce_privacy = False , save_charts = False , save_charts_path = \"\" , enable_cache = True , middlewares = None , custom_whitelisted_dependencies = None , enable_logging = True , non_default_prompts : Optional [ Dict [ str , Type [ AbstractPrompt ]]] = None , callback : Optional [ BaseCallback ] = None , ): \"\"\" __init__ method of the Class PandasAI Args: llm (object): LLMs option to be used for API access. Default is None conversational (bool): Whether to return answer in conversational form. Default to False verbose (bool): To show the intermediate outputs e.g. python code generated and execution step on the prompt. Default to False enforce_privacy (bool): Execute the codes with Privacy Mode ON. Default to False save_charts (bool): Save the charts generated in the notebook. Default to False enable_cache (bool): Enable the cache to store the results. Default to True middlewares (list): List of middlewares to be used. Default to None custom_whitelisted_dependencies (list): List of custom dependencies to be used. Default to None enable_logging (bool): Enable the logging. Default to True non_default_prompts (dict): Mapping from keys to replacement prompt classes. Used to override specific types of prompts. Defaults to None. \"\"\" # configure the logging # noinspection PyArgumentList # https://stackoverflow.com/questions/61226587/pycharm-does-not-recognize-logging-basicconfig-handlers-argument warnings . warn ( \"`PandasAI` (class) is deprecated since v1.0 and will be removed \" \"in a future release. Please use `SmartDataframe` instead.\" ) self . _config = Config ( conversational = conversational , verbose = verbose , enforce_privacy = enforce_privacy , save_charts = save_charts , save_charts_path = save_charts_path , enable_cache = enable_cache , middlewares = middlewares or [], custom_whitelisted_dependencies = custom_whitelisted_dependencies or [], enable_logging = enable_logging , non_default_prompts = non_default_prompts , llm = llm , callback = callback , ) def run ( self , data_frame : Union [ pd . DataFrame , List [ pd . DataFrame ]], prompt : str , show_code : bool = False , anonymize_df : bool = True , use_error_correction_framework : bool = True , ) -> Union [ str , pd . DataFrame ]: \"\"\" Run the PandasAI to make Dataframes Conversational. Args: data_frame (Union[pd.DataFrame, List[pd.DataFrame]]): A pandas Dataframe prompt (str): A prompt to query about the Dataframe show_code (bool): To show the intermediate python code generated on the prompt. Default to False anonymize_df (bool): Running the code with Sensitive Data. Default to True use_error_correction_framework (bool): Turn on Error Correction mechanism. Default to True Returns (str): Answer to the Input Questions about the DataFrame \"\"\" new_config = self . _config . dict () new_config [ \"show_code\" ] = show_code new_config [ \"anonymize_df\" ] = anonymize_df new_config [ \"use_error_correction_framework\" ] = use_error_correction_framework config = Config ( ** new_config ) . dict () if not isinstance ( data_frame , list ): data_frame = [ data_frame ] self . _dl = SmartDatalake ( data_frame , config ) return self . _dl . chat ( prompt ) def __call__ ( self , data_frame : Union [ pd . DataFrame , List [ pd . DataFrame ]], prompt : str , show_code : bool = False , anonymize_df : bool = True , use_error_correction_framework : bool = True , ) -> Union [ str , pd . DataFrame ]: \"\"\" __call__ method of PandasAI class. It calls the `run` method. Args: data_frame: prompt: show_code: anonymize_df: use_error_correction_framework: Returns (str): Answer to the Input Questions about the DataFrame. \"\"\" return self . run ( data_frame , prompt , show_code , anonymize_df , use_error_correction_framework , ) @property def logs ( self ) -> List [ dict [ str , str ]]: \"\"\"Return the logs\"\"\" if self . _dl is None : return [] return self . _dl . logs @property def last_prompt_id ( self ) -> str : \"\"\"Return the id of the last prompt that was run.\"\"\" if self . _dl is None : return None return self . _dl . last_prompt_id @property def last_prompt ( self ) -> str : \"\"\"Return the last prompt that was executed.\"\"\" if self . _dl is None : return None return self . _dl . last_prompt","title":"PandasAI"},{"location":"API/pandasai/#pandasai.PandasAI.last_prompt","text":"Return the last prompt that was executed.","title":"last_prompt"},{"location":"API/pandasai/#pandasai.PandasAI.last_prompt_id","text":"Return the id of the last prompt that was run.","title":"last_prompt_id"},{"location":"API/pandasai/#pandasai.PandasAI.logs","text":"Return the logs","title":"logs"},{"location":"API/pandasai/#pandasai.PandasAI.__call__","text":"call method of PandasAI class. It calls the run method. Parameters: data_frame ( Union [ DataFrame , List [ DataFrame ]] ) \u2013 prompt ( str ) \u2013 show_code ( bool , default: False ) \u2013 anonymize_df ( bool , default: True ) \u2013 use_error_correction_framework ( bool , default: True ) \u2013 Returns (str): Answer to the Input Questions about the DataFrame. pandasai\\__init__.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def __call__ ( self , data_frame : Union [ pd . DataFrame , List [ pd . DataFrame ]], prompt : str , show_code : bool = False , anonymize_df : bool = True , use_error_correction_framework : bool = True , ) -> Union [ str , pd . DataFrame ]: \"\"\" __call__ method of PandasAI class. It calls the `run` method. Args: data_frame: prompt: show_code: anonymize_df: use_error_correction_framework: Returns (str): Answer to the Input Questions about the DataFrame. \"\"\" return self . run ( data_frame , prompt , show_code , anonymize_df , use_error_correction_framework , )","title":"__call__()"},{"location":"API/pandasai/#pandasai.PandasAI.__init__","text":"init method of the Class PandasAI Parameters: llm ( object , default: None ) \u2013 LLMs option to be used for API access. Default is None conversational ( bool , default: False ) \u2013 Whether to return answer in conversational form. verbose ( bool , default: False ) \u2013 To show the intermediate outputs e.g. python code enforce_privacy ( bool , default: False ) \u2013 Execute the codes with Privacy Mode ON. save_charts ( bool , default: False ) \u2013 Save the charts generated in the notebook. enable_cache ( bool , default: True ) \u2013 Enable the cache to store the results. middlewares ( list , default: None ) \u2013 List of middlewares to be used. Default to None custom_whitelisted_dependencies ( list , default: None ) \u2013 List of custom dependencies to enable_logging ( bool , default: True ) \u2013 Enable the logging. Default to True non_default_prompts ( dict , default: None ) \u2013 Mapping from keys to replacement prompt classes. pandasai\\__init__.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def __init__ ( self , llm = None , conversational = False , verbose = False , enforce_privacy = False , save_charts = False , save_charts_path = \"\" , enable_cache = True , middlewares = None , custom_whitelisted_dependencies = None , enable_logging = True , non_default_prompts : Optional [ Dict [ str , Type [ AbstractPrompt ]]] = None , callback : Optional [ BaseCallback ] = None , ): \"\"\" __init__ method of the Class PandasAI Args: llm (object): LLMs option to be used for API access. Default is None conversational (bool): Whether to return answer in conversational form. Default to False verbose (bool): To show the intermediate outputs e.g. python code generated and execution step on the prompt. Default to False enforce_privacy (bool): Execute the codes with Privacy Mode ON. Default to False save_charts (bool): Save the charts generated in the notebook. Default to False enable_cache (bool): Enable the cache to store the results. Default to True middlewares (list): List of middlewares to be used. Default to None custom_whitelisted_dependencies (list): List of custom dependencies to be used. Default to None enable_logging (bool): Enable the logging. Default to True non_default_prompts (dict): Mapping from keys to replacement prompt classes. Used to override specific types of prompts. Defaults to None. \"\"\" # configure the logging # noinspection PyArgumentList # https://stackoverflow.com/questions/61226587/pycharm-does-not-recognize-logging-basicconfig-handlers-argument warnings . warn ( \"`PandasAI` (class) is deprecated since v1.0 and will be removed \" \"in a future release. Please use `SmartDataframe` instead.\" ) self . _config = Config ( conversational = conversational , verbose = verbose , enforce_privacy = enforce_privacy , save_charts = save_charts , save_charts_path = save_charts_path , enable_cache = enable_cache , middlewares = middlewares or [], custom_whitelisted_dependencies = custom_whitelisted_dependencies or [], enable_logging = enable_logging , non_default_prompts = non_default_prompts , llm = llm , callback = callback , )","title":"__init__()"},{"location":"API/pandasai/#pandasai.PandasAI.run","text":"Run the PandasAI to make Dataframes Conversational. Parameters: data_frame ( Union [ DataFrame , List [ DataFrame ]] ) \u2013 A pandas Dataframe prompt ( str ) \u2013 A prompt to query about the Dataframe show_code ( bool , default: False ) \u2013 To show the intermediate python code generated on the anonymize_df ( bool , default: True ) \u2013 Running the code with Sensitive Data. Default to True use_error_correction_framework ( bool , default: True ) \u2013 Turn on Error Correction mechanism. Returns (str): Answer to the Input Questions about the DataFrame pandasai\\__init__.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def run ( self , data_frame : Union [ pd . DataFrame , List [ pd . DataFrame ]], prompt : str , show_code : bool = False , anonymize_df : bool = True , use_error_correction_framework : bool = True , ) -> Union [ str , pd . DataFrame ]: \"\"\" Run the PandasAI to make Dataframes Conversational. Args: data_frame (Union[pd.DataFrame, List[pd.DataFrame]]): A pandas Dataframe prompt (str): A prompt to query about the Dataframe show_code (bool): To show the intermediate python code generated on the prompt. Default to False anonymize_df (bool): Running the code with Sensitive Data. Default to True use_error_correction_framework (bool): Turn on Error Correction mechanism. Default to True Returns (str): Answer to the Input Questions about the DataFrame \"\"\" new_config = self . _config . dict () new_config [ \"show_code\" ] = show_code new_config [ \"anonymize_df\" ] = anonymize_df new_config [ \"use_error_correction_framework\" ] = use_error_correction_framework config = Config ( ** new_config ) . dict () if not isinstance ( data_frame , list ): data_frame = [ data_frame ] self . _dl = SmartDatalake ( data_frame , config ) return self . _dl . chat ( prompt )","title":"run()"},{"location":"API/pandasai/#pandasai.SmartDataframe","text":"Bases: DataframeAbstract , Shortcuts Source code in pandasai\\smart_dataframe\\__init__.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 class SmartDataframe ( DataframeAbstract , Shortcuts ): _table_name : str _table_description : str _sample_head : str = None _original_import : any _core : SmartDataframeCore _lake : SmartDatalake def __init__ ( self , df : DataFrameType , name : str = None , description : str = None , sample_head : pd . DataFrame = None , config : Config = None , logger : Logger = None , ): \"\"\" Args: df (Union[pd.DataFrame, pl.DataFrame]): Pandas or Polars dataframe name (str, optional): Name of the dataframe. Defaults to None. description (str, optional): Description of the dataframe. Defaults to \"\". sample_head (pd.DataFrame, optional): Sample head of the dataframe. config (Config, optional): Config to be used. Defaults to None. logger (Logger, optional): Logger to be used. Defaults to None. \"\"\" self . _original_import = df if isinstance ( df , str ): if not ( df . endswith ( \".csv\" ) or df . endswith ( \".parquet\" ) or df . endswith ( \".xlsx\" ) or df . startswith ( \"https://docs.google.com/spreadsheets/\" ) ): df_config = self . _load_from_config ( df ) if df_config : if \"://\" in df_config [ \"import_path\" ]: connector_name = df_config [ \"import_path\" ] . split ( \"://\" )[ 0 ] connector_path = df_config [ \"import_path\" ] . split ( \"://\" )[ 1 ] connector_host = connector_path . split ( \":\" )[ 0 ] connector_port = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 0 ] connector_database = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 1 ] connector_table = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 2 ] connector_data = { \"host\" : connector_host , \"database\" : connector_database , \"table\" : connector_table , } if connector_port : connector_data [ \"port\" ] = connector_port # instantiate the connector df = getattr ( __import__ ( \"pandasai.connectors\" , fromlist = [ connector_name ] ), connector_name , )( config = connector_data ) else : df = df_config [ \"import_path\" ] if name is None : name = df_config [ \"name\" ] if description is None : description = df_config [ \"description\" ] else : raise ValueError ( \"Could not find a saved dataframe configuration \" \"with the given name.\" ) self . _core = SmartDataframeCore ( df , logger ) self . _table_name = name self . _table_description = description self . _lake = SmartDatalake ([ self ], config , logger ) # If no name is provided, use the fallback name provided the connector if self . _table_name is None and self . connector : self . _table_name = self . connector . fallback_name if sample_head is not None : self . _sample_head = sample_head . to_csv ( index = False ) def add_middlewares ( self , * middlewares : Optional [ Middleware ]): \"\"\" Add middlewares to PandasAI instance. Args: *middlewares: Middlewares to be added \"\"\" self . lake . add_middlewares ( * middlewares ) def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Run a query on the dataframe. Args: query (str): Query to run on the dataframe output_type (Optional[str]): Add a hint for LLM of which type should be returned by `analyze_data()` in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object Raises: ValueError: If the query is empty \"\"\" return self . lake . chat ( query , output_type ) def column_hash ( self ) -> str : \"\"\" Get the hash of the columns of the dataframe. Returns: str: Hash of the columns of the dataframe \"\"\" if not self . _core . _df_loaded and self . connector : return self . connector . column_hash columns_str = \"\" . join ( self . dataframe . columns ) hash_object = hashlib . sha256 ( columns_str . encode ()) return hash_object . hexdigest () def save ( self , name : str = None ): \"\"\" Saves the dataframe configuration to be used for later Args: name (str, optional): Name of the dataframe configuration. Defaults to None. \"\"\" config_manager = DfConfigManager ( self ) config_manager . save ( name ) def load_connector ( self , temporary : bool = False ): \"\"\" Load a connector into the smart dataframe Args: temporary (bool, optional): Whether the connector is temporary or not. Defaults to False. \"\"\" self . _core . load_connector ( temporary ) def _truncate_head_columns ( self , df : DataFrameType , max_size = 25 ) -> DataFrameType : \"\"\" Truncate the columns of the dataframe to a maximum of 20 characters. Args: df (DataFrameType): Pandas or Polars dataframe Returns: DataFrameType: Pandas or Polars dataframe \"\"\" if df_type ( df ) == \"pandas\" : df_trunc = df . copy () for col in df . columns : if df [ col ] . dtype == \"object\" : first_val = df [ col ] . iloc [ 0 ] if isinstance ( first_val , str ) and len ( first_val ) > max_size : df_trunc [ col ] = df_trunc [ col ] . str . slice ( 0 , max_size - 3 ) + \"...\" elif df_type ( df ) == \"polars\" : try : import polars as pl df_trunc = df . clone () for col in df . columns : if df [ col ] . dtype == pl . Utf8 : first_val = df [ col ][ 0 ] if isinstance ( first_val , str ) and len ( df_trunc [ col ]) > max_size : df_trunc [ col ] = ( df_trunc [ col ] . str . slice ( 0 , max_size - 3 ) + \"...\" ) except ImportError : raise ImportError ( \"Polars is not installed. \" \"Please install Polars to use this feature.\" ) return df_trunc def _get_sample_head ( self ) -> DataFrameType : head = None rows_to_display = 0 if self . lake . config . enforce_privacy else 5 if not self . _core . _df_loaded and self . connector : head = self . connector . head () else : head = self . dataframe . head ( rows_to_display ) if head is None : return None sampler = DataSampler ( head ) sampled_head = sampler . sample ( rows_to_display ) return self . _truncate_head_columns ( sampled_head ) def _load_from_config ( self , name : str ): \"\"\" Loads a saved dataframe configuration \"\"\" config_manager = DfConfigManager ( self ) return config_manager . load ( name ) @property def dataframe ( self ) -> DataFrameType : return self . _core . dataframe @property def engine ( self ): return self . _core . engine @property def connector ( self ): return self . _core . connector @connector . setter def connector ( self , connector : BaseConnector ): connector . logger = self . logger self . _core . connector = connector def validate ( self , schema : pydantic . BaseModel ): \"\"\" Validates Dataframe rows on the basis Pydantic schema input (Args): schema: Pydantic schema class verbose: Print Errors \"\"\" df_validator = DfValidator ( self . dataframe ) return df_validator . validate ( schema ) @property def lake ( self ) -> SmartDatalake : return self . _lake @lake . setter def lake ( self , lake : SmartDatalake ): self . _lake = lake @property def rows_count ( self ): if self . _core . _df_loaded : return self . dataframe . shape [ 0 ] elif self . connector is not None : return self . connector . rows_count else : raise ValueError ( \"Cannot determine rows_count. No dataframe or connector loaded.\" ) @property def columns_count ( self ): if self . _core . _df_loaded : return self . dataframe . shape [ 1 ] elif self . connector is not None : return self . connector . columns_count else : raise ValueError ( \"Cannot determine columns_count. No dataframe or connector loaded.\" ) @cached_property def head_df ( self ): \"\"\" Get the head of the dataframe as a dataframe. Returns: DataFrameType: Pandas or Polars dataframe \"\"\" return self . _get_sample_head () @cached_property def head_csv ( self ): \"\"\" Get the head of the dataframe as a CSV string. Returns: str: CSV string \"\"\" df_head = self . _get_sample_head () return df_head . to_csv ( index = False ) @property def last_prompt ( self ): return self . lake . last_prompt @property def last_prompt_id ( self ) -> str : return self . lake . last_prompt_id @property def last_code_generated ( self ): return self . lake . last_code_executed @property def last_code_executed ( self ): return self . lake . last_code_executed @property def last_result ( self ): return self . lake . last_result @property def last_error ( self ): return self . lake . last_error @property def cache ( self ): return self . lake . cache @property def middlewares ( self ): return self . lake . middlewares def original_import ( self ): return self . _original_import @property def logger ( self ): return self . lake . logger @logger . setter def logger ( self , logger : Logger ): self . lake . logger = logger @property def logs ( self ): return self . lake . logs @property def verbose ( self ): return self . lake . verbose @verbose . setter def verbose ( self , verbose : bool ): self . lake . verbose = verbose @property def save_logs ( self ): return self . lake . save_logs @save_logs . setter def save_logs ( self , save_logs : bool ): self . lake . save_logs = save_logs @property def callback ( self ): return self . lake . callback @callback . setter def callback ( self , callback : BaseCallback ): self . lake . callback = callback @property def enforce_privacy ( self ): return self . lake . enforce_privacy @enforce_privacy . setter def enforce_privacy ( self , enforce_privacy : bool ): self . lake . enforce_privacy = enforce_privacy @property def enable_cache ( self ): return self . lake . enable_cache @enable_cache . setter def enable_cache ( self , enable_cache : bool ): self . lake . enable_cache = enable_cache @property def use_error_correction_framework ( self ): return self . lake . use_error_correction_framework @use_error_correction_framework . setter def use_error_correction_framework ( self , use_error_correction_framework : bool ): self . lake . use_error_correction_framework = use_error_correction_framework @property def custom_prompts ( self ): return self . lake . custom_prompts @custom_prompts . setter def custom_prompts ( self , custom_prompts : dict ): self . lake . custom_prompts = custom_prompts @property def save_charts ( self ): return self . lake . save_charts @save_charts . setter def save_charts ( self , save_charts : bool ): self . lake . save_charts = save_charts @property def save_charts_path ( self ): return self . lake . save_charts_path @save_charts_path . setter def save_charts_path ( self , save_charts_path : str ): self . lake . save_charts_path = save_charts_path @property def custom_whitelisted_dependencies ( self ): return self . lake . custom_whitelisted_dependencies @custom_whitelisted_dependencies . setter def custom_whitelisted_dependencies ( self , custom_whitelisted_dependencies : List [ str ] ): self . lake . custom_whitelisted_dependencies = custom_whitelisted_dependencies @property def max_retries ( self ): return self . lake . max_retries @max_retries . setter def max_retries ( self , max_retries : int ): self . lake . max_retries = max_retries @property def llm ( self ): return self . lake . llm @llm . setter def llm ( self , llm : Union [ LLM , LangchainLLM ]): self . lake . llm = llm @property def table_name ( self ): return self . _table_name @property def table_description ( self ): return self . _table_description @property def sample_head ( self ): data = StringIO ( self . _sample_head ) return pd . read_csv ( data ) @sample_head . setter def sample_head ( self , sample_head : pd . DataFrame ): self . _sample_head = sample_head . to_csv ( index = False ) def __getattr__ ( self , name ): if name in self . _core . __dir__ (): return getattr ( self . _core , name ) elif name in self . dataframe . __dir__ (): return getattr ( self . dataframe , name ) else : return self . __getattribute__ ( name ) def __getitem__ ( self , key ): return self . dataframe . __getitem__ ( key ) def __setitem__ ( self , key , value ): return self . dataframe . __setitem__ ( key , value ) def __dir__ ( self ): return dir ( self . _core ) + dir ( self . dataframe ) + dir ( self . __class__ ) def __repr__ ( self ): return self . dataframe . __repr__ () def __len__ ( self ): return len ( self . dataframe )","title":"SmartDataframe"},{"location":"API/pandasai/#pandasai.SmartDataframe.head_csv","text":"Get the head of the dataframe as a CSV string. Returns: str \u2013 CSV string","title":"head_csv"},{"location":"API/pandasai/#pandasai.SmartDataframe.head_df","text":"Get the head of the dataframe as a dataframe. Returns: DataFrameType \u2013 Pandas or Polars dataframe","title":"head_df"},{"location":"API/pandasai/#pandasai.SmartDataframe.__init__","text":"Parameters: df ( Union [ DataFrame , DataFrame ] ) \u2013 Pandas or Polars dataframe name ( str , default: None ) \u2013 Name of the dataframe. Defaults to None. description ( str , default: None ) \u2013 Description of the dataframe. Defaults to \"\". sample_head ( DataFrame , default: None ) \u2013 Sample head of the dataframe. config ( Config , default: None ) \u2013 Config to be used. Defaults to None. logger ( Logger , default: None ) \u2013 Logger to be used. Defaults to None. pandasai\\smart_dataframe\\__init__.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def __init__ ( self , df : DataFrameType , name : str = None , description : str = None , sample_head : pd . DataFrame = None , config : Config = None , logger : Logger = None , ): \"\"\" Args: df (Union[pd.DataFrame, pl.DataFrame]): Pandas or Polars dataframe name (str, optional): Name of the dataframe. Defaults to None. description (str, optional): Description of the dataframe. Defaults to \"\". sample_head (pd.DataFrame, optional): Sample head of the dataframe. config (Config, optional): Config to be used. Defaults to None. logger (Logger, optional): Logger to be used. Defaults to None. \"\"\" self . _original_import = df if isinstance ( df , str ): if not ( df . endswith ( \".csv\" ) or df . endswith ( \".parquet\" ) or df . endswith ( \".xlsx\" ) or df . startswith ( \"https://docs.google.com/spreadsheets/\" ) ): df_config = self . _load_from_config ( df ) if df_config : if \"://\" in df_config [ \"import_path\" ]: connector_name = df_config [ \"import_path\" ] . split ( \"://\" )[ 0 ] connector_path = df_config [ \"import_path\" ] . split ( \"://\" )[ 1 ] connector_host = connector_path . split ( \":\" )[ 0 ] connector_port = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 0 ] connector_database = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 1 ] connector_table = connector_path . split ( \":\" )[ 1 ] . split ( \"/\" )[ 2 ] connector_data = { \"host\" : connector_host , \"database\" : connector_database , \"table\" : connector_table , } if connector_port : connector_data [ \"port\" ] = connector_port # instantiate the connector df = getattr ( __import__ ( \"pandasai.connectors\" , fromlist = [ connector_name ] ), connector_name , )( config = connector_data ) else : df = df_config [ \"import_path\" ] if name is None : name = df_config [ \"name\" ] if description is None : description = df_config [ \"description\" ] else : raise ValueError ( \"Could not find a saved dataframe configuration \" \"with the given name.\" ) self . _core = SmartDataframeCore ( df , logger ) self . _table_name = name self . _table_description = description self . _lake = SmartDatalake ([ self ], config , logger ) # If no name is provided, use the fallback name provided the connector if self . _table_name is None and self . connector : self . _table_name = self . connector . fallback_name if sample_head is not None : self . _sample_head = sample_head . to_csv ( index = False )","title":"__init__()"},{"location":"API/pandasai/#pandasai.SmartDataframe.add_middlewares","text":"Add middlewares to PandasAI instance. Parameters: *middlewares ( Optional [ Middleware ] , default: () ) \u2013 Middlewares to be added pandasai\\smart_dataframe\\__init__.py 312 313 314 315 316 317 318 319 320 def add_middlewares ( self , * middlewares : Optional [ Middleware ]): \"\"\" Add middlewares to PandasAI instance. Args: *middlewares: Middlewares to be added \"\"\" self . lake . add_middlewares ( * middlewares )","title":"add_middlewares()"},{"location":"API/pandasai/#pandasai.SmartDataframe.chat","text":"Run a query on the dataframe. Parameters: query ( str ) \u2013 Query to run on the dataframe output_type ( Optional [ str ] , default: None ) \u2013 Add a hint for LLM of which type should be returned by analyze_data() in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object Raises: ValueError \u2013 If the query is empty pandasai\\smart_dataframe\\__init__.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Run a query on the dataframe. Args: query (str): Query to run on the dataframe output_type (Optional[str]): Add a hint for LLM of which type should be returned by `analyze_data()` in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object Raises: ValueError: If the query is empty \"\"\" return self . lake . chat ( query , output_type )","title":"chat()"},{"location":"API/pandasai/#pandasai.SmartDataframe.column_hash","text":"Get the hash of the columns of the dataframe. Returns: str ( str ) \u2013 Hash of the columns of the dataframe pandasai\\smart_dataframe\\__init__.py 345 346 347 348 349 350 351 352 353 354 355 356 357 def column_hash ( self ) -> str : \"\"\" Get the hash of the columns of the dataframe. Returns: str: Hash of the columns of the dataframe \"\"\" if not self . _core . _df_loaded and self . connector : return self . connector . column_hash columns_str = \"\" . join ( self . dataframe . columns ) hash_object = hashlib . sha256 ( columns_str . encode ()) return hash_object . hexdigest ()","title":"column_hash()"},{"location":"API/pandasai/#pandasai.SmartDataframe.load_connector","text":"Load a connector into the smart dataframe Parameters: temporary ( bool , default: False ) \u2013 Whether the connector is temporary or not. pandasai\\smart_dataframe\\__init__.py 370 371 372 373 374 375 376 377 378 def load_connector ( self , temporary : bool = False ): \"\"\" Load a connector into the smart dataframe Args: temporary (bool, optional): Whether the connector is temporary or not. Defaults to False. \"\"\" self . _core . load_connector ( temporary )","title":"load_connector()"},{"location":"API/pandasai/#pandasai.SmartDataframe.save","text":"Saves the dataframe configuration to be used for later Parameters: name ( str , default: None ) \u2013 Name of the dataframe configuration. Defaults to None. pandasai\\smart_dataframe\\__init__.py 359 360 361 362 363 364 365 366 367 368 def save ( self , name : str = None ): \"\"\" Saves the dataframe configuration to be used for later Args: name (str, optional): Name of the dataframe configuration. Defaults to None. \"\"\" config_manager = DfConfigManager ( self ) config_manager . save ( name )","title":"save()"},{"location":"API/pandasai/#pandasai.SmartDataframe.validate","text":"Validates Dataframe rows on the basis Pydantic schema input (Args): schema: Pydantic schema class verbose: Print Errors pandasai\\smart_dataframe\\__init__.py 461 462 463 464 465 466 467 468 469 def validate ( self , schema : pydantic . BaseModel ): \"\"\" Validates Dataframe rows on the basis Pydantic schema input (Args): schema: Pydantic schema class verbose: Print Errors \"\"\" df_validator = DfValidator ( self . dataframe ) return df_validator . validate ( schema )","title":"validate()"},{"location":"API/pandasai/#pandasai.SmartDatalake","text":"Source code in pandasai\\smart_datalake\\__init__.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 class SmartDatalake : _dfs : List [ DataFrameType ] _config : Union [ Config , dict ] _llm : LLM _cache : Cache = None _logger : Logger _start_time : float _last_prompt_id : uuid . UUID _code_manager : CodeManager _memory : Memory _last_code_generated : str _last_result : str = None _last_error : str = None def __init__ ( self , dfs : List [ Union [ DataFrameType , Any ]], config : Optional [ Union [ Config , dict ]] = None , logger : Logger = None , memory : Memory = None , cache : Cache = None , ): \"\"\" Args: dfs (List[Union[DataFrameType, Any]]): List of dataframes to be used config (Union[Config, dict], optional): Config to be used. Defaults to None. logger (Logger, optional): Logger to be used. Defaults to None. \"\"\" self . initialize () self . _load_config ( config ) if logger : self . logger = logger else : self . logger = Logger ( save_logs = self . _config . save_logs , verbose = self . _config . verbose ) self . _load_dfs ( dfs ) if memory : self . _memory = memory else : self . _memory = Memory () self . _code_manager = CodeManager ( dfs = self . _dfs , config = self . _config , logger = self . logger , ) if cache : self . _cache = cache elif self . _config . enable_cache : self . _cache = Cache () context = Context ( self . _config , self . logger , self . engine ) if self . _config . response_parser : self . _response_parser = self . _config . response_parser ( context ) else : self . _response_parser = ResponseParser ( context ) def initialize ( self ): \"\"\"Initialize the SmartDatalake\"\"\" # Create exports/charts folder if it doesn't exist try : charts_dir = os . path . join (( find_project_root ()), \"exports\" , \"charts\" ) except ValueError : charts_dir = os . path . join ( os . getcwd (), \"exports\" , \"charts\" ) os . makedirs ( charts_dir , mode = 0o777 , exist_ok = True ) # Create /cache folder if it doesn't exist try : cache_dir = os . path . join (( find_project_root ()), \"cache\" ) except ValueError : cache_dir = os . path . join ( os . getcwd (), \"cache\" ) os . makedirs ( cache_dir , mode = 0o777 , exist_ok = True ) def _load_dfs ( self , dfs : List [ Union [ DataFrameType , Any ]]): \"\"\" Load all the dataframes to be used in the smart datalake. Args: dfs (List[Union[DataFrameType, Any]]): List of dataframes to be used \"\"\" from ..smart_dataframe import SmartDataframe smart_dfs = [] for df in dfs : if not isinstance ( df , SmartDataframe ): smart_dfs . append ( SmartDataframe ( df , config = self . _config , logger = self . logger ) ) else : smart_dfs . append ( df ) self . _dfs = smart_dfs def _load_config ( self , config : Union [ Config , dict ]): \"\"\" Load a config to be used to run the queries. Args: config (Union[Config, dict]): Config to be used \"\"\" config = load_config ( config ) if config . get ( \"llm\" ): self . _load_llm ( config [ \"llm\" ]) config [ \"llm\" ] = self . _llm self . _config = Config ( ** config ) def _load_llm ( self , llm : LLM ): \"\"\" Load a LLM to be used to run the queries. Check if it is a PandasAI LLM or a Langchain LLM. If it is a Langchain LLM, wrap it in a PandasAI LLM. Args: llm (object): LLMs option to be used for API access Raises: BadImportError: If the LLM is a Langchain LLM but the langchain package is not installed \"\"\" if hasattr ( llm , \"_llm_type\" ): llm = LangchainLLM ( llm ) self . _llm = llm def add_middlewares ( self , * middlewares : Optional [ Middleware ]): \"\"\" Add middlewares to PandasAI instance. Args: *middlewares: Middlewares to be added \"\"\" self . _code_manager . add_middlewares ( * middlewares ) def _start_timer ( self ): \"\"\"Start the timer\"\"\" self . _start_time = time . time () def _assign_prompt_id ( self ): \"\"\"Assign a prompt ID\"\"\" self . _last_prompt_id = uuid . uuid4 () if self . logger : self . logger . log ( f \"Prompt ID: { self . _last_prompt_id } \" ) def _get_prompt ( self , key : str , default_prompt : Type [ AbstractPrompt ], default_values : Optional [ dict ] = None , ) -> AbstractPrompt : \"\"\" Return a prompt by key. Args: key (str): The key of the prompt default_prompt (Type[AbstractPrompt]): The default prompt to use default_values (Optional[dict], optional): The default values to use for the prompt. Defaults to None. Returns: AbstractPrompt: The prompt \"\"\" if default_values is None : default_values = {} custom_prompt = self . _config . custom_prompts . get ( key ) prompt = custom_prompt if custom_prompt else default_prompt () # set default values for the prompt if \"dfs\" not in default_values : prompt . set_var ( \"dfs\" , self . _dfs ) if \"conversation\" not in default_values : prompt . set_var ( \"conversation\" , self . _memory . get_conversation ()) for key , value in default_values . items (): prompt . set_var ( key , value ) self . logger . log ( f \"Using prompt: { prompt } \" ) return prompt def _get_cache_key ( self ) -> str : \"\"\" Return the cache key for the current conversation. Returns: str: The cache key for the current conversation \"\"\" cache_key = self . _memory . get_conversation () # make the cache key unique for each combination of dfs for df in self . _dfs : hash = df . column_hash () cache_key += str ( hash ) return cache_key def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Run a query on the dataframe. Args: query (str): Query to run on the dataframe output_type (Optional[str]): Add a hint for LLM which type should be returned by `analyze_data()` in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object If none `output_type` is specified, the type can be any of the above or \"text\". Raises: ValueError: If the query is empty \"\"\" self . _start_timer () self . logger . log ( f \"Question: { query } \" ) self . logger . log ( f \"Running PandasAI with { self . _llm . type } LLM...\" ) self . _assign_prompt_id () self . _memory . add ( query , True ) try : output_type_helper = output_type_factory ( output_type , logger = self . logger ) if ( self . _config . enable_cache and self . _cache and self . _cache . get ( self . _get_cache_key ()) ): self . logger . log ( \"Using cached response\" ) code = self . _cache . get ( self . _get_cache_key ()) else : default_values = { # TODO: find a better way to determine the engine, \"engine\" : self . _dfs [ 0 ] . engine , \"output_type_hint\" : output_type_helper . template_hint , } generate_python_code_instruction = self . _get_prompt ( \"generate_python_code\" , default_prompt = GeneratePythonCodePrompt , default_values = default_values , ) code = self . _llm . generate_code ( generate_python_code_instruction ) if self . _config . enable_cache and self . _cache : self . _cache . set ( self . _get_cache_key (), code ) if self . _config . callback is not None : self . _config . callback . on_code ( code ) self . last_code_generated = code self . logger . log ( f \"\"\"Code generated: ``` { code } ``` \"\"\" ) retry_count = 0 code_to_run = code result = None while retry_count < self . _config . max_retries : try : # Execute the code result = self . _code_manager . execute_code ( code = code_to_run , prompt_id = self . _last_prompt_id , ) break except Exception as e : if ( not self . _config . use_error_correction_framework or retry_count >= self . _config . max_retries - 1 ): raise e retry_count += 1 self . _logger . log ( f \"Failed to execute code with a correction framework \" f \"[retry number: { retry_count } ]\" , level = logging . WARNING , ) traceback_error = traceback . format_exc () code_to_run = self . _retry_run_code ( code , traceback_error ) if result is not None : if isinstance ( result , dict ): validation_ok , validation_logs = output_type_helper . validate ( result ) if not validation_ok : self . logger . log ( \" \\n \" . join ( validation_logs ), level = logging . WARNING ) self . last_result = result self . logger . log ( f \"Answer: { result } \" ) except Exception as exception : self . last_error = str ( exception ) return ( \"Unfortunately, I was not able to answer your question, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) self . logger . log ( f \"Executed in: { time . time () - self . _start_time } s\" ) self . _add_result_to_memory ( result ) return self . _response_parser . parse ( result ) def _add_result_to_memory ( self , result : dict ): \"\"\" Add the result to the memory. Args: result (dict): The result to add to the memory \"\"\" if result is None : return if result [ \"type\" ] == \"string\" or result [ \"type\" ] == \"number\" : self . _memory . add ( result [ \"value\" ], False ) elif result [ \"type\" ] == \"dataframe\" or result [ \"type\" ] == \"plot\" : self . _memory . add ( \"Ok here it is\" , False ) def _retry_run_code ( self , code : str , e : Exception ): \"\"\" A method to retry the code execution with error correction framework. Args: code (str): A python code e (Exception): An exception dataframes Returns (str): A python code \"\"\" self . logger . log ( f \"Failed with error: { e } . Retrying\" , logging . ERROR ) default_values = { \"engine\" : self . _dfs [ 0 ] . engine , \"code\" : code , \"error_returned\" : e , } error_correcting_instruction = self . _get_prompt ( \"correct_error\" , default_prompt = CorrectErrorPrompt , default_values = default_values , ) code = self . _llm . generate_code ( error_correcting_instruction ) if self . _config . callback is not None : self . _config . callback . on_code ( code ) return code def clear_memory ( self ): \"\"\" Clears the memory \"\"\" self . _memory . clear () @property def engine ( self ): return self . _dfs [ 0 ] . engine @property def last_prompt ( self ): return self . _llm . last_prompt @property def last_prompt_id ( self ) -> str : \"\"\"Return the id of the last prompt that was run.\"\"\" if self . _last_prompt_id is None : raise ValueError ( \"Pandas AI has not been run yet.\" ) return self . _last_prompt_id @property def logs ( self ): return self . logger . logs @property def logger ( self ): return self . _logger @logger . setter def logger ( self , logger ): self . _logger = logger @property def config ( self ): return self . _config @property def cache ( self ): return self . _cache @property def middlewares ( self ): return self . _code_manager . middlewares @property def verbose ( self ): return self . _config . verbose @verbose . setter def verbose ( self , verbose : bool ): self . _config . verbose = verbose self . _logger . verbose = verbose @property def save_logs ( self ): return self . _config . save_logs @save_logs . setter def save_logs ( self , save_logs : bool ): self . _config . save_logs = save_logs self . _logger . save_logs = save_logs @property def callback ( self ): return self . _config . callback @callback . setter def callback ( self , callback : Any ): self . config . callback = callback @property def enforce_privacy ( self ): return self . _config . enforce_privacy @enforce_privacy . setter def enforce_privacy ( self , enforce_privacy : bool ): self . _config . enforce_privacy = enforce_privacy @property def enable_cache ( self ): return self . _config . enable_cache @enable_cache . setter def enable_cache ( self , enable_cache : bool ): self . _config . enable_cache = enable_cache if enable_cache : if self . cache is None : self . _cache = Cache () else : self . _cache = None @property def use_error_correction_framework ( self ): return self . _config . use_error_correction_framework @use_error_correction_framework . setter def use_error_correction_framework ( self , use_error_correction_framework : bool ): self . _config . use_error_correction_framework = use_error_correction_framework @property def custom_prompts ( self ): return self . _config . custom_prompts @custom_prompts . setter def custom_prompts ( self , custom_prompts : dict ): self . _config . custom_prompts = custom_prompts @property def save_charts ( self ): return self . _config . save_charts @save_charts . setter def save_charts ( self , save_charts : bool ): self . _config . save_charts = save_charts @property def save_charts_path ( self ): return self . _config . save_charts_path @save_charts_path . setter def save_charts_path ( self , save_charts_path : str ): self . _config . save_charts_path = save_charts_path @property def custom_whitelisted_dependencies ( self ): return self . _config . custom_whitelisted_dependencies @custom_whitelisted_dependencies . setter def custom_whitelisted_dependencies ( self , custom_whitelisted_dependencies : List [ str ] ): self . _config . custom_whitelisted_dependencies = custom_whitelisted_dependencies @property def max_retries ( self ): return self . _config . max_retries @max_retries . setter def max_retries ( self , max_retries : int ): self . _config . max_retries = max_retries @property def llm ( self ): return self . _llm @llm . setter def llm ( self , llm : LLM ): self . _load_llm ( llm ) @property def last_code_generated ( self ): return self . _last_code_generated @last_code_generated . setter def last_code_generated ( self , last_code_generated : str ): self . _last_code_generated = last_code_generated @property def last_code_executed ( self ): return self . _code_manager . last_code_executed @property def last_result ( self ): return self . _last_result @last_result . setter def last_result ( self , last_result : str ): self . _last_result = last_result @property def last_error ( self ): return self . _last_error @last_error . setter def last_error ( self , last_error : str ): self . _last_error = last_error @property def dfs ( self ): return self . _dfs @property def memory ( self ): return self . _memory","title":"SmartDatalake"},{"location":"API/pandasai/#pandasai.SmartDatalake.last_prompt_id","text":"Return the id of the last prompt that was run.","title":"last_prompt_id"},{"location":"API/pandasai/#pandasai.SmartDatalake.__init__","text":"Parameters: dfs ( List [ Union [ DataFrameType , Any ]] ) \u2013 List of dataframes to be used config ( Union [ Config , dict ] , default: None ) \u2013 Config to be used. Defaults to None. logger ( Logger , default: None ) \u2013 Logger to be used. Defaults to None. pandasai\\smart_datalake\\__init__.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def __init__ ( self , dfs : List [ Union [ DataFrameType , Any ]], config : Optional [ Union [ Config , dict ]] = None , logger : Logger = None , memory : Memory = None , cache : Cache = None , ): \"\"\" Args: dfs (List[Union[DataFrameType, Any]]): List of dataframes to be used config (Union[Config, dict], optional): Config to be used. Defaults to None. logger (Logger, optional): Logger to be used. Defaults to None. \"\"\" self . initialize () self . _load_config ( config ) if logger : self . logger = logger else : self . logger = Logger ( save_logs = self . _config . save_logs , verbose = self . _config . verbose ) self . _load_dfs ( dfs ) if memory : self . _memory = memory else : self . _memory = Memory () self . _code_manager = CodeManager ( dfs = self . _dfs , config = self . _config , logger = self . logger , ) if cache : self . _cache = cache elif self . _config . enable_cache : self . _cache = Cache () context = Context ( self . _config , self . logger , self . engine ) if self . _config . response_parser : self . _response_parser = self . _config . response_parser ( context ) else : self . _response_parser = ResponseParser ( context )","title":"__init__()"},{"location":"API/pandasai/#pandasai.SmartDatalake.add_middlewares","text":"Add middlewares to PandasAI instance. Parameters: *middlewares ( Optional [ Middleware ] , default: () ) \u2013 Middlewares to be added pandasai\\smart_datalake\\__init__.py 184 185 186 187 188 189 190 191 def add_middlewares ( self , * middlewares : Optional [ Middleware ]): \"\"\" Add middlewares to PandasAI instance. Args: *middlewares: Middlewares to be added \"\"\" self . _code_manager . add_middlewares ( * middlewares )","title":"add_middlewares()"},{"location":"API/pandasai/#pandasai.SmartDatalake.chat","text":"Run a query on the dataframe. Parameters: query ( str ) \u2013 Query to run on the dataframe output_type ( Optional [ str ] , default: None ) \u2013 Add a hint for LLM which type should be returned by analyze_data() in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object If none output_type is specified, the type can be any of the above or \"text\". Raises: ValueError \u2013 If the query is empty pandasai\\smart_datalake\\__init__.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def chat ( self , query : str , output_type : Optional [ str ] = None ): \"\"\" Run a query on the dataframe. Args: query (str): Query to run on the dataframe output_type (Optional[str]): Add a hint for LLM which type should be returned by `analyze_data()` in generated code. Possible values: \"number\", \"dataframe\", \"plot\", \"string\": * number - specifies that user expects to get a number as a response object * dataframe - specifies that user expects to get pandas/polars dataframe as a response object * plot - specifies that user expects LLM to build a plot * string - specifies that user expects to get text as a response object If none `output_type` is specified, the type can be any of the above or \"text\". Raises: ValueError: If the query is empty \"\"\" self . _start_timer () self . logger . log ( f \"Question: { query } \" ) self . logger . log ( f \"Running PandasAI with { self . _llm . type } LLM...\" ) self . _assign_prompt_id () self . _memory . add ( query , True ) try : output_type_helper = output_type_factory ( output_type , logger = self . logger ) if ( self . _config . enable_cache and self . _cache and self . _cache . get ( self . _get_cache_key ()) ): self . logger . log ( \"Using cached response\" ) code = self . _cache . get ( self . _get_cache_key ()) else : default_values = { # TODO: find a better way to determine the engine, \"engine\" : self . _dfs [ 0 ] . engine , \"output_type_hint\" : output_type_helper . template_hint , } generate_python_code_instruction = self . _get_prompt ( \"generate_python_code\" , default_prompt = GeneratePythonCodePrompt , default_values = default_values , ) code = self . _llm . generate_code ( generate_python_code_instruction ) if self . _config . enable_cache and self . _cache : self . _cache . set ( self . _get_cache_key (), code ) if self . _config . callback is not None : self . _config . callback . on_code ( code ) self . last_code_generated = code self . logger . log ( f \"\"\"Code generated: ``` { code } ``` \"\"\" ) retry_count = 0 code_to_run = code result = None while retry_count < self . _config . max_retries : try : # Execute the code result = self . _code_manager . execute_code ( code = code_to_run , prompt_id = self . _last_prompt_id , ) break except Exception as e : if ( not self . _config . use_error_correction_framework or retry_count >= self . _config . max_retries - 1 ): raise e retry_count += 1 self . _logger . log ( f \"Failed to execute code with a correction framework \" f \"[retry number: { retry_count } ]\" , level = logging . WARNING , ) traceback_error = traceback . format_exc () code_to_run = self . _retry_run_code ( code , traceback_error ) if result is not None : if isinstance ( result , dict ): validation_ok , validation_logs = output_type_helper . validate ( result ) if not validation_ok : self . logger . log ( \" \\n \" . join ( validation_logs ), level = logging . WARNING ) self . last_result = result self . logger . log ( f \"Answer: { result } \" ) except Exception as exception : self . last_error = str ( exception ) return ( \"Unfortunately, I was not able to answer your question, \" \"because of the following error: \\n \" f \" \\n { exception } \\n \" ) self . logger . log ( f \"Executed in: { time . time () - self . _start_time } s\" ) self . _add_result_to_memory ( result ) return self . _response_parser . parse ( result )","title":"chat()"},{"location":"API/pandasai/#pandasai.SmartDatalake.clear_memory","text":"Clears the memory pandasai\\smart_datalake\\__init__.py 429 430 431 432 433 def clear_memory ( self ): \"\"\" Clears the memory \"\"\" self . _memory . clear ()","title":"clear_memory()"},{"location":"API/pandasai/#pandasai.SmartDatalake.initialize","text":"Initialize the SmartDatalake pandasai\\smart_datalake\\__init__.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def initialize ( self ): \"\"\"Initialize the SmartDatalake\"\"\" # Create exports/charts folder if it doesn't exist try : charts_dir = os . path . join (( find_project_root ()), \"exports\" , \"charts\" ) except ValueError : charts_dir = os . path . join ( os . getcwd (), \"exports\" , \"charts\" ) os . makedirs ( charts_dir , mode = 0o777 , exist_ok = True ) # Create /cache folder if it doesn't exist try : cache_dir = os . path . join (( find_project_root ()), \"cache\" ) except ValueError : cache_dir = os . path . join ( os . getcwd (), \"cache\" ) os . makedirs ( cache_dir , mode = 0o777 , exist_ok = True )","title":"initialize()"},{"location":"API/pandasai/#pandasai.clear_cache","text":"Clear the cache pandasai\\__init__.py 254 255 256 257 def clear_cache ( filename : str = None ): \"\"\"Clear the cache\"\"\" cache = Cache ( filename or \"cache_db\" ) cache . clear ()","title":"clear_cache()"},{"location":"API/pandasai/#constants","text":"Some of the package level constants are defined here.","title":"Constants"},{"location":"API/pandasai/#pandasai.constants","text":"Constants used in the pandasai package. It includes Start & End Code tags, Whitelisted Python Packages and While List Builtin Methods.","title":"constants"},{"location":"API/pandasai/#exception-handling","text":"The pandasai specific Exception handling mechanism defined here.","title":"Exception Handling"},{"location":"API/pandasai/#pandasai.exceptions","text":"PandasAI's custom exceptions. This module contains the implementation of Custom Exceptions.","title":"exceptions"},{"location":"API/pandasai/#pandasai.exceptions.APIKeyNotFoundError","text":"Bases: Exception Raised when the API key is not defined/declared. Parameters: Exception ( Exception ) \u2013 APIKeyNotFoundError Source code in pandasai\\exceptions.py 8 9 10 11 12 13 14 15 class APIKeyNotFoundError ( Exception ): \"\"\" Raised when the API key is not defined/declared. Args: Exception (Exception): APIKeyNotFoundError \"\"\"","title":"APIKeyNotFoundError"},{"location":"API/pandasai/#pandasai.exceptions.BadImportError","text":"Bases: Exception Raised when a library not in the whitelist is imported. Parameters: Exception ( Exception ) \u2013 BadImportError Source code in pandasai\\exceptions.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class BadImportError ( Exception ): \"\"\" Raised when a library not in the whitelist is imported. Args: Exception (Exception): BadImportError \"\"\" def __init__ ( self , library_name ): \"\"\" __init__ method of BadImportError Class Args: library_name (str): Name of the library that is not in the whitelist. \"\"\" self . library_name = library_name super () . __init__ ( f \"Generated code includes import of { library_name } which\" \" is not in whitelist.\" )","title":"BadImportError"},{"location":"API/pandasai/#pandasai.exceptions.BadImportError.__init__","text":"init method of BadImportError Class Parameters: library_name ( str ) \u2013 Name of the library that is not in the whitelist. pandasai\\exceptions.py 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , library_name ): \"\"\" __init__ method of BadImportError Class Args: library_name (str): Name of the library that is not in the whitelist. \"\"\" self . library_name = library_name super () . __init__ ( f \"Generated code includes import of { library_name } which\" \" is not in whitelist.\" )","title":"__init__()"},{"location":"API/pandasai/#pandasai.exceptions.LLMNotFoundError","text":"Bases: Exception Raised when the LLM is not provided. Parameters: Exception ( Exception ) \u2013 LLMNotFoundError Source code in pandasai\\exceptions.py 18 19 20 21 22 23 24 class LLMNotFoundError ( Exception ): \"\"\" Raised when the LLM is not provided. Args: Exception (Exception): LLMNotFoundError \"\"\"","title":"LLMNotFoundError"},{"location":"API/pandasai/#pandasai.exceptions.MethodNotImplementedError","text":"Bases: Exception Raised when a method is not implemented. Parameters: Exception ( Exception ) \u2013 MethodNotImplementedError Source code in pandasai\\exceptions.py 36 37 38 39 40 41 42 class MethodNotImplementedError ( Exception ): \"\"\" Raised when a method is not implemented. Args: Exception (Exception): MethodNotImplementedError \"\"\"","title":"MethodNotImplementedError"},{"location":"API/pandasai/#pandasai.exceptions.NoCodeFoundError","text":"Bases: Exception Raised when no code is found in the response. Parameters: Exception ( Exception ) \u2013 NoCodeFoundError Source code in pandasai\\exceptions.py 27 28 29 30 31 32 33 class NoCodeFoundError ( Exception ): \"\"\" Raised when no code is found in the response. Args: Exception (Exception): NoCodeFoundError \"\"\"","title":"NoCodeFoundError"},{"location":"API/pandasai/#pandasai.exceptions.TemplateFileNotFoundError","text":"Bases: FileNotFoundError Raised when a template file cannot be found. Source code in pandasai\\exceptions.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class TemplateFileNotFoundError ( FileNotFoundError ): \"\"\" Raised when a template file cannot be found. \"\"\" def __init__ ( self , template_path , prompt_name = \"Unknown\" ): \"\"\" __init__ method of TemplateFileNotFoundError Class Args: template_path (str): Path for template file. prompt_name (str): Prompt name. Defaults to \"Unknown\". \"\"\" self . template_path = template_path super () . __init__ ( f \"Unable to find a file with template at ' { template_path } ' \" f \"for ' { prompt_name } ' prompt.\" )","title":"TemplateFileNotFoundError"},{"location":"API/pandasai/#pandasai.exceptions.TemplateFileNotFoundError.__init__","text":"init method of TemplateFileNotFoundError Class Parameters: template_path ( str ) \u2013 Path for template file. prompt_name ( str , default: 'Unknown' ) \u2013 Prompt name. Defaults to \"Unknown\". pandasai\\exceptions.py 81 82 83 84 85 86 87 88 89 90 91 92 93 def __init__ ( self , template_path , prompt_name = \"Unknown\" ): \"\"\" __init__ method of TemplateFileNotFoundError Class Args: template_path (str): Path for template file. prompt_name (str): Prompt name. Defaults to \"Unknown\". \"\"\" self . template_path = template_path super () . __init__ ( f \"Unable to find a file with template at ' { template_path } ' \" f \"for ' { prompt_name } ' prompt.\" )","title":"__init__()"},{"location":"API/pandasai/#pandasai.exceptions.UnsupportedOpenAIModelError","text":"Bases: Exception Raised when an unsupported OpenAI model is used. Parameters: Exception ( Exception ) \u2013 UnsupportedOpenAIModelError Source code in pandasai\\exceptions.py 45 46 47 48 49 50 51 class UnsupportedOpenAIModelError ( Exception ): \"\"\" Raised when an unsupported OpenAI model is used. Args: Exception (Exception): UnsupportedOpenAIModelError \"\"\"","title":"UnsupportedOpenAIModelError"},{"location":"API/prompts/","text":"Prompts This module includes some methods on optimally handling prompts when interacting with LLMs. Base Prompt A base prompt Base class to implement a new Prompt In order to better handle the instructions, this prompt module is written. AbstractPrompt Bases: ABC Base class to implement a new Prompt. Inheritors have to override template property. Source code in pandasai\\prompts\\base.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class AbstractPrompt ( ABC ): \"\"\"Base class to implement a new Prompt. Inheritors have to override `template` property. \"\"\" _args : dict = None def __init__ ( self , ** kwargs ): \"\"\" __init__ method of Base class of Prompt Module Args: **kwargs: Inferred Keyword Arguments \"\"\" if self . _args is None : self . _args = {} self . _args . update ( kwargs ) self . setup ( ** kwargs ) def setup ( self , ** kwargs ) -> None : pass def _generate_dataframes ( self , dfs ): \"\"\" Generate the dataframes metadata Args: dfs: List of Dataframes \"\"\" dataframes = [] for index , df in enumerate ( dfs , start = 1 ): description = \"\"\"<dataframe> Dataframe \"\"\" if df . table_name is not None : description += f \" { df . table_name } (dfs[ { index - 1 } ])\" else : description += f \"dfs[ { index - 1 } ]\" description += ( f \", with { df . rows_count } rows and { df . columns_count } columns.\" ) if df . table_description is not None : description += f \" \\n Description: { df . table_description } \" description += f \"\"\" This is the metadata of the dataframe dfs[ { index - 1 } ]: { df . head_csv } </dataframe>\"\"\" # noqa: E501 dataframes . append ( description ) return \" \\n\\n \" . join ( dataframes ) @property @abstractmethod def template ( self ) -> str : ... def set_var ( self , var , value ): if self . _args is None : self . _args = {} if var == \"dfs\" : self . _args [ \"dataframes\" ] = self . _generate_dataframes ( value ) self . _args [ var ] = value def to_string ( self ): return self . template . format ( ** self . _args ) def __str__ ( self ): return self . to_string () def validate ( self , output : str ) -> bool : return isinstance ( output , str ) __init__ ( ** kwargs ) init method of Base class of Prompt Module Args: **kwargs: Inferred Keyword Arguments pandasai\\prompts\\base.py 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , ** kwargs ): \"\"\" __init__ method of Base class of Prompt Module Args: **kwargs: Inferred Keyword Arguments \"\"\" if self . _args is None : self . _args = {} self . _args . update ( kwargs ) self . setup ( ** kwargs ) FileBasedPrompt Bases: AbstractPrompt Base class for prompts supposed to read template content from a file. _path_to_template attribute has to be specified. Source code in pandasai\\prompts\\base.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class FileBasedPrompt ( AbstractPrompt ): \"\"\"Base class for prompts supposed to read template content from a file. `_path_to_template` attribute has to be specified. \"\"\" _path_to_template : str def __init__ ( self , ** kwargs ): if ( template_path := kwargs . pop ( \"path_to_template\" , None )) is not None : self . _path_to_template = template_path else : current_dir_path = Path ( __file__ ) . parent self . _path_to_template = os . path . join ( current_dir_path , \"..\" , self . _path_to_template ) super () . __init__ ( ** kwargs ) @property def template ( self ) -> str : try : with open ( self . _path_to_template ) as fp : return fp . read () except FileNotFoundError : raise TemplateFileNotFoundError ( self . _path_to_template , self . __class__ . __name__ ) except IOError as exc : raise RuntimeError ( f \"Failed to read template file ' { self . _path_to_template } ': { exc } \" ) options: show_root_heading: true Generate Python Code A standard prompt is designed to be used when querying the LLMs to generate Python Code. Prompt to generate Python code You are provided with the following pandas DataFrames: {dataframes} <conversation> {conversation} </conversation> This is the initial python code to be updated: ```python # TODO import all the dependencies required {default_import} def analyze_data(dfs: list[{engine_df_name}]) -> dict: \"\"\" Analyze the data 1. Prepare: Preprocessing and cleaning data if necessary 2. Process: Manipulating data for analysis (grouping, filtering, aggregating, etc.) 3. Analyze: Conducting the actual analysis (if the user asks to plot a chart save it to an image in temp_chart.png and do not show the chart.) At the end, return a dictionary of: - type (possible values \"text\", \"number\", \"dataframe\", \"plot\") - value (can be a string, a dataframe or the path of the plot, NOT a dictionary) Example output: {{ \"type\": \"string\", \"value\": f\"The average loan amount is {{average_amount}}\" }} \"\"\" Using the provided dataframes ( dfs ), update the python code based on the last question in the conversation. {conversation} Updated code: GeneratePythonCodePrompt Bases: FileBasedPrompt Prompt to generate Python code Source code in pandasai\\prompts\\generate_python_code.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class GeneratePythonCodePrompt ( FileBasedPrompt ): \"\"\"Prompt to generate Python code\"\"\" _path_to_template = \"assets/prompt_templates/generate_python_code.tmpl\" def setup ( self , ** kwargs ) -> None : default_import = \"import pandas as pd\" engine_df_name = \"pd.DataFrame\" self . set_var ( \"default_import\" , default_import ) self . set_var ( \"engine_df_name\" , engine_df_name ) if \"custom_instructions\" in kwargs : self . _set_instructions ( kwargs [ \"custom_instructions\" ]) else : self . _set_instructions ( \"\"\"Analyze the data 1. Prepare: Preprocessing and cleaning data if necessary 2. Process: Manipulating data for analysis (grouping, filtering, aggregating, etc.) 3. Analyze: Conducting the actual analysis (if the user asks to plot a chart save it to an image in temp_chart.png and do not show the chart.)\"\"\" # noqa: E501 ) def _set_instructions ( self , instructions : str ): lines = instructions . split ( \" \\n \" ) indented_lines = [ \" \" + line for line in lines [ 1 :]] result = \" \\n \" . join ([ lines [ 0 ]] + indented_lines ) self . set_var ( \"instructions\" , result ) options: show_root_heading: true Generate Python Code On Error A prompt to generate Python Code on Error Prompt to correct Python Code on Error ``` You are provided with the following pandas DataFrames with the following metadata: {dataframes} The user asked the following question: {conversation} You generated this python code: {code} It fails with the following error: {error_returned} Correct the python code and return a new python code that fixes the above mentioned error. Do not generate the same code again. CorrectErrorPrompt Bases: FileBasedPrompt Prompt to Correct Python code on Error Source code in pandasai\\prompts\\correct_error_prompt.py 22 23 24 25 class CorrectErrorPrompt ( FileBasedPrompt ): \"\"\"Prompt to Correct Python code on Error\"\"\" _path_to_template = \"assets/prompt_templates/correct_error_prompt.tmpl\" options: show_root_heading: true","title":"Prompts"},{"location":"API/prompts/#prompts","text":"This module includes some methods on optimally handling prompts when interacting with LLMs.","title":"Prompts"},{"location":"API/prompts/#base-prompt","text":"A base prompt Base class to implement a new Prompt In order to better handle the instructions, this prompt module is written.","title":"Base Prompt"},{"location":"API/prompts/#pandasai.prompts.base.AbstractPrompt","text":"Bases: ABC Base class to implement a new Prompt. Inheritors have to override template property. Source code in pandasai\\prompts\\base.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class AbstractPrompt ( ABC ): \"\"\"Base class to implement a new Prompt. Inheritors have to override `template` property. \"\"\" _args : dict = None def __init__ ( self , ** kwargs ): \"\"\" __init__ method of Base class of Prompt Module Args: **kwargs: Inferred Keyword Arguments \"\"\" if self . _args is None : self . _args = {} self . _args . update ( kwargs ) self . setup ( ** kwargs ) def setup ( self , ** kwargs ) -> None : pass def _generate_dataframes ( self , dfs ): \"\"\" Generate the dataframes metadata Args: dfs: List of Dataframes \"\"\" dataframes = [] for index , df in enumerate ( dfs , start = 1 ): description = \"\"\"<dataframe> Dataframe \"\"\" if df . table_name is not None : description += f \" { df . table_name } (dfs[ { index - 1 } ])\" else : description += f \"dfs[ { index - 1 } ]\" description += ( f \", with { df . rows_count } rows and { df . columns_count } columns.\" ) if df . table_description is not None : description += f \" \\n Description: { df . table_description } \" description += f \"\"\" This is the metadata of the dataframe dfs[ { index - 1 } ]: { df . head_csv } </dataframe>\"\"\" # noqa: E501 dataframes . append ( description ) return \" \\n\\n \" . join ( dataframes ) @property @abstractmethod def template ( self ) -> str : ... def set_var ( self , var , value ): if self . _args is None : self . _args = {} if var == \"dfs\" : self . _args [ \"dataframes\" ] = self . _generate_dataframes ( value ) self . _args [ var ] = value def to_string ( self ): return self . template . format ( ** self . _args ) def __str__ ( self ): return self . to_string () def validate ( self , output : str ) -> bool : return isinstance ( output , str )","title":"AbstractPrompt"},{"location":"API/prompts/#pandasai.prompts.base.AbstractPrompt.__init__","text":"init method of Base class of Prompt Module Args: **kwargs: Inferred Keyword Arguments pandasai\\prompts\\base.py 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , ** kwargs ): \"\"\" __init__ method of Base class of Prompt Module Args: **kwargs: Inferred Keyword Arguments \"\"\" if self . _args is None : self . _args = {} self . _args . update ( kwargs ) self . setup ( ** kwargs )","title":"__init__()"},{"location":"API/prompts/#pandasai.prompts.base.FileBasedPrompt","text":"Bases: AbstractPrompt Base class for prompts supposed to read template content from a file. _path_to_template attribute has to be specified. Source code in pandasai\\prompts\\base.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class FileBasedPrompt ( AbstractPrompt ): \"\"\"Base class for prompts supposed to read template content from a file. `_path_to_template` attribute has to be specified. \"\"\" _path_to_template : str def __init__ ( self , ** kwargs ): if ( template_path := kwargs . pop ( \"path_to_template\" , None )) is not None : self . _path_to_template = template_path else : current_dir_path = Path ( __file__ ) . parent self . _path_to_template = os . path . join ( current_dir_path , \"..\" , self . _path_to_template ) super () . __init__ ( ** kwargs ) @property def template ( self ) -> str : try : with open ( self . _path_to_template ) as fp : return fp . read () except FileNotFoundError : raise TemplateFileNotFoundError ( self . _path_to_template , self . __class__ . __name__ ) except IOError as exc : raise RuntimeError ( f \"Failed to read template file ' { self . _path_to_template } ': { exc } \" ) options: show_root_heading: true","title":"FileBasedPrompt"},{"location":"API/prompts/#generate-python-code","text":"A standard prompt is designed to be used when querying the LLMs to generate Python Code. Prompt to generate Python code You are provided with the following pandas DataFrames: {dataframes} <conversation> {conversation} </conversation> This is the initial python code to be updated: ```python # TODO import all the dependencies required {default_import} def analyze_data(dfs: list[{engine_df_name}]) -> dict: \"\"\" Analyze the data 1. Prepare: Preprocessing and cleaning data if necessary 2. Process: Manipulating data for analysis (grouping, filtering, aggregating, etc.) 3. Analyze: Conducting the actual analysis (if the user asks to plot a chart save it to an image in temp_chart.png and do not show the chart.) At the end, return a dictionary of: - type (possible values \"text\", \"number\", \"dataframe\", \"plot\") - value (can be a string, a dataframe or the path of the plot, NOT a dictionary) Example output: {{ \"type\": \"string\", \"value\": f\"The average loan amount is {{average_amount}}\" }} \"\"\" Using the provided dataframes ( dfs ), update the python code based on the last question in the conversation. {conversation} Updated code:","title":"Generate Python Code"},{"location":"API/prompts/#pandasai.prompts.generate_python_code.GeneratePythonCodePrompt","text":"Bases: FileBasedPrompt Prompt to generate Python code Source code in pandasai\\prompts\\generate_python_code.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class GeneratePythonCodePrompt ( FileBasedPrompt ): \"\"\"Prompt to generate Python code\"\"\" _path_to_template = \"assets/prompt_templates/generate_python_code.tmpl\" def setup ( self , ** kwargs ) -> None : default_import = \"import pandas as pd\" engine_df_name = \"pd.DataFrame\" self . set_var ( \"default_import\" , default_import ) self . set_var ( \"engine_df_name\" , engine_df_name ) if \"custom_instructions\" in kwargs : self . _set_instructions ( kwargs [ \"custom_instructions\" ]) else : self . _set_instructions ( \"\"\"Analyze the data 1. Prepare: Preprocessing and cleaning data if necessary 2. Process: Manipulating data for analysis (grouping, filtering, aggregating, etc.) 3. Analyze: Conducting the actual analysis (if the user asks to plot a chart save it to an image in temp_chart.png and do not show the chart.)\"\"\" # noqa: E501 ) def _set_instructions ( self , instructions : str ): lines = instructions . split ( \" \\n \" ) indented_lines = [ \" \" + line for line in lines [ 1 :]] result = \" \\n \" . join ([ lines [ 0 ]] + indented_lines ) self . set_var ( \"instructions\" , result ) options: show_root_heading: true","title":"GeneratePythonCodePrompt"},{"location":"API/prompts/#generate-python-code-on-error","text":"A prompt to generate Python Code on Error Prompt to correct Python Code on Error ``` You are provided with the following pandas DataFrames with the following metadata: {dataframes} The user asked the following question: {conversation} You generated this python code: {code} It fails with the following error: {error_returned} Correct the python code and return a new python code that fixes the above mentioned error. Do not generate the same code again.","title":"Generate Python Code On Error"},{"location":"API/prompts/#pandasai.prompts.correct_error_prompt.CorrectErrorPrompt","text":"Bases: FileBasedPrompt Prompt to Correct Python code on Error Source code in pandasai\\prompts\\correct_error_prompt.py 22 23 24 25 class CorrectErrorPrompt ( FileBasedPrompt ): \"\"\"Prompt to Correct Python code on Error\"\"\" _path_to_template = \"assets/prompt_templates/correct_error_prompt.tmpl\" options: show_root_heading: true","title":"CorrectErrorPrompt"},{"location":"LLMs/langchain/","text":"LangChain models PandasAI has also built-in support for LangChain models. In order to use LangChain models, you need to install the langchain package: pip install pandasai[langchain] Once you have installed the langchain package, you can use it to instantiate a LangChain object: from pandasai import SmartDataframe from langchain.llms import OpenAI langchain_llm = OpenAI(openai_api_key=\"my-openai-api-key\") df = SmartDataframe(\"data.csv\", {\"llm\": langchain_llm}) PandasAI will automatically detect that you are using a LangChain llm and will convert it to a PandasAI llm. More information For more information about LangChain models, please refer to the LangChain documentation .","title":"LangChain models"},{"location":"LLMs/langchain/#langchain-models","text":"PandasAI has also built-in support for LangChain models. In order to use LangChain models, you need to install the langchain package: pip install pandasai[langchain] Once you have installed the langchain package, you can use it to instantiate a LangChain object: from pandasai import SmartDataframe from langchain.llms import OpenAI langchain_llm = OpenAI(openai_api_key=\"my-openai-api-key\") df = SmartDataframe(\"data.csv\", {\"llm\": langchain_llm}) PandasAI will automatically detect that you are using a LangChain llm and will convert it to a PandasAI llm.","title":"LangChain models"},{"location":"LLMs/langchain/#more-information","text":"For more information about LangChain models, please refer to the LangChain documentation .","title":"More information"},{"location":"LLMs/llms/","text":"Large language models (LLMs) PandasAI supports several large language models (LLMs). LLMs are used to generate code from natural language queries. The generated code is then executed to produce the result. You can either choose a LLM by instantiating one and passing it to the SmartDataFrame or SmartDatalake constructor, or you can specify one in the pandasai.json file. If the model expects one or more parameters, you can pass them to the constructor or specify them in the pandasai.json file, in the llm_options param, as it follows: { \"llm\": \"OpenAI\", \"llm_options\": { \"api_token\": \"API_TOKEN_GOES_HERE\" } } In order to use OpenAI models, you need to have an OpenAI API key. You can get one here . Once you have an API key, you can use it to instantiate an OpenAI object: from pandasai import SmartDataframe from pandasai.llm import OpenAI llm = OpenAI(api_token=\"my-openai-api-key\") pandas_ai = SmartDataframe(\"data.csv\", config={\"llm\": llm}) As an alternative, you can set the OPENAI_API_KEY environment variable and instantiate the OpenAI object without passing the API key: from pandasai import SmartDataframe from pandasai.llm import OpenAI llm = OpenAI() # no need to pass the API key, it will be read from the environment variable pandas_ai = SmartDataframe(\"data.csv\", config={\"llm\": llm}) If you are behind an explicit proxy, you can specify openai_proxy when instantiating the OpenAI object or set the OPENAI_PROXY environment variable to pass through. Count tokens You can count the number of tokens used by a prompt as follows: \"\"\"Example of using PandasAI with a pandas dataframe\"\"\" from pandasai import SmartDataframe from pandasai.llm import OpenAI from pandasai.helpers.openai_info import get_openai_callback import pandas as pd llm = OpenAI() # conversational=False is supposed to display lower usage and cost df = SmartDataframe(\"data.csv\", {\"llm\": llm, \"conversational\": False}) with get_openai_callback() as cb: response = df.chat(\"Calculate the sum of the gdp of north american countries\") print(response) print(cb) # The sum of the GDP of North American countries is 19,294,482,071,552. # Tokens Used: 375 # Prompt Tokens: 210 # Completion Tokens: 165 # Total Cost (USD): $ 0.000750 HuggingFace models In order to use HuggingFace models, you need to have a HuggingFace API key. You can create a HuggingFace account here and get an API key here . Once you have an API key, you can use it to instantiate one of the HuggingFace models. At the moment, PandasAI supports the following HuggingFace models: Starcoder: bigcode/starcoder Falcon: tiiuae/falcon-7b-instruct from pandasai import SmartDataframe from pandasai.llm import Starcoder, Falcon llm = Starcoder(api_token=\"my-huggingface-api-key\") # or llm = Falcon(api_token=\"my-huggingface-api-key\") df = SmartDataframe(\"data.csv\", config={\"llm\": llm}) As an alternative, you can set the HUGGINGFACE_API_KEY environment variable and instantiate the HuggingFace object without passing the API key: from pandasai import SmartDataframe from pandasai.llm import Starcoder, Falcon llm = Starcoder() # no need to pass the API key, it will be read from the environment variable # or llm = Falcon() # no need to pass the API key, it will be read from the environment variable df = SmartDataframe(\"data.csv\", config={\"llm\": llm}) Google PaLM In order to use Google PaLM models, you need to have a Google Cloud API key. You can get one here . Once you have an API key, you can use it to instantiate a Google PaLM object: from pandasai import SmartDataframe from pandasai.llm import GooglePalm llm = GooglePalm(api_key=\"my-google-cloud-api-key\") df = SmartDataframe(\"data.csv\", config={\"llm\": llm}) Google Vertexai In order to use Google PaLM models through Vertexai api, you need to have Google Cloud Project Region of Project Set up Install optional dependency google-cloud-aiplatform Authentication of gcloud Once you have basic setup, you can use it to instantiate a Google PaLM through vertex ai: from pandasai import SmartDataframe from pandasai.llm import GoogleVertexAI llm = GoogleVertexAI(project_id=\"generative-ai-training\", location=\"us-central1\", model=\"text-bison@001\") df = SmartDataframe(\"data.csv\", config={\"llm\": llm}) Azure OpenAI In order to use Azure OpenAI models, you need to have an Azure OpenAI API key as well as an Azure OpenAI endpoint. You can get one here . To instantiate an Azure OpenAI object you also need to specify the name of your deployed model on Azure and the API version: from pandasai import SmartDataframe from pandasai.llm import AzureOpenAI llm = AzureOpenAI( api_token=\"my-azure-openai-api-key\", api_base=\"my-azure-openai-api-endpoint\", api_version=\"2023-05-15\", deployment_name=\"my-deployment-name\" ) df = SmartDataframe(\"data.csv\", config={\"llm\": llm}) As an alternative, you can set the OPENAI_API_KEY , OPENAI_API_VERSION , and OPENAI_API_BASE environment variables and instantiate the Azure OpenAI object without passing them: from pandasai import SmartDataframe from pandasai.llm import AzureOpenAI llm = AzureOpenAI( deployment_name=\"my-deployment-name\" ) # no need to pass the API key, endpoint and API version. They are read from the environment variable df = SmartDataframe(\"data.csv\", config={\"llm\": llm}) If you are behind an explicit proxy, you can specify openai_proxy when instantiating the AzureOpenAI object or set the OPENAI_PROXY environment variable to pass through. HuggingFace via Text Generation In order to use HuggingFace models via text-generation, you need to first serve a supported large language model (LLM). Read text-generation docs for more on how to setup an inference server. This can be used, for example, to use models like LLaMa2, CodeLLaMa, etc. You can find more information about text-generation here . The inference_server_url is the only required parameter to instantiate an HuggingFaceTextGen model: from pandasai.llm import HuggingFaceTextGen from pandasai import SmartDataframe llm = HuggingFaceTextGen( inference_server_url=\"http://127.0.0.1:8080\" ) df = SmartDataframe(\"data.csv\", config={\"llm\": llm})","title":"Large language models (LLMs)"},{"location":"LLMs/llms/#large-language-models-llms","text":"PandasAI supports several large language models (LLMs). LLMs are used to generate code from natural language queries. The generated code is then executed to produce the result. You can either choose a LLM by instantiating one and passing it to the SmartDataFrame or SmartDatalake constructor, or you can specify one in the pandasai.json file. If the model expects one or more parameters, you can pass them to the constructor or specify them in the pandasai.json file, in the llm_options param, as it follows: { \"llm\": \"OpenAI\", \"llm_options\": { \"api_token\": \"API_TOKEN_GOES_HERE\" } } In order to use OpenAI models, you need to have an OpenAI API key. You can get one here . Once you have an API key, you can use it to instantiate an OpenAI object: from pandasai import SmartDataframe from pandasai.llm import OpenAI llm = OpenAI(api_token=\"my-openai-api-key\") pandas_ai = SmartDataframe(\"data.csv\", config={\"llm\": llm}) As an alternative, you can set the OPENAI_API_KEY environment variable and instantiate the OpenAI object without passing the API key: from pandasai import SmartDataframe from pandasai.llm import OpenAI llm = OpenAI() # no need to pass the API key, it will be read from the environment variable pandas_ai = SmartDataframe(\"data.csv\", config={\"llm\": llm}) If you are behind an explicit proxy, you can specify openai_proxy when instantiating the OpenAI object or set the OPENAI_PROXY environment variable to pass through.","title":"Large language models (LLMs)"},{"location":"LLMs/llms/#count-tokens","text":"You can count the number of tokens used by a prompt as follows: \"\"\"Example of using PandasAI with a pandas dataframe\"\"\" from pandasai import SmartDataframe from pandasai.llm import OpenAI from pandasai.helpers.openai_info import get_openai_callback import pandas as pd llm = OpenAI() # conversational=False is supposed to display lower usage and cost df = SmartDataframe(\"data.csv\", {\"llm\": llm, \"conversational\": False}) with get_openai_callback() as cb: response = df.chat(\"Calculate the sum of the gdp of north american countries\") print(response) print(cb) # The sum of the GDP of North American countries is 19,294,482,071,552. # Tokens Used: 375 # Prompt Tokens: 210 # Completion Tokens: 165 # Total Cost (USD): $ 0.000750","title":"Count tokens"},{"location":"LLMs/llms/#huggingface-models","text":"In order to use HuggingFace models, you need to have a HuggingFace API key. You can create a HuggingFace account here and get an API key here . Once you have an API key, you can use it to instantiate one of the HuggingFace models. At the moment, PandasAI supports the following HuggingFace models: Starcoder: bigcode/starcoder Falcon: tiiuae/falcon-7b-instruct from pandasai import SmartDataframe from pandasai.llm import Starcoder, Falcon llm = Starcoder(api_token=\"my-huggingface-api-key\") # or llm = Falcon(api_token=\"my-huggingface-api-key\") df = SmartDataframe(\"data.csv\", config={\"llm\": llm}) As an alternative, you can set the HUGGINGFACE_API_KEY environment variable and instantiate the HuggingFace object without passing the API key: from pandasai import SmartDataframe from pandasai.llm import Starcoder, Falcon llm = Starcoder() # no need to pass the API key, it will be read from the environment variable # or llm = Falcon() # no need to pass the API key, it will be read from the environment variable df = SmartDataframe(\"data.csv\", config={\"llm\": llm})","title":"HuggingFace models"},{"location":"LLMs/llms/#google-palm","text":"In order to use Google PaLM models, you need to have a Google Cloud API key. You can get one here . Once you have an API key, you can use it to instantiate a Google PaLM object: from pandasai import SmartDataframe from pandasai.llm import GooglePalm llm = GooglePalm(api_key=\"my-google-cloud-api-key\") df = SmartDataframe(\"data.csv\", config={\"llm\": llm})","title":"Google PaLM"},{"location":"LLMs/llms/#google-vertexai","text":"In order to use Google PaLM models through Vertexai api, you need to have Google Cloud Project Region of Project Set up Install optional dependency google-cloud-aiplatform Authentication of gcloud Once you have basic setup, you can use it to instantiate a Google PaLM through vertex ai: from pandasai import SmartDataframe from pandasai.llm import GoogleVertexAI llm = GoogleVertexAI(project_id=\"generative-ai-training\", location=\"us-central1\", model=\"text-bison@001\") df = SmartDataframe(\"data.csv\", config={\"llm\": llm})","title":"Google Vertexai"},{"location":"LLMs/llms/#azure-openai","text":"In order to use Azure OpenAI models, you need to have an Azure OpenAI API key as well as an Azure OpenAI endpoint. You can get one here . To instantiate an Azure OpenAI object you also need to specify the name of your deployed model on Azure and the API version: from pandasai import SmartDataframe from pandasai.llm import AzureOpenAI llm = AzureOpenAI( api_token=\"my-azure-openai-api-key\", api_base=\"my-azure-openai-api-endpoint\", api_version=\"2023-05-15\", deployment_name=\"my-deployment-name\" ) df = SmartDataframe(\"data.csv\", config={\"llm\": llm}) As an alternative, you can set the OPENAI_API_KEY , OPENAI_API_VERSION , and OPENAI_API_BASE environment variables and instantiate the Azure OpenAI object without passing them: from pandasai import SmartDataframe from pandasai.llm import AzureOpenAI llm = AzureOpenAI( deployment_name=\"my-deployment-name\" ) # no need to pass the API key, endpoint and API version. They are read from the environment variable df = SmartDataframe(\"data.csv\", config={\"llm\": llm}) If you are behind an explicit proxy, you can specify openai_proxy when instantiating the AzureOpenAI object or set the OPENAI_PROXY environment variable to pass through.","title":"Azure OpenAI"},{"location":"LLMs/llms/#huggingface-via-text-generation","text":"In order to use HuggingFace models via text-generation, you need to first serve a supported large language model (LLM). Read text-generation docs for more on how to setup an inference server. This can be used, for example, to use models like LLaMa2, CodeLLaMa, etc. You can find more information about text-generation here . The inference_server_url is the only required parameter to instantiate an HuggingFaceTextGen model: from pandasai.llm import HuggingFaceTextGen from pandasai import SmartDataframe llm = HuggingFaceTextGen( inference_server_url=\"http://127.0.0.1:8080\" ) df = SmartDataframe(\"data.csv\", config={\"llm\": llm})","title":"HuggingFace via Text Generation"}]}