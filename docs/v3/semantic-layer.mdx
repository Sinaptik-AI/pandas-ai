---
title: 'Semantic Layer'
description: 'Turn raw data into semantic-enhanced and clean dataframes'
---

## What's the Semantic Layer?

The semantic layer allows you to turn raw data into [dataframes](/v3/dataframes) you can ask questions to and [share with your team](/v3/share-dataframes) as conversational AI dashboards. It serves several important purposes:
1. **Data Configuration**: Define how your data should be loaded and processed
2. **Semantic Information**: Add context and meaning to your data columns
3. **Data Transformation**: Specify how data should be cleaned and transformed

## How to use the Semantic Layer?

There are two ways to use the semantic layer:

### For CSV files: using the save method

The simplest way to create a semantic layer for CSV files is using the `save` method:

```python
import pandasai as pai

df = pai.read_csv("data.csv")

df.save(
    path="company/sales-data",         # Format: "organization/dataset"
    name="sales-data",                 # Human-readable name
    description="Sales data from our retail stores",  # Optional description
    columns=[
        {
            "name": "transaction_id",
            "type": "string",
            "description": "Unique identifier for each sale"
        },
        {
            "name": "sale_date"
            "type": "datetime",
            "description": "Date and time of the sale"
        }
    ]
)
```

#### name

The name field identifies your dataset in the save method.

```python
df.save(
    path="company/sales-data",
    name="sales-data",  # Unique, descriptive name
    ...
)
```
**Type**: `str`
- A string without special characters or spaces
- Using kebab-case naming convention
- Unique within your project
- Examples: "sales-data", "customer-profiles"

#### description
A clear text description that helps others understand the dataset's contents and purpose.

```python
df.save(
    path="company/sales-data",
    name="sales-data",
    description="Daily sales transactions from all retail stores, including transaction IDs, dates, and amounts",
    ...
)
```

**Type**: `str`
- The purpose of the dataset
- The type of data contained
- Any relevant context about data collection or usage
- Optional but recommended for better data understanding


#### columns
Define the structure and metadata of your dataset's columns to help PandaAI understand your data better.

```python
df.save(
    path="company/sales-data",
    name="sales-data",
    description="Daily sales transactions from all retail stores",
    columns=[
        {
            "name": "transaction_id",
            "type": "string",
            "description": "Unique identifier for each sale"
        },
        {
            "name": "sale_date"
            "type": "datetime",
            "description": "Date and time of the sale"
        },
        {
            "name": "quantity",
            "type": "integer",
            "description": "Number of units sold"
        },
        {
            "name": "price",
            "type": "float",
            "description": "Price per unit in USD"
        },
        {
            "name": "is_online",
            "type": "boolean",
            "description": "Whether the sale was made online"
        }
    ]
)
```

**Type**: `dict[str, dict]`
- Keys: column names as they appear in your DataFrame
- Values: dictionary containing:
  - `type` (str): Data type of the column
    - "string": IDs, names, categories
    - "integer": counts, whole numbers
    - "float": prices, percentages
    - "datetime": timestamps, dates
    - "boolean": flags, true/false values
  - `description` (str): Clear explanation of what the column represents


### For other data sources: YAML configuration

For other data sources (SQL databases, data warehouses, etc.), create a YAML file in your datasets folder:
> Keep in mind that you have to install the sql, cloud data (ee), or yahoo_finance data extension to use this feature.

Example

```yaml
name: SalesData  # Dataset name
description: "Sales data from our SQL database"

source:
  type: postgresql
  connection_string: "postgresql://user:pass@localhost:5432/db"
  query: "SELECT * FROM sales"

columns:
  - name: transaction_id
    type: string
    description: Unique identifier for each sale
  - name: sale_date
    type: datetime
    description: Date and time of the sale
```

### YAML Semantic Layer Configuration

The following sections detail all available configuration options for your schema.yaml file:

#### name (mandatory)
The name field identifies your dataset in the schema.yaml file.
```yaml
name: sales-data
```


**Type**: `str`
- A string without special characters or spaces
- Using kebab-case naming convention
- Unique within your project
- Examples: "sales-data", "customer-profiles"


#### columns
Define the structure and metadata of your dataset's columns to help PandaAI understand your data better.
```yaml
columns:
  - name: transaction_id
    type: string
    description: Unique identifier for each sale
  - name: sale_date
    type: datetime
    description: Date and time of the sale
```

**Type**: `list[dict]`
- Each dictionary represents a column
- `name` (str): Name of the column
- `type` (str): Data type of the column
  - "string": IDs, names, categories
  - "integer": counts, whole numbers
  - "float": prices, percentages
  - "datetime": timestamps, dates
  - "boolean": flags, true/false values
- `description` (str): Clear explanation of what the column represents

#### transformations
Apply transformations to your data to clean, convert, or anonymize it.

```yaml
transformations:
  - type: anonymize
    params:
      columns:
        - transaction_id
      method: hash
  - type: convert_timezone
    params:
      columns:
        - sale_date
      from_timezone: UTC
      to_timezone: America/New_York
```

**Type**: `list[dict]`
- Each dictionary represents a transformation
- `type` (str): Type of transformation
  - "anonymize" for anonymizing data
  - "convert_timezone" for converting timezones
- `params` (dict): Parameters for the transformation


#### source (mandatory)
Specify the data source for your dataset.

```yaml
source:
  type: postgresql
  connection_string: "postgresql://user:pass@localhost:5432/db"
  query: "SELECT * FROM sales"
```

> The available data sources depends on the installed data extensions (sql, cloud data (ee), yahoo_finance).

**Type**: `dict`
- `type` (str): Type of data source
  - "postgresql" for PostgreSQL databases
  - "mysql" for MySQL databases
  - "sqlite" for SQLite databases
  - "yahoo_finance" for Yahoo Finance data
  - "bigquery" for Google BigQuery data
  - "snowflake" for Snowflake data
  - "databricks" for Databricks data
  - "oracle" for Oracle databases
- `connection_string` (str): Connection string for the data source
- `query` (str): Query to retrieve data from the data source

{/* commented as destination and update frequency will be only in the materialized case
#### destination (mandatory)
Specify the destination for your dataset.

**Type**: `dict`
- `type` (str): Type of destination
  - "local" for local storage
- `format` (str): Format of the data
  - "parquet" for Parquet format
- `path` (str): Path to store the data

```yaml
destination:
  type: local
  format: parquet
  path: /path/to/data
```


#### update_frequency
Specify the frequency of updates for your dataset.

**Type**: `str`
- "daily" for daily updates
- "weekly" for weekly updates
- "monthly" for monthly updates
- "yearly" for yearly updates

```yaml
update_frequency: daily
```
*/}

#### order_by
Specify the columns to order by.

**Type**: `list[str]`
- Each string should be in the format "column_name DESC" or "column_name ASC"

```yaml
order_by:
  - transaction_id DESC
  - sale_date ASC
```

#### limit
Specify the maximum number of records to load.

**Type**: `int`

```yaml
limit: 1000